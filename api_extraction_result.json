{
  "success": true,
  "document_info": {
    "total_pages": 23
  },
  "graph_statistics": {
    "total_components": 17,
    "total_relationships": 18,
    "components_by_type": {
      "Method": 5,
      "Background": 3,
      "Result": 3,
      "Conclusion": 3,
      "Limitation": 2,
      "Claim": 1
    },
    "relationships_by_type": {
      "builds_on": 1,
      "motivates": 4,
      "demonstrates": 4,
      "elaborates": 6,
      "addresses": 1,
      "supported_by": 1,
      "leads_to": 1
    }
  },
  "argument_graph": {
    "nodes": [
      {
        "id": "P1-M5",
        "type": "Method",
        "text": "Our contributions include: Foundations—we survey evaluation datasets in encyclopedic knowledge graph completion to motivate a new benchmark (§2 and Appendix A). Data—we introduce CODEX, a benchmark consisting of three knowledge graphs varying in size and structure, entity types, multilingual labels and descriptions, and—unique to CODEX—manually verified hard negative triples (§ 3). To better understand CODEX, we analyze the logical relation patterns in each of its datasets (§4). Benchmarking—we conduct large-scale model selection and benchmarking experiments, reporting baseline link prediction and triple classification results on CODEX for five widely used embedding models from different architectural classes (§5). Comparative analysis—Finally, to demonstrate the unique value of CODEX, we differentiate CODEX from the popular FB15K-237 knowledge graph completion dataset by showing that CODEX covers more diverse and interpretable content, and is a more difficult link prediction benchmark.",
        "page": 1
      },
      {
        "id": "P1-B2",
        "type": "Background",
        "text": "As progress in artificial intelligence depends heavily on data, a relevant and high-quality benchmark is imperative to evaluating and advancing the state of the art in KGC. However, the field has largely remained static in this regard over the past decade. Outdated subsets of Freebase (Bollacker et al., 2008) are most commonly used for evaluation in KGC, even though Freebase had known quality issues (Tanon et al., 2016) and was eventually deprecated in favor of the more recent Wikidata knowledge base (Vrandecˇic´ and Krötzsch, 2014).",
        "page": 1
      },
      {
        "id": "P4-B6",
        "type": "Background",
        "text": "Knowledge graphs are unique in that they only contain positive statements, meaning that triples not observed in a given knowledge graph are not necessarily false, but merely unseen; this is called the Open World Assumption (Galárraga et al., 2013). However, most machine learning tasks on knowledge graphs require negatives in some capacity. While different negative sampling strategies exist (Cai and Wang, 2018), the most common approach is to randomly perturb observed triples to generate negatives, following Bordes et al. (2013).",
        "page": 4
      },
      {
        "id": "P5-R11",
        "type": "Result",
        "text": "To identify compositional paths, we use the AMIE3 system (Lajus et al., 2020), which outputs rules with confidence scores that capture how many times those rules are seen versus violated, to identify paths of lengths two and three; we omit longer paths as they are relatively costly to compute. We identify 26, 44, and 93 rules in CODEX-S, CODEX-M, and CODEX-L, respectively, with average confidence (out of 1) of 0.630, 0.556, and 0.459. Table 4 gives the percentage of triples per dataset participating in a discovered rule.",
        "page": 5
      },
      {
        "id": "P9-C15",
        "type": "Conclusion",
        "text": "We conclude that while FB15K-237 is a valuable dataset, CODEX is more appropriately difficult for link prediction. Additionally, we note that in FB15K-237, all validation and test triples containing entity pairs directly linked in the training set were deleted (Toutanova and Chen, 2015), meaning that symmetry cannot be tested for in FB15K-237. Given that CODEX datasets contain both symmetry and compositionality, CODEX is more suitable for assessing how well models can learn relation patterns that go beyond frequency.",
        "page": 9
      },
      {
        "id": "P4-L7",
        "type": "Limitation",
        "text": "While random negative sampling is beneficial and even necessary in the case where a large number of negatives is needed (i.e., training), it is not necessarily useful for evaluation. For example, in the task of triple classification, the goal is to discriminate between positive (true) and negative (false) triples. As we show in §5.5, triple classification over randomly generated negatives is trivially easy for state-of-the-art models because random negatives are generally not meaningful or plausible.",
        "page": 4
      },
      {
        "id": "P7-C14",
        "type": "Conclusion",
        "text": "Overall, we find that the choice of loss function in particular significantly impacts model performance. Each model consistently achieved its respective peak performance with cross-entropy (CE) loss, a finding which is corroborated by several other KGC comparison papers (Kadlec et al., 2017; Ruffinelli et al., 2020; Jain et al., 2020). As far as negative sampling techniques, we do not find that a single strategy is dominant, suggesting that the choice of loss function is more important.",
        "page": 7
      },
      {
        "id": "P4-M8",
        "type": "Method",
        "text": "Therefore, we generate and manually evaluate hard negatives for KGC evaluation. Generation—to generate hard negatives, we used each pre-trained embedding model from § 5.2 to predict tail entities of triples in CODEX. For each model, we took as candidate negatives the triples (h,r,tˆ) for which (i) the type of the predicted tail entity tˆ matched the type of the true tail entity t; (ii) tˆ was ranked in the top-10 predictions by that model; and (iii) (h,r,tˆ) was not observed in G.",
        "page": 4
      },
      {
        "id": "P1-B1",
        "type": "Background",
        "text": "Knowledge graphs are multi-relational graphs that express facts about the world by connecting entities (people, places, things, concepts) via different types of relationships. The field of automatic knowledge graph completion (KGC), which is motivated by the fact that knowledge graphs are usually incomplete, is an active research direction spanning several subfields of artificial intelligence (Nickel et al., 2015; Wang et al., 2017; Ji et al., 2020).",
        "page": 1
      },
      {
        "id": "P1-M18",
        "type": "Method",
        "text": "We conduct large-scale model selection and benchmarking experiments, reporting baseline link prediction and triple classification results on CODEX for five extensively tuned embedding models. Finally, we differentiate CODEX from the popular FB15K-237 knowledge graph completion dataset by showing that CODEX covers more diverse and interpretable content, and is a more difficult link prediction benchmark.",
        "page": 1
      },
      {
        "id": "P5-R10",
        "type": "Result",
        "text": "Symmetric relations are relations r for which (h,r,t) ∈ G implies (t,r,h) ∈ G. For each relation, we compute the number of its (head, tail) pairs that overlap with its (tail, head) pairs, divided by the total number of pairs, and take those with 50% overlap or higher as symmetric. CODEX datasets have five such relations: diplomatic relation, shares border with, sibling, spouse, and unmarried partner.",
        "page": 5
      },
      {
        "id": "P5-M12",
        "type": "Method",
        "text": "As recent studies have observed that training strategies are equally, if not more, important than architecture for link prediction (Kadlec et al., 2017; Lacroix et al., 2018; Ruffinelli et al., 2020), we search across a large range of hyperparameters to ensure a truly fair comparison. To this end we use the PyTorch-based LibKGE framework for training and selecting knowledge graph embeddings.",
        "page": 5
      },
      {
        "id": "P1-L3",
        "type": "Limitation",
        "text": "Indeed, KGC benchmarks extracted from Freebase like FB15K and FB15K-237 (Bordes et al., 2013; Toutanova and Chen, 2015) are questionable in quality. For example, FB15K was shown to have train/test leakage (Toutanova and Chen, 2015). Later in this paper (§6.2), we will show that a relatively large proportion of relations in FB15K-237 can be covered by a trivial frequency rule.",
        "page": 1
      },
      {
        "id": "P6-R13",
        "type": "Result",
        "text": "Table 5 gives link prediction results. We find that ComplEx is the best at modeling symmetry and antisymmetry, and indeed it was designed specifically to improve upon bilinear models that do not capture symmetry, like DistMult (Trouillon et al., 2016). As such, it performs the best on CODEX-S, which has the highest proportion of symmetric relations.",
        "page": 6
      },
      {
        "id": "P1-C4",
        "type": "Claim",
        "text": "To address the need for a solid benchmark in KGC, we present CODEX, a set of knowledge graph COmpletion Datasets EXtracted from Wikidata and its sister project Wikipedia. Inasmuch as Wikidata is considered the successor of Freebase, CODEX improves upon existing Freebase-based KGC benchmarks in terms of scope and level of difficulty (Table 1).",
        "page": 1
      },
      {
        "id": "P4-M9",
        "type": "Method",
        "text": "Annotation—we manually labeled all candidate negative triples generated for CODEX-S and CODEX-M as true or false using the guidelines provided in Appendix C. We randomly selected among the triples labeled as false to create validation and test negatives for CODEX-S and CODEX-M, examples of which are given in Table 3.",
        "page": 4
      },
      {
        "id": "P9-C16",
        "type": "Conclusion",
        "text": "Finally, we present CODEX, a set of knowledge graph COmpletion Datasets EXtracted from Wikidata and Wikipedia, and show that CODEX is suitable for multiple KGC tasks. We release data, code, and pretrained models for use by the community at https://bit.ly/2EPbrJs.",
        "page": 9
      }
    ],
    "edges": [
      {
        "source": "P1-C4",
        "target": "P1-M5",
        "relation": "builds_on",
        "page": 1
      },
      {
        "source": "P1-B2",
        "target": "P1-C4",
        "relation": "motivates",
        "page": 1
      },
      {
        "source": "P1-M5",
        "target": "P9-C16",
        "relation": "demonstrates",
        "page": 9
      },
      {
        "source": "P4-B6",
        "target": "P4-L7",
        "relation": "elaborates",
        "page": 4
      },
      {
        "source": "P4-M8",
        "target": "P4-L7",
        "relation": "addresses",
        "page": 4
      },
      {
        "source": "P1-L3",
        "target": "P5-R11",
        "relation": "demonstrates",
        "page": 5
      },
      {
        "source": "P5-R11",
        "target": "P5-M12",
        "relation": "elaborates",
        "page": 5
      },
      {
        "source": "P1-M18",
        "target": "P6-R13",
        "relation": "supported_by",
        "page": 6
      },
      {
        "source": "P5-M12",
        "target": "P7-C14",
        "relation": "leads_to",
        "page": 7
      },
      {
        "source": "P4-M9",
        "target": "P4-M8",
        "relation": "elaborates",
        "page": 4
      },
      {
        "source": "P1-C4",
        "target": "P6-R13",
        "relation": "demonstrates",
        "page": 6
      },
      {
        "source": "P4-B6",
        "target": "P4-M9",
        "relation": "motivates",
        "page": 4
      },
      {
        "source": "P1-M18",
        "target": "P9-C15",
        "relation": "demonstrates",
        "page": 9
      },
      {
        "source": "P1-M5",
        "target": "P5-R11",
        "relation": "elaborates",
        "page": 5
      },
      {
        "source": "P1-B1",
        "target": "P1-C4",
        "relation": "motivates",
        "page": 1
      },
      {
        "source": "P1-M5",
        "target": "P5-M12",
        "relation": "elaborates",
        "page": 5
      },
      {
        "source": "P1-L3",
        "target": "P1-C4",
        "relation": "motivates",
        "page": 1
      },
      {
        "source": "P6-R13",
        "target": "P7-C14",
        "relation": "elaborates",
        "page": 7
      }
    ]
  },
  "evaluation": {
    "score": 0,
    "justification": "Failed to parse model output: ```json\n{\n  \"score\": 70,\n  \"justification\": \"The conclusion section of the article is partially supported by the results section. The results provide a detailed comparison of the CODEX datasets with e"
  }
}