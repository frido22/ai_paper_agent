[
  {
    "page_number": 1,
    "text": "LLaMA: Open and Efficient Foundation Language Models\nHugoTouvron∗,ThibautLavril∗, GautierIzacard∗,XavierMartinet\nMarie-AnneLachaux,TimotheeLacroix,BaptisteRozière,NamanGoyal\nEricHambro,FaisalAzhar,AurelienRodriguez,ArmandJoulin\nEdouardGrave∗,GuillaumeLample∗\nMetaAI\nAbstract performance, a smaller one trained longer will\nultimately be cheaper at inference. For instance,\nWeintroduceLLaMA,acollectionoffounda-\nalthough Hoffmann et al. (2022) recommends\ntionlanguagemodelsrangingfrom7Bto65B\ntraining a 10B model on 200B tokens, we find\nparameters. We train our models on trillions\nthat the performance of a 7B model continues to\noftokens, andshowthatitispossibletotrain\nstate-of-the-art models using publicly avail- improveevenafter1Ttokens.\nable datasets exclusively, without resorting\nto proprietary and inaccessible datasets. In The focus of this work is to train a series of\nparticular, LLaMA-13B outperforms GPT-3 languagemodelsthatachievethebestpossibleper-\n(175B) on most benchmarks, and LLaMA- formanceatvariousinferencebudgets,bytraining\n65B is competitive with the best models,\non more tokens than what is typically used. The\nChinchilla-70B and PaLM-540B. We release\nresultingmodels,calledLLaMA,rangesfrom7B\nallourmodelstotheresearchcommunity1.\nto65Bparameterswithcompetitiveperformance\n1 Introduction comparedtothebestexistingLLMs. Forinstance,\nLLaMA-13BoutperformsGPT-3onmostbench-\nLargeLanguagesModels(LLMs)trainedonmas-\nmarks,despitebeing10×smaller. Webelievethat\nsivecorporaoftextshaveshowntheirabilitytoper-\nthis model will help democratize the access and\nformnewtasksfromtextualinstructionsorfroma\nstudyofLLMs,sinceitcanberunonasingleGPU.\nfewexamples(Brownetal.,2020). Thesefew-shot\nAtthehigher-endofthescale,our65B-parameter\npropertiesfirstappearedwhenscalingmodelstoa\nmodel is also competitive with the best large lan-\nsufficientsize(Kaplanetal.,2020),resultingina\nguagemodelssuchasChinchillaorPaLM-540B.\nlineofworkthatfocusesonfurtherscalingthese\nmodels(Chowdheryetal.,2022;Raeetal.,2021). Unlike Chinchilla, PaLM, or GPT-3, we only\nThese efforts are based on the assumption that usepubliclyavailabledata,makingourworkcom-\nmore parameters will lead to better performance. patible with open-sourcing, while most existing\nHowever,recentworkfromHoffmannetal.(2022) models rely on data which is either not publicly\nshows that, for a given compute budget, the best availableorundocumented(e.g. “Books–2TB”or\nperformancesarenotachievedbythelargestmod- “Social media conversations”). There exist some\nels,butbysmallermodelstrainedonmoredata. exceptions, notably OPT (Zhang et al., 2022),\nThe objective of the scaling laws from Hoff- GPT-NeoX (Black et al., 2022), BLOOM (Scao\nmann et al. (2022) is to determine how to best etal.,2022)andGLM(Zengetal.,2022),butnone\nscale the dataset and model sizes for a particular thatarecompetitivewithPaLM-62BorChinchilla.\ntrainingcomputebudget. However,thisobjective\ndisregards the inference budget, which becomes Intherestofthispaper,wepresentanoverview\ncritical when serving a language model at scale. of the modifications we made to the transformer\nInthiscontext,givenatargetlevelofperformance, architecture(Vaswanietal.,2017),aswellasour\nthepreferredmodelisnotthefastesttotrainbutthe trainingmethod. Wethenreporttheperformanceof\nfastestatinference,andalthoughitmaybecheaper ourmodelsandcomparewithothersLLMsonaset\nto train a large model to reach a certain level of ofstandardbenchmarks. Finally,weexposesome\nof thebiases and toxicityencoded inour models,\n∗ Equal contribution. Correspondence: {htouvron,\nthibautlav,gizacard,egrave,glample}@meta.com using some of the most recent benchmarks from\n1https://github.com/facebookresearch/llama theresponsibleAIcommunity.\n3202\nbeF\n72\n]LC.sc[\n1v17931.2032:viXra",
    "tables": [],
    "text_stats": {
      "word_count": 336,
      "char_count": 3757,
      "line_count": 67,
      "table_count": 0
    }
  },
  {
    "page_number": 2,
    "text": "2 Approach\nDataset Samplingprop. Epochs Disksize\nOur training approach is similar to the methods CommonCrawl 67.0% 1.10 3.3TB\ndescribed in previous work (Brown et al., 2020; C4 15.0% 1.06 783GB\nChowdhery et al., 2022), and is inspired by the Github 4.5% 0.64 328GB\nChinchilla scaling laws (Hoffmann et al., 2022). Wikipedia 4.5% 2.45 83GB\nWetrainlargetransformersonalargequantityof Books 4.5% 2.23 85GB\ntextualdatausingastandardoptimizer. ArXiv 2.5% 1.06 92GB\nStackExchange 2.0% 1.03 78GB\n2.1 Pre-trainingData\nOurtrainingdatasetisamixtureofseveralsources, Table1:Pre-trainingdata.Datamixturesusedforpre-\nreportedinTable1,thatcoveradiversesetofdo- training, for each subset we list the sampling propor-\ntion, numberofepochsperformedonthesubsetwhen\nmains. For the most part, we reuse data sources\ntrainingon1.4Ttokens,anddisksize. Thepre-training\nthathavebeenleveragedtotrainotherLLMs,with\nrunson1Ttokenshavethesamesamplingproportion.\nthe restriction of only using data that is publicly\navailable,andcompatiblewithopensourcing. This\nleadstothefollowingmixtureofdataandtheper- languages, which use either the Latin or Cyrillic\ncentagetheyrepresentinthetrainingset: scripts: bg, ca, cs, da, de, en, es, fr, hr, hu, it,\nnl,pl,pt,ro,ru,sl,sr,sv,uk. Weprocessthe\nEnglishCommonCrawl[67%]. Wepreprocess\ndata to remove hyperlinks, comments and other\nfive CommonCrawl dumps, ranging from 2017\nformattingboilerplate.\nto 2020, with the CCNet pipeline (Wenzek et al.,\n2020). This process deduplicates the data at the\nGutenberg and Books3 [4.5%]. We include\nline level, performs language identification with\ntwobookcorporainourtrainingdataset: theGuten-\nafastTextlinearclassifiertoremovenon-English\nbergProject,whichcontainsbooksthatareinthe\npages and filters low quality content with an n-\npublic domain, and the Books3 section of TheP-\ngram language model. In addition, we trained a\nile(Gaoetal.,2020),apubliclyavailabledataset\nlinear model to classify pages used as references\nfor training large language models. We perform\nin Wikipedia v.s. randomly sampled pages, and\ndeduplication at the book level, removing books\ndiscardedpagesnotclassifiedasreferences.\nwithmorethan90%contentoverlap.\nC4[15%]. Duringexploratoryexperiments,we\nArXiv [2.5%]. We process arXiv Latex files\nobserved that using diverse pre-processed Com-\nto add scientific data to our dataset. Following\nmonCrawldatasetsimprovesperformance. Wethus\nLewkowyczetal.(2022),weremovedeverything\nincludedthepubliclyavailableC4dataset(Raffel\nbeforethefirstsection,aswellasthebibliography.\netal.,2020)inourdata. ThepreprocessingofC4\nWealsoremovedthecommentsfromthe.texfiles,\nalsocontainsdeduplicationandlanguageidentifi-\nandinline-expandeddefinitionsandmacroswritten\ncation steps: the main difference with CCNet is\nbyuserstoincreaseconsistencyacrosspapers.\nthequalityfiltering,whichmostlyreliesonheuris-\nticssuchaspresenceofpunctuationmarksorthe\nStack Exchange [2%]. We include a dump of\nnumberofwordsandsentencesinawebpage.\nStack Exchange, a website of high quality ques-\nGithub [4.5%]. We use the public GitHub tions and answers that covers a diverse set of do-\ndataset available on Google BigQuery. We only mains,rangingfromcomputersciencetochemistry.\nkeptprojectsthataredistributedundertheApache, Wekeptthedatafromthe28largestwebsites,re-\nBSD and MIT licenses. Additionally, we filtered moved the HTML tags from text and sorted the\nlowqualityfileswithheuristicsbasedontheline answersbyscore(fromhighesttolowest).\nlength or proportion of alphanumeric characters,\nTokenizer. We tokenize the data with the byte-\nandremovedboilerplate,suchasheaders,withreg-\npair encoding (BPE) algorithm (Sennrich et al.,\nularexpressions. Finally,wededuplicatetheresult-\n2015), using the implementation from Sentence-\ningdatasetatthefilelevel,withexactmatches.\nPiece(KudoandRichardson,2018). Notably,we\nWikipedia [4.5%]. We add Wikipedia dumps splitallnumbersintoindividualdigits,andfallback\nfrom the June-August 2022 period, covering 20 tobytestodecomposeunknownUTF-8characters.",
    "tables": [],
    "text_stats": {
      "word_count": 392,
      "char_count": 4008,
      "line_count": 77,
      "table_count": 0
    }
  },
  {
    "page_number": 3,
    "text": "params dimension nheads nlayers learningrate batchsize ntokens\n6.7B 4096 32 32 3.0e−4 4M 1.0T\n13.0B 5120 40 40 3.0e−4 4M 1.0T\n32.5B 6656 52 60 1.5e−4 4M 1.4T\n65.2B 8192 64 80 1.5e−4 4M 1.4T\nTable2: Modelsizes,architectures,andoptimizationhyper-parameters.\nOverall, our entire training dataset contains\n2.2\nroughly1.4Ttokensaftertokenization. Formostof\n2.1\nourtrainingdata,eachtokenisusedonlyoncedur-\n2.0\ning training, with the exception of the Wikipedia\nand Books domains, over which we perform ap- 1.9\nproximatelytwoepochs. 1.8\n1.7\n2.2 Architecture\n1.6\nFollowingrecentworkonlargelanguagemodels,\nournetworkisbasedonthetransformerarchitec- 1.5\n0 200 400 600 800 1000 1200 1400\nture (Vaswani et al., 2017). We leverage various Billion of tokens\nimprovements that were subsequently proposed,\nandusedindifferentmodelssuchasPaLM.Here\narethemaindifferencewiththeoriginalarchitec-\nture,andwherewewerefoundtheinspirationfor\nthischange(inbracket):\nPre-normalization [GPT3]. To improve the\ntraining stability, we normalize the input of each\ntransformersub-layer,insteadofnormalizingthe\noutput. WeusetheRMSNormnormalizingfunc-\ntion,introducedbyZhangandSennrich(2019).\nSwiGLU activation function [PaLM]. We re-\nplacetheReLUnon-linearitybytheSwiGLUac-\ntivationfunction,introducedbyShazeer(2020)to\nimprovetheperformance. Weuseadimensionof\n24dinsteadof4dasinPaLM.\n3\nRotaryEmbeddings[GPTNeo]. Weremovethe\nabsolutepositionalembeddings,andinstead,add\nrotarypositionalembeddings(RoPE),introduced\nbySuetal.(2021),ateachlayerofthenetwork.\nThedetailsofthehyper-parametersforourdif-\nferentmodelsaregiveninTable2.\n2.3 Optimizer\nOur models are trained using the AdamW opti-\nmizer(LoshchilovandHutter,2017),withthefol-\nlowing hyper-parameters: β = 0.9,β = 0.95.\n1 2\nWeuse acosine learningrate schedule, such that\nthefinallearningrateisequalto10%ofthemaxi-\nmallearningrate. Weuseaweightdecayof0.1and\ngradient clipping of 1.0. We use 2,000 warmup\nssol\ngniniarT\nLLaMA 7B\nLLaMA 13B\nLLaMA 33B\nLLaMA 65B\nFigure1: Traininglossovertraintokensforthe7B,\n13B,33B,and65models. LLaMA-33BandLLaMA-\n65Bweretrainedon1.4Ttokens. Thesmallermodels\nwere trained on 1.0T tokens. All models are trained\nwithabatchsizeof4Mtokens.\nsteps,andvarythelearningrateandbatchsizewith\nthesizeofthemodel(seeTable2fordetails).\n2.4 Efficientimplementation\nWemakeseveraloptimizationstoimprovethetrain-\ningspeedofourmodels. First,weuseanefficient\nimplementationofthecausalmulti-headattention\ntoreducememoryusageandruntime. Thisimple-\nmentation, available in the xformers library,2 is\ninspired by Rabe and Staats (2021) and uses the\nbackwardfromDaoetal.(2022). Thisisachieved\nbynotstoringtheattentionweightsandnotcom-\nputingthekey/queryscoresthataremaskeddueto\nthecausalnatureofthelanguagemodelingtask.\nTo further improve training efficiency, we re-\nduced the amount of activations that are recom-\nputed during the backward pass with checkpoint-\ning. More precisely, we save the activations that\nare expensive to compute, such as the outputs of\nlinearlayers. Thisisachievedbymanuallyimple-\nmentingthebackwardfunctionforthetransformer\nlayers,insteadofrelyingonthePyTorchautograd.\nTofullybenefitfromthisoptimization,weneedto\n2https://github.com/facebookresearch/xformers",
    "tables": [
      [
        [
          "LLaM",
          "A 7B"
        ],
        [
          "LLaM",
          "A 13B"
        ],
        [
          "LLaM LLaM",
          "A 33B A 65B"
        ]
      ]
    ],
    "text_stats": {
      "word_count": 278,
      "char_count": 3203,
      "line_count": 87,
      "table_count": 1
    }
  },
  {
    "page_number": 4,
    "text": "BoolQ PIQA SIQA HellaSwag WinoGrande ARC-e ARC-c OBQA\nGPT-3 175B 60.5 81.0 - 78.9 70.2 68.8 51.4 57.6\nGopher 280B 79.3 81.8 50.6 79.2 70.1 - - -\nChinchilla 70B 83.7 81.8 51.3 80.8 74.9 - - -\nPaLM 62B 84.8 80.5 - 79.7 77.0 75.2 52.5 50.4\nPaLM-cont 62B 83.9 81.4 - 80.6 77.0 - - -\nPaLM 540B 88.0 82.3 - 83.4 81.1 76.6 53.0 53.4\n7B 76.5 79.8 48.9 76.1 70.1 72.8 47.6 57.2\n13B 78.1 80.1 50.4 79.2 73.0 74.8 52.7 56.4\nLLaMA\n33B 83.1 82.3 50.4 82.8 76.0 80.0 57.8 58.6\n65B 85.3 82.8 52.3 84.2 77.0 78.9 56.0 60.2\nTable3: Zero-shotperformanceonCommonSenseReasoningtasks.\nreduce the memory usage of the model by using We evaluate LLaMA on free-form generation\nmodel and sequence parallelism, as described by tasks and multiple choice tasks. In the multiple\nKorthikantietal.(2022). Moreover,wealsoover- choice tasks, the objective is to select the most\nlapthecomputationofactivationsandthecommu- appropriate completion among a set of given op-\nnicationbetweenGPUsoverthenetwork(dueto tions,basedonaprovidedcontext. Weselectthe\nall_reduceoperations)asmuchaspossible. completion with the highest likelihood given the\nWhentraininga65B-parametermodel,ourcode provided context. We follow Gao et al. (2021)\nprocesses around 380 tokens/sec/GPU on 2048 andusethelikelihoodnormalizedbythenumber\nA100GPUwith80GBofRAM.Thismeansthat ofcharactersinthecompletion,exceptforcertain\ntraining over our dataset containing 1.4T tokens datasets(OpenBookQA,BoolQ),forwhichwefol-\ntakesapproximately21days. low Brown et al. (2020), and select a completion\nbased on the likelihood normalized by the likeli-\n3 Mainresults\nhoodofthecompletiongiven“Answer:” ascontext:\nFollowingpreviouswork(Brownetal.,2020),we P(completion|context)/P(completion|“Answer:”).\nconsiderzero-shotandfew-shottasks,andreport\nresultsonatotalof20benchmarks: 0-shot 1-shot 5-shot 64-shot\n• Zero-shot. Weprovideatextualdescription GPT-3 175B 14.6 23.0 - 29.9\nof the task and a test example. The model Gopher 280B 10.1 - 24.5 28.2\neitherprovidesananswerusingopen-ended Chinchilla 70B 16.6 - 31.5 35.5\ngeneration,orrankstheproposedanswers.\n8B 8.4 10.6 - 14.6\n• Few-shot. Weprovideafewexamplesofthe PaLM 62B 18.1 26.5 - 27.6\ntask (between 1 and 64) and a test example. 540B 21.2 29.3 - 39.6\nThemodeltakesthistextasinputandgener-\n7B 16.8 18.7 22.0 26.1\natestheanswerorranksdifferentoptions.\n13B 20.1 23.4 28.1 31.9\nLLaMA\nWecompareLLaMAwithotherfoundationmod- 33B 24.9 28.3 32.9 36.0\nels, namely the non-publicly available language 65B 23.8 31.0 35.0 39.9\nmodelsGPT-3(Brownetal.,2020),Gopher(Rae\net al., 2021), Chinchilla (Hoffmann et al., 2022) Table4: NaturalQuestions. Exactmatchperformance.\nand PaLM (Chowdhery et al., 2022), as well as\ntheopen-sourcedOPTmodels(Zhangetal.,2022),\nGPT-J(WangandKomatsuzaki,2021),andGPT-\n3.1 CommonSenseReasoning\nNeo (Black et al., 2022). In Section 4, we also\nbriefly compare LLaMA with instruction-tuned We consider eight standard common sense rea-\nmodels such as OPT-IML (Iyer et al., 2022) and soning benchmarks: BoolQ (Clark et al., 2019),\nFlan-PaLM(Chungetal.,2022). PIQA(Bisketal.,2020),SIQA(Sapetal.,2019),",
    "tables": [],
    "text_stats": {
      "word_count": 379,
      "char_count": 3080,
      "line_count": 54,
      "table_count": 0
    }
  },
  {
    "page_number": 5,
    "text": "HellaSwag(Zellersetal.,2019),WinoGrande(Sak- RACE-middle RACE-high\naguchietal.,2021),ARCeasyandchallenge(Clark\nGPT-3 175B 58.4 45.5\net al., 2018) and OpenBookQA (Mihaylov et al.,\n2018). ThesedatasetsincludeClozeandWinograd 8B 57.9 42.3\nstyletasks,aswellasmultiplechoicequestionan- PaLM 62B 64.3 47.5\nswering. We evaluate in the zero-shot setting as 540B 68.1 49.1\ndoneinthelanguagemodelingcommunity.\n7B 61.1 46.9\nIn Table 3, we compare with existing models\n13B 61.6 47.2\nofvarioussizesandreportnumbersfromthecor- LLaMA\n33B 64.1 48.3\nresponding papers. First, LLaMA-65B outper-\n65B 67.9 51.6\nformsChinchilla-70Bonallreportedbenchmarks\nbutBoolQ.Similarly,thismodelsurpassesPaLM-\nTable 6: Reading Comprehension. Zero-shot accu-\n540BeverywherebutonBoolQandWinoGrande. racy.\nLLaMA-13B model also outperforms GPT-3 on\nmostbenchmarksdespitebeing10×smaller.\nschoolChinesestudents. Wefollowtheevaluation\n3.2 Closed-bookQuestionAnswering setupfromBrownetal.(2020)andreportresults\ninTable6. Onthesebenchmarks,LLaMA-65Bis\nWe compare LLaMA to existing large language\ncompetitivewithPaLM-540B,and,LLaMA-13B\nmodels on two closed-book question answering\noutperformsGPT-3byafewpercents.\nbenchmarks: Natural Questions (Kwiatkowski\netal.,2019)andTriviaQA(Joshietal.,2017). For\n3.4 Mathematicalreasoning\nboth benchmarks, we report exact match perfor-\nWeevaluateourmodelsontwomathematicalrea-\nmanceinaclosedbooksetting,i.e.,wherethemod-\nsoning benchmarks: MATH (Hendrycks et al.,\nels do not have access to documents that contain\n2021) and GSM8k (Cobbe et al., 2021). MATH\nevidence to answer the question. In Table 4, we\nisadatasetof12Kmiddleschoolandhighschool\nreportperformanceonNaturalQuestions,andinTa-\nmathematicsproblemswritteninLaTeX.GSM8k\nble5,wereportonTriviaQA.Onbothbenchmarks,\nis a set of middle school mathematical problems.\nLLaMA-65Bachievestate-of-the-artsperformance\nIn Table 7, we compare with PaLM and Min-\nin the zero-shot and few-shot settings. More im-\nerva(Lewkowyczetal.,2022). Minervaisaseries\nportantly,theLLaMA-13Bisalsocompetitiveon\nof PaLM models finetuned on 38.5B tokens ex-\nthesebenchmarkswithGPT-3andChinchilla,de-\ntracted from ArXiv and Math Web Pages, while\nspite being 5-10× smaller. This model runs on a\nneitherPaLMorLLaMAarefinetunedonmathe-\nsingleV100GPUduringinference.\nmaticaldata. ThenumbersforPaLMandMinerva\nare taken from Lewkowycz et al. (2022), and we\n0-shot 1-shot 5-shot 64-shot\ncompare with and without maj1@k. maj1@k de-\nGopher 280B 43.5 - 57.0 57.2 notesevaluationswherewegeneratek samplesfor\nChinchilla 70B 55.4 - 64.1 64.6 eachproblemandperformamajorityvoting(Wang\netal.,2022). OnGSM8k,weobservethatLLaMA-\n7B 50.0 53.4 56.3 57.6\n65BoutperformsMinerva-62B,althoughithasnot\n13B 56.6 60.5 63.1 64.0\nLLaMA\nbeenfine-tunedonmathematicaldata.\n33B 65.1 67.9 69.9 70.4\n65B 68.2 71.6 72.6 73.0\n3.5 Codegeneration\nTable 5: TriviaQA. Zero-shot and few-shot exact We evaluate the ability of our models to write\nmatchperformanceonthefiltereddevset. code from a natural language description on two\nbenchmarks: HumanEval(Chenetal.,2021)and\nMBPP (Austin et al., 2021). For both tasks, the\n3.3 ReadingComprehension\nmodel receives a description of the program in a\nWeevaluateourmodelsontheRACEreadingcom- few sentences, as well as a few input-output ex-\nprehension benchmark (Lai et al., 2017). This amples. InHumanEval,italsoreceivesafunction\ndataset was collected from English reading com- signature, and the prompt is formatted as natural\nprehension exams designed for middle and high code with the textual description and tests in a",
    "tables": [],
    "text_stats": {
      "word_count": 375,
      "char_count": 3547,
      "line_count": 79,
      "table_count": 0
    }
  },
  {
    "page_number": 6,
    "text": "MATH +maj1@k GSM8k +maj1@k Params HumanEval MBPP\npass@ @1 @100 @1 @80\n8B 1.5 - 4.1 -\nPaLM 62B 4.4 - 33.0 - LaMDA 137B 14.0 47.3 14.8 62.4\n540B 8.8 - 56.5 - PaLM 8B 3.6∗ 18.7∗ 5.0∗ 35.7∗\nPaLM 62B 15.9 46.3∗ 21.4 63.2∗\n8B 14.1 25.4 16.2 28.4\nPaLM-cont 62B 23.7 - 31.2 -\nMinerva 62B 27.6 43.4 52.4 68.5\nPaLM 540B 26.2 76.2 36.8 75.0\n540B 33.6 50.3 68.5 78.5\n7B 10.5 36.5 17.7 56.2\n7B 2.9 6.9 11.0 18.1\n13B 15.8 52.5 22.0 64.0\n13B 3.9 8.8 17.8 29.3 LLaMA\nLLaMA 33B 21.7 70.7 30.2 73.4\n33B 7.1 15.2 35.6 53.1\n65B 23.7 79.3 37.7 76.8\n65B 10.6 20.5 50.9 69.7\nTable 8: Model performance for code generation.\nTable7: Modelperformanceonquantitativereason-\nWe report the pass@ score on HumanEval and MBPP.\ning datasets. For majority voting, we use the same\nHumanEval generations are done in zero-shot and\nsetup as Minerva, with k = 256 samples for MATH\nMBBP with 3-shot prompts similar to Austin et al.\nandk = 100forGSM8k(Minerva540Busesk = 64\n(2021). Thevaluesmarkedwith∗arereadfromfigures\nforMATHandandk =40forGSM8k). LLaMA-65B\ninChowdheryetal.(2022).\noutperforms Minerva 62B on GSM8k, although it has\nnotbeenfine-tunedonmathematicaldata.\n3.6 MassiveMultitaskLanguage\nUnderstanding\ndocstring. ThemodelneedstogenerateaPython\nprogram that fits the description and satisfies the The massive multitask language understanding\ntest cases. In Table 8, we compare the pass@1 benchmark,orMMLU,introducedbyHendrycks\nscoresofourmodelswithexistinglanguagemod- etal.(2020)consistsofmultiplechoicequestions\nels that have not been finetuned on code, namely covering various domains of knowledge, includ-\nPaLMandLaMDA(Thoppilanetal.,2022). PaLM ing humanities, STEM and social sciences. We\nandLLaMAweretrainedondatasetsthatcontain evaluateourmodelsinthe5-shotsetting,usingthe\nasimilarnumberofcodetokens. examplesprovidedbythebenchmark,andreport\nresultsinTable9. Onthisbenchmark,weobserve\nAs show in Table 8, for a similar number\nthat the LLaMA-65B is behind both Chinchilla-\nof parameters, LLaMA outperforms other gen-\n70BandPaLM-540Bbyafewpercentinaverage,\neral models such as LaMDA and PaLM, which\nandacrossmostdomains. Apotentialexplanation\nare not trained or finetuned specifically for code.\nis that we have used a limited amount of books\nLLaMA with 13B parameters and more outper-\nandacademicpapersinourpre-trainingdata,i.e.,\nforms LaMDA 137B on both HumanEval and\nArXiv,GutenbergandBooks3,thatsumsuptoonly\nMBPP.LLaMA65BalsooutperformsPaLM62B,\n177GB,whilethesemodelsweretrainedonupto\nevenwhenitistrainedlonger. Thepass@1results\n2TB of books. This large quantity of books used\nreported in this table were obtained by sampling\nbyGopher,ChinchillaandPaLMmayalsoexplain\nwithtemperature0.1. Thepass@100andpass@80\nwhyGopheroutperformsGPT-3onthisbenchmark,\nmetrics were obtained with temperature 0.8. We\nwhileitiscomparableonotherbenchmarks.\nusethesamemethodasChenetal.(2021)toobtain\nunbiasedestimatesofthepass@k.\n3.7 Evolutionofperformanceduringtraining\nItispossibletoimprovetheperformanceoncode\nbyfinetuningoncode-specifictokens. Forinstance, Duringtraining,wetrackedtheperformanceofour\nPaLM-Coder (Chowdhery et al., 2022) increases modelsonafewquestionansweringandcommon\nthe pass@1 score of PaLM on HumanEval from sense benchmarks, and report them in Figure 2.\n26.2% for PaLM to 36%. Other models trained On most benchmarks, the performance improves\nspecificallyforcodealsoperformbetterthangen- steadily,andcorrelateswiththetrainingperplexity\neralmodelsonthesetasks(Chenetal.,2021;Ni- of the model (see Figure 1). The exceptions are\njkampetal.,2022;Friedetal.,2022). Finetuning SIQA and WinoGrande. Most notably, on SIQA,\noncodetokensisbeyondthescopeofthispaper. we observe a lot of variance in performance,",
    "tables": [],
    "text_stats": {
      "word_count": 424,
      "char_count": 3683,
      "line_count": 77,
      "table_count": 0
    }
  },
  {
    "page_number": 7,
    "text": "Humanities STEM SocialSciences Other Average\nGPT-NeoX 20B 29.8 34.9 33.7 37.7 33.6\nGPT-3 175B 40.8 36.7 50.4 48.8 43.9\nGopher 280B 56.2 47.4 71.9 66.1 60.0\nChinchilla 70B 63.6 54.9 79.3 73.9 67.5\n8B 25.6 23.8 24.1 27.8 25.4\nPaLM 62B 59.5 41.9 62.7 55.8 53.7\n540B 77.0 55.6 81.0 69.6 69.3\n7B 34.0 30.5 38.3 38.1 35.1\n13B 45.0 35.8 53.8 53.3 46.9\nLLaMA\n33B 55.8 46.0 66.7 63.4 57.8\n65B 61.8 51.7 72.9 67.4 63.4\nTable9: MassiveMultitaskLanguageUnderstanding(MMLU).Five-shotaccuracy.\nthat may indicate that this benchmark is not InTable10,wereporttheresultsofourinstruct\nreliable. On WinoGrande, the performance does modelLLaMA-IonMMLUandcomparewithex-\nnot correlate as well with training perplexity: isting instruction finetuned models of moderate\nthe LLaMA-33B and LLaMA-65B have similar sizes,namely,OPT-IML(Iyeretal.,2022)andthe\nperformanceduringthetraining. Flan-PaLMseries(Chungetal.,2022). Allthere-\nportednumbersarefromthecorrespondingpapers.\n4 InstructionFinetuning Despitethesimplicityoftheinstructionfinetuning\napproach used here, we reach 68.9% on MMLU.\nInthissection,weshowthatbrieflyfinetuningon\nLLaMA-I(65B)outperformsonMMLUexisting\ninstructions data rapidly leads to improvements\ninstructionfinetunedmodelsofmoderatesizes,but\non MMLU. Although the non-finetuned version\nare still far from the state-of-the-art, that is 77.4\nofLLaMA-65Bisalreadyabletofollowbasicin- forGPTcode-davinci-002onMMLU(numbers\nstructions,weobservethataverysmallamountof\ntaken from Iyer et al. (2022)). The details of the\nfinetuning improves the performance on MMLU,\nperformance on MMLU on the 57 tasks can be\nand further improves the ability of the model to\nfoundinTable16oftheappendix.\nfollow instructions. Since this is not the focus of\nthispaper,weonlyconductedasingleexperiment\nfollowingthesameprotocolasChungetal.(2022) 5 Bias,ToxicityandMisinformation\ntotrainaninstructmodel,LLaMA-I.\nLarge language models have been showed to re-\nOPT 30B 26.1\nproduce and amplify biases that are existing in\nGLM 120B 44.8\nthetrainingdata(Shengetal.,2019;Kuritaetal.,\nPaLM 62B 55.1\n2019), and to generate toxic or offensive con-\nPaLM-cont 62B 62.8\ntent(Gehmanetal.,2020). Asourtrainingdataset\nChinchilla 70B 67.5\ncontainsalargeproportionofdatafromtheWeb,\nLLaMA 65B 63.4\nwe believe that it is crucial to determine the po-\nOPT-IML-Max 30B 43.2 tential for our models to generate such content.\nFlan-T5-XXL 11B 55.1 TounderstandthepotentialharmofLLaMA-65B,\nFlan-PaLM 62B 59.6 weevaluateondifferentbenchmarksthatmeasure\nFlan-PaLM-cont 62B 66.1 toxiccontentproductionandstereotypesdetection.\nLLaMA-I 65B 68.9 Whilewehaveselectedsomeofthestandardbench-\nmarks that are used by the language model com-\nTable 10: Instruction finetuning – MMLU (5-shot). munity to indicate some of the issues with these\nComparisonofmodelsofmoderatesizewithandwith- models,theseevaluationsarenotsufficienttofully\noutinstructionfinetuningonMMLU. understandtherisksassociatedwiththesemodels.",
    "tables": [],
    "text_stats": {
      "word_count": 318,
      "char_count": 2935,
      "line_count": 61,
      "table_count": 0
    }
  },
  {
    "page_number": 8,
    "text": "70\n60\n50\n40\n30\n20\n0 250 500 750 1000 1250 1500\nycaruccA\nTriviaQA HellaSwag NaturalQuestions\n85\n35\n80\n30\n75\n25\n70\n20\n65\n15\n60 10\n55 5\n50 0\n0 250 500 750 1000 1250 1500 0 250 500 750 1000 1250 1500\n52\n50\n48\n46\n44\n42\n40\n0 250 500 750 1000 1250 1500\nBillion of tokens\nycaruccA\nSIQA WinoGrande PIQA\n80\n82.5\n75\n80.0\n70 77.5\n75.0\n65\nLLaMA 7B\n72.5\n60 LLaMA 13B\n70.0 LLaMA 33B\n55 LLaMA 65B 67.5\nChinchilla\n50 65.0\n0 250 500 750 1000 1250 1500 0 250 500 750 1000 1250 1500\nBillion of tokens Billion of tokens\nFigure2: Evolutionofperformanceonquestionansweringandcommonsensereasoningduringtraining.\n5.1 RealToxicityPrompts Basic Respectful\nLanguagemodelscangeneratetoxiclanguage,e.g., 7B 0.106 0.081\ninsults,hatespeechorthreats. Thereisaverylarge 13B 0.104 0.095\nLLaMA\nrange of toxic content that a model can generate, 33B 0.107 0.087\nmakingathoroughevaluationchallenging. Several 65B 0.128 0.141\nrecentwork(Zhangetal.,2022;Hoffmannetal.,\n2022) have considered the RealToxicityPrompts Table11: RealToxicityPrompts. Werunagreedyde-\nbenchmark(Gehmanetal.,2020)asanindicator coderonthe100kpromptsfromthisbenchmark. The\nofhowtoxicistheirmodel. RealToxicityPrompts “respectful” versions are prompts starting with “Com-\npletethefollowingsentenceinapolite,respectful,and\nconsistsofabout100kpromptsthatthemodelmust\nunbiased manner:”, and “Basic” is without it. Scores\ncomplete; then a toxicity score is automatically\nwere obtained using the PerplexityAPI, with higher\nevaluatedbymakingarequesttoPerspectiveAPI3.\nscoreindicatingmoretoxicgenerations.\nWedonothavecontroloverthepipelineusedby\nthethird-partyPerspectiveAPI,makingcomparison\nwithpreviousmodelsdifficult.\nForeachofthe100kprompts,wegreedilygen-\nerate with our models, and measure their toxic-\nity score. The score per prompt ranges from 0\nwiththesizeofthemodel,especiallyforRespect-\n(non-toxic)to1(toxic). InTable11,wereportour\nful prompts. This was also observed in previous\naveragedscoreonbasicandrespectfulpromptcat-\nwork(Zhangetal.,2022),withthenotableexcep-\negoriesofRealToxicityPrompts. Thesescoresare\ntionofHoffmannetal.(2022)wheretheydonot\n“comparable” with what we observe in the litera-\nsee a difference between Chinchilla and Gopher,\nture (e.g., 0.087 for Chinchilla) but the method-\ndespitedifferentsizes. Thiscouldbeexplainedby\nologies differ between these work and ours (in\nthe fact that the larger model, Gopher, has worse\ntermsofsamplingstrategy,numberofpromptsand\nperformance than Chinchilla, suggesting that the\ntime of API). We observe that toxicity increases\nrelationbetweentoxicityandmodelsizemayonly\n3https://perspectiveapi.com/ applywithinamodelfamily.",
    "tables": [
      [
        [
          "",
          "7B"
        ],
        [
          "LLaMA",
          "7B"
        ],
        [
          "LLaMA",
          "13B"
        ],
        [
          "LLaMA",
          "33B"
        ],
        [
          "LLaMA Chinch",
          "65B illa"
        ]
      ]
    ],
    "text_stats": {
      "word_count": 289,
      "char_count": 2614,
      "line_count": 92,
      "table_count": 1
    }
  },
  {
    "page_number": 9,
    "text": "the context of the sentence. The goal is to reveal\nLLaMA GPT3 OPT\nif societal biases associated with occupations\nGender 70.6 62.6 65.7\nhave been captured by the model. For example,\nReligion 79.0 73.3 68.6\na sentence in the WinoGender dataset is “The\nRace/Color 57.0 64.7 68.6\nnurse notified the patient that his shift would be\nSexualorientation 81.0 76.2 78.6 ending in an hour.”, which is followed by ‘His’\nAge 70.1 64.4 67.8 refers to. Wethencomparetheperplexityofthe\nNationality 64.2 61.6 62.9 continuationsthe nurseandthe patienttoper-\nDisability 66.7 76.7 76.7\nformco-referenceresolutionwiththemodel. We\nPhysicalappearance 77.8 74.6 76.2\nevaluatetheperformancewhenusing3pronouns:\nSocioeconomicstatus 71.5 73.8 76.2\n“her/her/she”,“his/him/he”and“their/them/some-\nAverage 66.6 67.2 69.5 one” (the different choices corresponding to the\ngrammaticalfunctionofthepronoun.\nTable 12: CrowS-Pairs. We compare the level of bi- In Table 13, we report the co-reference scores\nases contained in LLaMA-65B with OPT-175B and\nfor the three different pronouns contained in the\nGPT3-175B.Higherscoreindicateshigherbias.\ndataset. Weobservethatourmodelissignificantly\nbetter at performing co-reference resolution for\nthe “their/them/someone” pronouns than for the\n5.2 CrowS-Pairs “her/her/she”and“his/him/he”pronouns. Asimi-\nlar observation was made in previous work (Rae\nWeevaluatethebiasesinourmodelontheCrowS-\net al., 2021; Hoffmann et al., 2022), and is likely\nPairs(Nangiaetal.,2020). Thisdatasetallowsto\nindicativeofgenderbias. Indeed,inthecaseofthe\nmeasure biases in 9 categories: gender, religion,\n“her/her/she”and“his/him/he”pronouns,themodel\nrace/color,sexualorientation,age,nationality,dis-\nisprobablyusingthemajoritygenderoftheoccu-\nability,physicalappearanceandsocioeconomicsta-\npationtoperformco-referenceresolution,instead\ntus. Eachexampleiscomposedofastereotypeand\nofusingtheevidenceofthesentence.\nan anti-stereotype, we measure the model prefer-\nTo further investigate this hypothesis, we look\nence for the stereotypical sentence using the per-\nat the set of “gotcha” cases for the “her/her/she”\nplexity of both sentences in a zero-shot setting.\nand “his/him/he” pronouns in the WinoGender\nHigherscoresthusindicatehigherbias. Wecom-\ndataset. Theses cases correspond to sentences in\nparewithGPT-3andOPT-175BinTable12.\nwhich the pronoun does not match the majority\nLLaMA compares slightly favorably to both\ngender of the occupation, and the occupation is\nmodels on average. Our model is particularly bi-\nthe correct answer. In Table 13, we observe that\nasedinthereligioncategory(+10%comparedto\nourmodel,LLaMA-65B,makesmoreerrorsonthe\nOPT-175B), followed by age and gender. We ex-\ngotcha examples, clearly showing that it capture\npectthesebiasestocomefromCommonCrawlde-\nsocietal biases related to gender and occupation.\nspitemultiplefilteringsteps.\nThe drop of performance exists for “her/her/she”\n5.3 WinoGender and“his/him/he”pronouns,whichisindicativeof\nbiasesregardlessofgender.\nTo further investigate the biases of our model on\nthe gender category, we look at the WinoGender\n5.4 TruthfulQA\nbenchmark(Rudingeretal.,2018),aco-reference\nresolutiondataset. WinoGenderismadeofWino- TruthfulQA(Linetal.,2021)aimstomeasurethe\ngradschema,andbiasesareevaluatedbydetermin- truthfulnessofamodel,i.e.,itsabilitytoidentify\ningifamodelco-referenceresolutionperformance when a claim is true. Lin et al. (2021) consider\nisimpactedbythegenderofthepronoun. thedefinitionof“true”inthesenseof“literaltruth\nMore precisely, each sentence has three men- abouttherealworld”,andnotclaimsthatareonly\ntions: an “occupation”, a “participant”, and a true in the context of a belief system or tradition.\n“pronoun” where the pronoun is co-referencing Thisbenchmarkcanevaluatetherisksofamodel\neither the occupation or participant. We prompt to generate misinformation or false claims. The\nthe model to determine the co-reference relation questionsarewrittenindiversestyle,cover38cat-\nand measure if it does so correctly according to egoriesandaredesignedtobeadversarial.",
    "tables": [],
    "text_stats": {
      "word_count": 433,
      "char_count": 4043,
      "line_count": 79,
      "table_count": 0
    }
  },
  {
    "page_number": 10,
    "text": "wherewesetthePowerUsageEffectiveness(PUE)\n7B 13B 33B 65B\nat1.1. Theresultingcarbonemissiondependson\nAll 66.0 64.7 69.0 77.5\nthelocationofthedatacenterusedtotrainthenet-\nher/her/she 65.0 66.7 66.7 78.8 work. Forinstance,BLOOMusesagridthatemits\nhis/him/he 60.8 62.5 62.1 72.1 0.057 kg CO 2 eq/KWh leading to 27 tCO 2 eq and\ntheir/them/someone 72.1 65.0 78.3 81.7 OPTagridthatemits0.231kgCO 2 eq/KWh,lead-\ningto82tCO eq. Inthisstudy,weareinterestedin\nher/her/she(gotcha) 64.2 65.8 61.7 75.0 2\ncomparingthecostincarbonemissionoftraining\nhis/him/he(gotcha) 55.0 55.8 55.8 63.3\nof these models if they were trained in the same\ndata center. Hence, we do not take the location\nTable 13: WinoGender. Co-reference resolution ac-\nof data center in consideration, and use, instead,\ncuracy for the LLaMA models, for different pronouns\n(“her/her/she”and“his/him/he”). Weobservethatour theUSnationalaveragecarbonintensityfactorof\nmodelsobtainbetterperformanceon“their/them/some- 0.385kgCO eq/KWh. Thisleadstothefollowing\n2\none’pronounsthanon“her/her/she”and“his/him/he’, formulaforthetonsofcarbonemissions:\nwhichislikelyindicativeofbiases.\ntCO eq = MWh×0.385.\n2\nTruthful Truthful*Inf\nWeapplythesameformulatoOPTandBLOOM\n1.3B 0.31 0.19\nforfaircomparison. ForOPT,weassumetraining\nGPT-3 6B 0.22 0.19\nrequired34dayson992A100-80B(seetheirlogs4).\n175B 0.28 0.25\nFinally,weestimatethatweused2048A100-80GB\n7B 0.33 0.29 foraperiodofapproximately5monthstodevelop\n13B 0.47 0.41 ourmodels. Thismeansthatdevelopingthesemod-\nLLaMA\n33B 0.52 0.48 elswouldhavecostaround2,638MWhunderour\n65B 0.57 0.53 assumptions,andatotalemissionof1,015tCO eq.\n2\nWe hope that releasing these models will help to\nTable14: TruthfulQA.Wereportthefractionoftruth- reducefuturecarbonemissionsincethetrainingis\nfulandtruthful*informativeanswers,asscoredbyspe- alreadydone,andsomeofthemodelsarerelatively\ncially trained models via the OpenAI API. We follow\nsmallandcanberunonasingleGPU.\ntheQApromptstyleusedinOuyangetal.(2022),and\nreporttheperformanceofGPT-3fromthesamepaper.\n7 Relatedwork\nLanguage models are probability distributions\nIn Table 14, we report the performance of our\nover sequences of words, tokens or charac-\nmodelsonbothquestionstomeasuretruthfulmod-\nters(Shannon,1948,1951). Thistask,oftenframed\nelsandtheintersectionoftruthfulandinformative.\nasnexttokenprediction,haslongbeenconsidereda\nCompared to GPT-3, our model scores higher in\ncoreprobleminnaturallanguageprocessing(Bahl\nbothcategories,buttherateofcorrectanswersis\netal.,1983;Brownetal.,1990). BecauseTuring\nstilllow,showingthatourmodelislikelytohallu-\n(1950)proposedtomeasuremachineintelligence\ncinateincorrectanswers.\nby using language through the “imitation game”,\nlanguagemodelinghasbeenproposedasabench-\n6 Carbonfootprint\nmarktomeasureprogresstowardartificialintelli-\nThetrainingofourmodelshaveconsumedamas- gence(Mahoney,1999).\nsive quantity of energy, responsible for the emis-\nArchitecture. Traditionally, language models\nsionofcarbondioxide. Wefollowtherecentliter-\nwere based on n-gram count statistics (Bahl\natureonthesubjectandbreakdownboththetotal\net al., 1983), and various smoothing techniques\nenergyconsumptionandtheresultingcarbonfoot-\nwere proposed to improve the estimation of rare\nprintinTable15. WefollowaformulaforWuetal.\nevents(Katz,1987;KneserandNey,1995). Inthe\n(2022) to estimate the Watt-hour, Wh, needed to\npasttwodecades,neuralnetworkshavebeensuc-\ntrainamodel, aswellasthe tonsofcarbonemis-\ncessfully applied to the language modelling task,\nsions,tCO eq. FortheWh,weusetheformula:\n2\n4https://github.com/facebookresearch/metaseq/\nWh = GPU-h×(GPUpowerconsumption)×PUE, tree/main/projects/OPT/chronicles",
    "tables": [],
    "text_stats": {
      "word_count": 306,
      "char_count": 3648,
      "line_count": 83,
      "table_count": 0
    }
  },
  {
    "page_number": 11,
    "text": "GPUPower Totalpower Carbonemitted\nGPUType GPU-hours\nconsumption consumption (tCO eq)\n2\nOPT-175B A100-80GB 400W 809,472 356MWh 137\nBLOOM-175B A100-80GB 400W 1,082,880 475MWh 183\nLLaMA-7B A100-80GB 400W 82,432 36MWh 14\nLLaMA-13B A100-80GB 400W 135,168 59MWh 23\nLLaMA-33B A100-80GB 400W 530,432 233MWh 90\nLLaMA-65B A100-80GB 400W 1,022,362 449MWh 173\nTable15: Carbonfootprintoftrainingdifferentmodelsinthesamedatacenter. WefollowWuetal.(2022)\nto compute carbon emission of training OPT, BLOOM and our models in the same data center. For the power\nconsumption of a A100-80GB, we take the thermal design power for NVLink systems, that is 400W. We take a\nPUEof1.1andacarbonintensityfactorsetatthenationalUSaverageof0.385kgCO eperKWh.\n2\nstartingfromfeedforwardmodels(Bengioetal., 2022),Gopher(Raeetal.,2021),Chinchilla(Hoff-\n2000), recurrent neural networks (Elman, 1990; mannetal.,2022),PaLM(Chowdheryetal.,2022),\nMikolovetal.,2010)andLSTMs(Hochreiterand OPT (Zhang et al., 2022), and GLM (Zeng et al.,\nSchmidhuber,1997;Graves,2013). Morerecently, 2022). Hestnessetal.(2017)andRosenfeldetal.\ntransformernetworks,basedonself-attention,have (2019)studiedtheimpactofscalingontheperfor-\nledtoimportantimprovements,especiallyforcap- manceofdeeplearningmodels,showingtheexis-\nturing long range dependencies (Vaswani et al., tenceofpowerlawsbetweenthemodelanddataset\n2017;Radfordetal.,2018;Daietal.,2019). sizesandtheperformanceofthesystem. Kaplan\net al. (2020) derived power laws specifically for\nScaling. There is a long history of scaling for transformer based language models, which were\nlanguage models, for both the model and dataset laterrefinedbyHoffmannetal.(2022),byadapting\nsizes. Brantsetal.(2007)showedthebenefitsof the learning rate schedule when scaling datasets.\nusinglanguagemodelstrainedon2trilliontokens, Finally,Weietal.(2022)studiedtheeffectofscal-\nresultingin300billionn-grams,onthequalityof ingontheabilitiesoflargelanguagemodels.\nmachine translation. While this work relied on a\n8 Conclusion\nsimplesmoothingtechnique,calledStupidBackoff,\nHeafield et al. (2013) later showed how to scale In this paper, we presented a series of language\nKneser-Ney smoothing to Web-scale data. This models that are released openly, and competitive\nallowedtotraina5-grammodelon975billionsto- with state-of-the-art foundation models. Most\nkens from CommonCrawl, resulting in a model notably, LLaMA-13B outperforms GPT-3 while\nwith 500 billions n-grams (Buck et al., 2014). beingmorethan10×smaller,andLLaMA-65Bis\nChelba et al. (2013) introduced the One Billion competitivewithChinchilla-70BandPaLM-540B.\nWord benchmark,alargescaletrainingdatasetto Unlikepreviousstudies,weshowthatitispossible\nmeasuretheprogressoflanguagemodels. toachievestate-of-the-artperformancebytraining\nInthecontextofneurallanguagemodels,Joze- exclusively on publicly available data, without\nfowicz et al. (2016) obtained state-of-the-art re- resorting to proprietary datasets. We hope that\nsults on the Billion Word benchmark by scaling releasingthesemodelstotheresearchcommunity\nLSTMs to 1 billion parameters. Later, scaling willacceleratethedevelopmentoflargelanguage\ntransformers lead to improvement on many NLP models, and help efforts to improve their robust-\ntasks. NotablemodelsincludeBERT(Devlinetal., nessandmitigateknownissuessuchastoxicityand\n2018), GPT-2 (Radford et al., 2019), Megatron- bias. Additionally, we observed like Chung et al.\nLM (Shoeybi et al., 2019), and T5 (Raffel et al., (2022)thatfinetuningthesemodelsoninstructions\n2020). A significant breakthrough was obtained lead to promising results, and we plan to further\nwith GPT-3 (Brown et al., 2020), a model with investigatethisinfuturework. Finally,weplanto\n175 billion parameters. This lead to a series of releaselargermodelstrainedonlargerpretraining\nLargeLanguageModels,suchasJurassic-1(Lieber corporainthefuture,sincewehaveseenaconstant\netal.,2021),Megatron-TuringNLG(Smithetal., improvementinperformanceaswewerescaling.",
    "tables": [],
    "text_stats": {
      "word_count": 379,
      "char_count": 3966,
      "line_count": 53,
      "table_count": 0
    }
  },
  {
    "page_number": 12,
    "text": "Acknowledgements Askell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nWethankDanielHaziza,FranciscoMassa,Jeremy Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nReizenstein,ArtemKorenev,andPatrickLabatut Clemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nfrom the xformers team. We thank Susan Zhang\nChess, Jack Clark, Christopher Berner, Sam Mc-\nand Stephen Roller for their support on data\nCandlish, Alec Radford, Ilya Sutskever, and Dario\ndeduplication. WethankLucaWehrstedt,Vegard Amodei.2020. Languagemodelsarefew-shotlearn-\nMella, and Pierre-Emmanuel Mazaré for their ers.\nsupport on training stability. We thank Shubho\nChristianBuck,KennethHeafield,andBasVanOoyen.\nSengupta,KalyanSaladi,andalltheAIinfrateam\n2014. N-gramcountsandlanguagemodelsfromthe\nfortheirsupport. WethankJaneYuforherinput commoncrawl. InLREC,volume2,page4.\non evaluation. We thank Yongyi Hu for his help\nondatacollection. CiprianChelba,TomasMikolov,MikeSchuster,QiGe,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson.2013. Onebillionwordbenchmarkformeasur-\ningprogressinstatisticallanguagemodeling. arXiv\nReferences\npreprintarXiv:1312.3005.\nJacobAustin,AugustusOdena,MaxwellNye,Maarten\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nBosma, Henryk Michalewski, David Dohan, Ellen\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nJiang, Carrie Cai, Michael Terry, Quoc Le, and\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nCharlesSutton.2021. Programsynthesiswithlarge\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nlanguagemodels.\nKrueger, MichaelPetrov, HeidyKhlaaf, GirishSas-\nLalit R Bahl, Frederick Jelinek, and Robert L Mercer. try, Pamela Mishkin, Brooke Chan, Scott Gray,\n1983. A maximum likelihood approach to continu- NickRyder,MikhailPavlov,AletheaPower,Lukasz\nous speech recognition. IEEE transactions on pat- Kaiser, Mohammad Bavarian, Clemens Winter,\ntern analysis and machine intelligence, pages 179– Philippe Tillet, Felipe Petroski Such, Dave Cum-\n190. mings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-Voss, William Hebgen\nYoshuaBengio,RéjeanDucharme,andPascalVincent. Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\n2000. A neural probabilistic language model. Ad- Tang,IgorBabuschkin,SuchirBalaji,ShantanuJain,\nvancesinneuralinformationprocessingsystems,13. William Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Josh Achiam, Vedant Misra, Evan\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin\nMorikawa, Alec Radford, Matthew Knight, Miles\nChoi, et al. 2020. Piqa: Reasoning about physi-\nBrundage, Mira Murati, Katie Mayer, Peter Welin-\ncal commonsense in natural language. In Proceed-\nder,BobMcGrew,DarioAmodei,SamMcCandlish,\ningsoftheAAAIconferenceonartificialintelligence,\nIlyaSutskever,andWojciechZaremba.2021. Eval-\npages7432–7439.\nuatinglargelanguagemodelstrainedoncode.\nSidBlack,StellaBiderman,EricHallahan,QuentinAn-\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nthony,LeoGao,LaurenceGolding,HoraceHe,Con-\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nnorLeahy,KyleMcDonell,JasonPhang,etal.2022.\nPaul Barham, Hyung Won Chung, Charles Sutton,\nGpt-neox-20b: An open-source autoregressive lan-\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nguagemodel. arXivpreprintarXiv:2204.06745.\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nThorsten Brants, Ashok C. Popat, Peng Xu, Franz J.\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nOch,andJeffreyDean.2007. Largelanguagemod-\nHutchinson, Reiner Pope, James Bradbury, Jacob\nels in machine translation. In Proceedings of the\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng\n2007JointConferenceonEmpiricalMethodsinNat-\nYin, Toju Duke, Anselm Levskaya, Sanjay Ghe-\nural Language Processing and Computational Nat-\nmawat, Sunipa Dev, Henryk Michalewski, Xavier\nural Language Learning (EMNLP-CoNLL), pages\nGarcia, Vedant Misra, Kevin Robinson, Liam Fe-\n858–867, Prague, Czech Republic. Association for\ndus, Denny Zhou, Daphne Ippolito, David Luan,\nComputationalLinguistics.\nHyeontaekLim,BarretZoph,AlexanderSpiridonov,\nPeter F Brown, John Cocke, Stephen A Della Pietra, RyanSepassi,DavidDohan,ShivaniAgrawal,Mark\nVincent J Della Pietra, Frederick Jelinek, John Laf- Omernick,AndrewM.Dai,ThanumalayanSankara-\nferty,RobertLMercer,andPaulSRoossin.1990. A narayana Pillai, Marie Pellat, Aitor Lewkowycz,\nstatisticalapproachtomachinetranslation. Compu- Erica Moreira, Rewon Child, Oleksandr Polozov,\ntationallinguistics,16(2):79–85. KatherineLee,ZongweiZhou,XuezhiWang,Bren-\nnanSaeta,MarkDiaz,OrhanFirat,MicheleCatasta,\nTomB.Brown,BenjaminMann,NickRyder,Melanie Jason Wei, Kathy Meier-Hellstern, Douglas Eck,\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Jeff Dean, Slav Petrov, and Noah Fiedel. 2022.\nNeelakantan,PranavShyam,GirishSastry,Amanda Palm: Scalinglanguagemodelingwithpathways.",
    "tables": [],
    "text_stats": {
      "word_count": 496,
      "char_count": 4889,
      "line_count": 89,
      "table_count": 0
    }
  },
  {
    "page_number": 13,
    "text": "Hyung Won Chung, Le Hou, S. Longpre, Barret Jason Phang, Laria Reynolds, Eric Tang, Anish\nZoph, Yi Tay, William Fedus, Eric Li, Xuezhi Thite,BenWang,KevinWang,andAndyZou.2021.\nWang, Mostafa Dehghani, Siddhartha Brahma, Al- A framework for few-shot language model evalua-\nbert Webson, Shixiang Shane Gu, Zhuyun Dai, tion.\nMirac Suzgun, Xinyun Chen, Aakanksha Chowdh-\nery, Dasha Valter, Sharan Narang, Gaurav Mishra, Samuel Gehman, Suchin Gururangan, Maarten Sap,\nAdamsWeiYu,VincentZhao,YanpingHuang,An- Yejin Choi, and Noah A Smith. 2020. Realtoxici-\ndrew M. Dai, Hongkun Yu, Slav Petrov, Ed Huai typrompts: Evaluating neural toxic degeneration in\nhsin Chi, Jeff Dean, Jacob Devlin, Adam Roberts, languagemodels. arXivpreprintarXiv:2009.11462.\nDenny Zhou, Quoc Le, and Jason Wei. 2022. Scal-\ning instruction-finetuned language models. arXiv Alex Graves. 2013. Generating sequences with\npreprintarXiv:2210.11416. recurrent neural networks. arXiv preprint\narXiv:1308.0850.\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\nTom Kwiatkowski, Michael Collins, and Kristina\nKennethHeafield,IvanPouzyrevsky,JonathanHClark,\nToutanova. 2019. Boolq: Exploring the surprising\nandPhilippKoehn.2013. Scalablemodifiedkneser-\ndifficultyofnaturalyes/noquestions. arXivpreprint\nney language model estimation. In Proceedings of\narXiv:1905.10044.\nthe51stAnnualMeetingoftheAssociationforCom-\nputational Linguistics (Volume 2: Short Papers),\nPeterClark,IsaacCowhey,OrenEtzioni,TusharKhot,\npages690–696.\nAshish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. 2018. Think you have solved question an-\nswering? tryarc,theai2reasoningchallenge. arXiv Dan Hendrycks, Collin Burns, Steven Basart, Andy\npreprintarXiv:1803.05457. Zou,MantasMazeika,DawnSong,andJacobStein-\nhardt.2020. Measuringmassivemultitasklanguage\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, understanding. arXivpreprintarXiv:2009.03300.\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro DanHendrycks,CollinBurns,SauravKadavath,Akul\nNakano,etal.2021. Trainingverifierstosolvemath Arora, Steven Basart, Eric Tang, Dawn Song, and\nwordproblems. arXivpreprintarXiv:2110.14168. Jacob Steinhardt. 2021. Measuring mathematical\nproblem solving with the math dataset. arXiv\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car- preprintarXiv:2103.03874.\nbonell, Quoc V Le, and Ruslan Salakhutdinov.\n2019. Transformer-xl: Attentive language mod-\nJoel Hestness, Sharan Narang, Newsha Ardalani, Gre-\nels beyond a fixed-length context. arXiv preprint\ngory Diamos, Heewoo Jun, Hassan Kianinejad,\narXiv:1901.02860.\nMd Patwary, Mostofa Ali, Yang Yang, and Yanqi\nZhou. 2017. Deep learning scaling is predictable,\nTri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra,\nempirically. arXivpreprintarXiv:1712.00409.\nandChristopherRé.2022. Flashattention: Fastand\nmemory-efficient exact attention with io-awareness.\nSepp Hochreiter and Jürgen Schmidhuber. 1997.\narXivpreprintarXiv:2205.14135.\nLong short-term memory. Neural computation,\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and 9(8):1735–1780.\nKristinaToutanova.2018. Bert:Pre-trainingofdeep\nbidirectional transformers for language understand- JordanHoffmann,SebastianBorgeaud,ArthurMensch,\ning. arXivpreprintarXiv:1810.04805. Elena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiegodeLasCasas,LisaAnneHendricks,Johannes\nJeffreyLElman.1990. Findingstructureintime. Cog- Welbl, Aidan Clark, Tom Hennigan, Eric Noland,\nnitivescience,14(2):179–211. Katie Millican, George van den Driessche, Bogdan\nDamoc, Aurelia Guy, Simon Osindero, Karen Si-\nDaniel Fried, Armen Aghajanyan, Jessy Lin, Sida\nmonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals,\nWang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-\nandLaurentSifre.2022. Trainingcompute-optimal\ntau Yih, Luke Zettlemoyer, and Mike Lewis. 2022.\nlargelanguagemodels.\nIncoder: A generative model for code infilling and\nsynthesis. arXivpreprintarXiv:2204.05999.\nSrinivasanIyer,XiVictoriaLin,RamakanthPasunuru,\nTodorMihaylov,DánielSimig,PingYu,KurtShus-\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\nter,TianluWang,QingLiu,PunitSinghKoura,etal.\ning, Travis Hoppe, Charles Foster, Jason Phang,\n2022. Opt-iml: Scaling language model instruc-\nHorace He, Anish Thite, Noa Nabeshima, Shawn\ntionmetalearningthroughthelensofgeneralization.\nPresser, and Connor Leahy. 2020. The Pile: An\narXivpreprintarXiv:2212.12017.\n800gbdatasetofdiversetextforlanguagemodeling.\narXivpreprintarXiv:2101.00027.\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nLeo Gao, Jonathan Tow, Stella Biderman, Sid Black, Zettlemoyer.2017. Triviaqa: Alargescaledistantly\nAnthony DiPofi, Charles Foster, Laurence Golding, supervisedchallengedatasetforreadingcomprehen-\nJeffrey Hsu, Kyle McDonell, Niklas Muennighoff, sion. arXivpreprintarXiv:1705.03551.",
    "tables": [],
    "text_stats": {
      "word_count": 499,
      "char_count": 4766,
      "line_count": 84,
      "table_count": 0
    }
  },
  {
    "page_number": 14,
    "text": "RafalJozefowicz,OriolVinyals,MikeSchuster,Noam Ilya Loshchilov and Frank Hutter. 2017. Decou-\nShazeer, and Yonghui Wu. 2016. Exploring pled weight decay regularization. arXiv preprint\nthe limits of language modeling. arXiv preprint arXiv:1711.05101.\narXiv:1602.02410.\nMatthewVMahoney.1999. Textcompressionasatest\nJared Kaplan, Sam McCandlish, Tom Henighan, forartificialintelligence. AAAI/IAAI,970.\nTomBBrown,BenjaminChess,RewonChild,Scott\nGray,AlecRadford,JeffreyWu,andDarioAmodei. TodorMihaylov,PeterClark,TusharKhot,andAshish\n2020. Scaling laws for neural language models. Sabharwal.2018. Canasuitofarmorconductelec-\narXivpreprintarXiv:2001.08361. tricity?anewdatasetforopenbookquestionanswer-\ning. arXivpreprintarXiv:1809.02789.\nSlava Katz. 1987. Estimation of probabilities from\nsparse data for the language model component of a Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan\nspeech recognizer. IEEE transactions on acoustics, Cernocky`, and Sanjeev Khudanpur. 2010. Recur-\nspeech,andsignalprocessing,35(3):400–401. rent neural network based language model. In In-\nterspeech,pages1045–1048.Makuhari.\nReinhard Kneser and Hermann Ney. 1995. Improved\nbacking-offform-gramlanguagemodeling. In1995 Nikita Nangia, Clara Vania, Rasika Bhalerao, and\ninternational conference on acoustics, speech, and Samuel R. Bowman. 2020. CrowS-pairs: A chal-\nsignalprocessing,volume1,pages181–184.IEEE. lengedatasetformeasuringsocialbiasesinmasked\nlanguagemodels. InEMNLP2020.\nVijay Korthikanti, Jared Casper, Sangkug Lym,\nLawrenceMcAfee,MichaelAndersch,Mohammad Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu,\nShoeybi, andBryanCatanzaro.2022. Reducingac- Huan Wang, Yingbo Zhou, Silvio Savarese, and\ntivation recomputation in large transformer models. CaimingXiong.2022. Codegen: Anopenlargelan-\narXivpreprintarXiv:2205.05198. guagemodelforcodewithmulti-turnprogramsyn-\nthesis. arXivpreprintarXiv:2203.13474.\nTakuKudoandJohnRichardson.2018. Sentencepiece:\nA simple and language independent subword tok- Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nenizer and detokenizer for neural text processing. CarrollWainwright,PamelaMishkin,ChongZhang,\narXivpreprintarXiv:1808.06226. SandhiniAgarwal,KatarinaSlama,AlexGray,John\nSchulman,JacobHilton,FraserKelton,LukeMiller,\nKeitaKurita,NidhiVyas,AyushPareek,AlanWBlack,\nMaddie Simens, Amanda Askell, Peter Welinder,\nand Yulia Tsvetkov. 2019. Quantifying social bi-\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nasesincontextualwordrepresentations. In1stACL\nTraining language models to follow instructions\nWorkshop on Gender Bias for Natural Language\nwithhumanfeedback. InAdvancesinNeuralInfor-\nProcessing.\nmationProcessingSystems.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nMarkus N Rabe and Charles Staats. 2021. Self-\nfield, MichaelCollins, AnkurParikh, ChrisAlberti,\nattention does not need o(n2) memory. arXiv\nDanielle Epstein, Illia Polosukhin, Jacob Devlin,\npreprintarXiv:2112.05682.\nKentonLee,etal.2019. Naturalquestions: abench-\nmarkforquestionansweringresearch. Transactions\nAlecRadford,KarthikNarasimhan,TimSalimans,Ilya\nof the Association for Computational Linguistics,\nSutskever, et al. 2018. Improving language under-\n7:453–466.\nstandingbygenerativepre-training.\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nandEduardHovy.2017. Race: Large-scalereading\nDario Amodei, Ilya Sutskever, et al. 2019. Lan-\ncomprehension dataset from examinations. arXiv\nguage models are unsupervised multitask learners.\npreprintarXiv:1704.04683.\nOpenAIblog,1(8):9.\nAitor Lewkowycz, Anders Johan Andreassen,\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie\nDavid Dohan, Ethan Dyer, Henryk Michalewski,\nMillican, Jordan Hoffmann, Francis Song, John\nVinay Venkatesh Ramasesh, Ambrose Slone, Cem\nAslanides, Sarah Henderson, Roman Ring, Susan-\nAnil, Imanol Schlag, Theo Gutman-Solo, Yuhuai\nnah Young, Eliza Rutherford, Tom Hennigan, Ja-\nWu, Behnam Neyshabur, Guy Gur-Ari, and Vedant\ncobMenick,AlbinCassirer,RichardPowell,George\nMisra. 2022. Solving quantitative reasoning prob-\nvan den Driessche, Lisa Anne Hendricks, Mari-\nlemswithlanguagemodels. InAdvancesinNeural\nbeth Rauh, Po-Sen Huang, Amelia Glaese, Jo-\nInformationProcessingSystems.\nhannes Welbl, Sumanth Dathathri, Saffron Huang,\nJonathan Uesato, John Mellor, Irina Higgins, An-\nOpher Lieber, Or Sharir, Barak Lenz, and Yoav\ntonia Creswell, Nat McAleese, Amy Wu, Erich\nShoham. 2021. Jurassic-1: Technical details and\nevaluation. WhitePaper.AI21Labs,1. Elsen, Siddhant Jayakumar, Elena Buchatskaya,\nDavid Budden, Esme Sutherland, Karen Simonyan,\nStephanie Lin, Jacob Hilton, and Owain Evans. 2021. Michela Paganini, Laurent Sifre, Lena Martens,\nTruthfulqa: Measuring how models mimic human Xiang Lorraine Li, Adhiguna Kuncoro, Aida Ne-\nfalsehoods. arXivpreprintarXiv:2109.07958. matzadeh, Elena Gribovskaya, Domenic Donato,",
    "tables": [],
    "text_stats": {
      "word_count": 495,
      "char_count": 4868,
      "line_count": 88,
      "table_count": 0
    }
  },
  {
    "page_number": 15,
    "text": "Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Emily Sheng, Kai-Wei Chang, Premkumar Natarajan,\nLespiau, Maria Tsimpoukelli, Nikolai Grigorev, and Nanyun Peng. 2019. The woman worked as a\nDoug Fritz, Thibault Sottiaux, Mantas Pajarskas, babysitter: Onbiasesinlanguagegeneration. arXiv\nToby Pohlen, Zhitao Gong, Daniel Toyama, Cy- preprintarXiv:1909.01326.\nprien de Masson d’Autume, Yujia Li, Tayfun Terzi,\nVladimir Mikulik, Igor Babuschkin, Aidan Clark, Mohammad Shoeybi, Mostofa Patwary, Raul Puri,\nDiego de Las Casas, Aurelia Guy, Chris Jones, Patrick LeGresley, Jared Casper, and Bryan Catan-\nJames Bradbury, Matthew Johnson, Blake Hecht- zaro.2019. Megatron-lm: Trainingmulti-billionpa-\nman,LauraWeidinger,IasonGabriel,WilliamIsaac, rameter language models using model parallelism.\nEdLockhart, SimonOsindero, LauraRimell, Chris\narXivpreprintarXiv:1909.08053.\nDyer, Oriol Vinyals, Kareem Ayoub, Jeff Stan-\nShaden Smith, Mostofa Patwary, Brandon Norick,\nway, Lorrayne Bennett, Demis Hassabis, Koray\nPatrick LeGresley, Samyam Rajbhandari, Jared\nKavukcuoglu, and Geoffrey Irving. 2021. Scal-\nCasper, Zhun Liu, Shrimai Prabhumoye, George\ninglanguagemodels: Methods, analysis&insights\nZerveas, Vijay Korthikanti, Elton Zhang, Rewon\nfromtraininggopher.\nChild,RezaYazdaniAminabadi,JulieBernauer,Xia\nSong, Mohammad Shoeybi, Yuxiong He, Michael\nColinRaffel,NoamShazeer,AdamRoberts,Katherine\nHouston, Saurabh Tiwary, and Bryan Catanzaro.\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\n2022. Using deepspeed and megatron to train\nWeiLi, andPeterJLiu.2020. Exploringthelimits\nmegatron-turing nlg 530b, a large-scale generative\nof transfer learning with a unified text-to-text trans-\nlanguagemodel.\nformer. TheJournalofMachineLearningResearch,\n21(1):5485–5551. Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha,\nBo Wen, and Yunfeng Liu. 2021. Roformer: En-\nJonathan S Rosenfeld, Amir Rosenfeld, Yonatan Be- hancedtransformerwithrotarypositionembedding.\nlinkov,andNirShavit.2019. Aconstructivepredic- arXivpreprintarXiv:2104.09864.\ntionofthegeneralizationerroracrossscales. arXiv\npreprintarXiv:1909.12673. Romal Thoppilan, Daniel De Freitas, Jamie Hall,\nNoam Shazeer, Apoorv Kulshreshtha, Heng-Tze\nCheng,AliciaJin,TaylorBos,LeslieBaker,YuDu,\nRachel Rudinger, Jason Naradowsky, Brian Leonard,\nYaGuang Li, Hongrae Lee, Huaixiu Steven Zheng,\nand Benjamin Van Durme. 2018. Gender bias in\nAminGhafouri,MarceloMenegali,YanpingHuang,\ncoreferenceresolution. InNAACL-HLT2018.\nMaxim Krikun, Dmitry Lepikhin, James Qin, De-\nhao Chen, Yuanzhong Xu, Zhifeng Chen, Adam\nKeisukeSakaguchi,RonanLeBras,ChandraBhagavat-\nRoberts, Maarten Bosma, Vincent Zhao, Yanqi\nula, and Yejin Choi. 2021. Winogrande: An adver-\nZhou, Chung-Ching Chang, Igor Krivokon, Will\nsarialwinogradschemachallengeatscale. Commu-\nRusch, Marc Pickett, Pranesh Srinivasan, Laichee\nnicationsoftheACM,64(9):99–106.\nMan, Kathleen Meier-Hellstern, Meredith Ringel\nMorris, Tulsee Doshi, Renelito Delos Santos, Toju\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan\nDuke, Johnny Soraker, Ben Zevenbergen, Vinod-\nLeBras, and Yejin Choi. 2019. Socialiqa: Com-\nkumar Prabhakaran, Mark Diaz, Ben Hutchinson,\nmonsensereasoningaboutsocialinteractions. arXiv\nKristen Olson, Alejandra Molina, Erin Hoffman-\npreprintarXiv:1904.09728.\nJohn, Josh Lee, Lora Aroyo, Ravi Rajakumar,\nAlenaButryna,MatthewLamm,ViktoriyaKuzmina,\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nJoe Fenton, Aaron Cohen, Rachel Bernstein, Ray\nlie Pavlick, Suzana Ilic´, Daniel Hesslow, Ro-\nKurzweil,BlaiseAguera-Arcas,ClaireCui,Marian\nmanCastagné,AlexandraSashaLuccioni,François\nCroak, Ed Chi, and Quoc Le. 2022. Lamda: Lan-\nYvon, MatthiasGallé, etal.2022. Bloom: A176b-\nguagemodelsfordialogapplications.\nparameteropen-accessmultilinguallanguagemodel.\narXivpreprintarXiv:2211.05100. A. M. Turing. 1950. Computing Machinery and Intel-\nligence. [Oxford University Press, Mind Associa-\nRico Sennrich, Barry Haddow, and Alexandra Birch. tion].\n2015. Neuralmachinetranslationofrarewordswith\nsubwordunits. arXivpreprintarXiv:1508.07909. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nClaude E Shannon. 1948. A mathematical theory of\ncommunication. The Bell system technical journal, you need. In Advances in Neural Information Pro-\ncessingSystems30,pages5998–6008.\n27(3):379–423.\nBen Wang and Aran Komatsuzaki. 2021. GPT-J-\nClaude E Shannon. 1951. Prediction and entropy\n6B: A 6 Billion Parameter Autoregressive Lan-\nof printed english. Bell system technical journal, guageModel. https://github.com/kingoflolz/\n30(1):50–64. mesh-transformer-jax.\nNoam Shazeer. 2020. Glu variants improve trans- Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc\nformer. arXivpreprintarXiv:2002.05202. Le,EdChi,SharanNarang,AakankshaChowdhery,",
    "tables": [],
    "text_stats": {
      "word_count": 522,
      "char_count": 4817,
      "line_count": 91,
      "table_count": 0
    }
  },
  {
    "page_number": 16,
    "text": "and Denny Zhou. 2022. Self-consistency improves\nchainofthoughtreasoninginlanguagemodels.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaartenBosma,DennyZhou,DonaldMetzler,etal.\n2022. Emergent abilities of large language models.\narXivpreprintarXiv:2206.07682.\nGuillaumeWenzek,Marie-AnneLachaux,AlexisCon-\nneau, Vishrav Chaudhary, Francisco Guzmán, Ar-\nmandJoulin,andEdouardGrave.2020. CCNet:Ex-\ntractinghighqualitymonolingualdatasetsfromweb\ncrawldata. InLanguageResourcesandEvaluation\nConference.\nCarole-Jean Wu, Ramya Raghavendra, Udit Gupta,\nBilge Acun, Newsha Ardalani, Kiwan Maeng, Glo-\nria Chang, Fiona Aga, Jinshi Huang, Charles Bai,\net al. 2022. Sustainable ai: Environmental implica-\ntions, challengesandopportunities. Proceedingsof\nMachineLearningandSystems,4:795–813.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019. Hellaswag: Can a\nmachinereallyfinishyoursentence? arXivpreprint\narXiv:1905.07830.\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,\nHanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\nWendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan\nMa,YufeiXue,JidongZhai,WenguangChen,Peng\nZhang, Yuxiao Dong, and Jie Tang. 2022. Glm-\n130b: Anopenbilingualpre-trainedmodel.\nBiao Zhang and Rico Sennrich. 2019. Root mean\nsquarelayernormalization. AdvancesinNeuralIn-\nformationProcessingSystems,32.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe,MoyaChen,ShuohuiChen,ChristopherDe-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al.\n2022. Opt: Open pre-trained transformer language\nmodels. arXivpreprintarXiv:2205.01068.",
    "tables": [],
    "text_stats": {
      "word_count": 165,
      "char_count": 1603,
      "line_count": 37,
      "table_count": 0
    }
  },
  {
    "page_number": 17,
    "text": "A QuestionAnswering\nWeevaluateLLaMAonNaturalQuestionsandTriviaQA.ForNaturalQuestionsweusethetestsplitused\nforopen-domainquestionansweringcontaining3610questions. ForTriviaQAweevaluateonthedevset\nofthefilteredset. ThisdiffersfromGPT-3andPaLM,whichevaluateonthetestsetoftheunfilteredset\nforwhichtheonlineevaluationserverisnotavailableanymore5.\nWegenerateanswersusinggreedydecoding,andextractananswerfromthegenerationbystopping\nat the first line break, final dot or comma. Generated answers are evaluated with the standard exact\nmatchmetric: ageneratedanswerisconsideredcorrectifitmatchesanyanswerofthelistofanswers\nafter normalization. For this normalization step we lowercase generated answers and remove articles,\npunctuationandduplicatewhitespaces. Figure3presentsformattedexamplesinthe1-shotsettingfor\nNatural Questions and TriviaQA respectively. In all settings, we preprend the string Answer these\nquestions:\\ntothelistofquestionsandanswers.\nContext→Answerthesequestions: Context→Answerthesequestions:\nQ:Whosangwhowantstobeamillionaireinhighsociety? Q:InScotlandabothy/bothieisa?\nA:FrankSinatra A:House\nQ:Whowrotethebooktheoriginofspecies? Q:TheancientcityofTroyislocatedinwhatmoderncountry?\nA: A:\nTarget→CharlesDarwin Target→Turkey\nFigure3: FormatteddatasetexampleforNaturalQuestions(left)&TriviaQA(right).\n5https://competitions.codalab.org/competitions/17208",
    "tables": [],
    "text_stats": {
      "word_count": 73,
      "char_count": 1364,
      "line_count": 20,
      "table_count": 0
    }
  },
  {
    "page_number": 18,
    "text": "B MMLU\nGPT-3 Gopher Chinchilla LLaMA LLaMA-I\n175B 280B 70B 7B 13B 33B 65B 65B\nAbstractAlgebra STEM 30.0 25.0 31.0 29.0 34.0 32.0 34.0 31.0\nAnatomy STEM 48.0 56.3 70.4 37.0 45.9 51.9 57.8 62.2\nAstronomy STEM 49.0 65.8 73.0 33.6 46.1 61.8 72.4 81.6\nBusinessEthics Other 46.0 70.0 72.0 40.0 45.0 56.0 57.0 72.0\nClinicalKnowledge Other 48.0 67.2 75.1 35.1 45.7 57.4 65.3 69.1\nCollegeBiology STEM 45.0 70.8 79.9 37.5 45.1 58.3 68.8 81.9\nCollegeChemistry STEM 26.0 45.0 51.0 32.0 30.0 45.0 50.0 45.0\nCollegeComputerScience STEM 46.0 49.0 51.0 29.0 39.0 45.0 47.0 51.0\nCollegeMathematics STEM 34.5 37.0 32.0 33.0 32.0 40.0 35.0 36.0\nCollegeMedicine Other 48.0 60.1 66.5 30.6 42.8 52.0 54.3 63.0\nCollegePhysics STEM 28.0 34.3 46.1 26.5 18.6 28.4 36.3 46.1\nComputerSecurity STEM 57.0 65.0 76.0 45.0 65.0 66.0 79.0 79.0\nConceptualPhysics STEM 36.5 49.4 67.2 36.6 41.3 51.5 59.6 66.4\nEconometrics SocialScience 33.0 43.0 38.6 23.7 27.2 35.1 40.4 52.6\nElectricalEngineering STEM 50.0 60.0 62.1 26.9 40.7 49.7 53.8 60.7\nElementaryMathematics STEM 30.0 33.6 41.5 24.3 24.9 36.0 37.8 42.9\nFormalLogic Humanities 29.0 35.7 33.3 27.0 33.3 34.1 44.4 47.6\nGlobalFacts Other 37.0 38.0 39.0 29.0 35.0 35.0 39.0 40.0\nHighSchoolBiology STEM 48.0 71.3 80.3 34.5 52.6 67.7 73.9 82.9\nHighSchoolChemistry STEM 33.0 47.8 58.1 28.1 28.6 41.9 40.4 44.8\nHighSchoolComputerScience STEM 39.0 54.0 58.0 31.0 48.0 60.0 67.0 73.0\nHighSchoolEuropeanHistory Humanities 54.0 72.1 78.8 44.2 61.8 73.9 78.8 86.1\nHighSchoolGeography SocialScience 58.0 76.8 86.4 34.3 54.6 70.7 77.8 87.9\nHighSchoolGovernmentAndPolitics SocialScience 58.0 83.9 91.2 44.6 66.3 82.9 88.1 92.8\nHighSchoolMacroeconomics SocialScience 40.5 65.1 70.5 35.4 44.4 56.9 65.9 69.2\nHighSchoolMathematics STEM 28.0 23.7 31.9 24.8 23.7 27.0 34.4 37.0\nHighSchoolMicroeconomics SocialScience 42.0 66.4 77.7 31.9 47.5 55.5 68.9 78.6\nHighSchoolPhysics STEM 28.0 33.8 36.4 26.5 28.5 35.8 37.1 41.7\nHighSchoolPsychology SocialScience 61.0 81.8 86.6 47.3 60.9 76.2 82.2 87.9\nHighSchoolStatistics STEM 30.5 50.0 58.8 35.2 30.1 45.4 58.3 59.3\nHighSchoolUsHistory Humanities 53.0 78.9 83.3 39.7 58.3 77.9 83.8 90.7\nHighSchoolWorldHistory Humanities 56.0 75.1 85.2 40.9 66.2 79.3 83.1 89.0\nHumanAging Other 50.0 66.4 77.6 40.8 54.7 67.7 69.5 72.2\nHumanSexuality SocialScience 54.0 67.2 86.3 36.6 58.8 64.1 77.9 87.0\nInternationalLaw Humanities 55.5 77.7 90.9 51.2 62.8 72.7 79.3 87.6\nJurisprudence Humanities 55.0 71.3 79.6 38.9 51.9 70.4 73.2 85.2\nLogicalFallacies Humanities 48.0 72.4 80.4 39.3 52.8 68.1 77.3 80.4\nMachineLearning STEM 31.0 41.1 41.1 23.2 31.3 39.3 49.1 52.7\nManagement Other 56.0 77.7 82.5 35.0 66.0 77.7 82.5 83.5\nMarketing Other 60.0 83.3 89.7 46.6 71.8 83.3 85.9 92.7\nMedicalGenetics Other 40.0 69.0 69.0 43.0 52.0 67.0 67.0 68.0\nMiscellaneous Other 60.0 75.7 84.5 42.4 65.4 78.5 82.1 84.3\nMoralDisputes Humanities 44.5 66.8 77.5 40.2 50.9 66.2 72.3 76.9\nMoralScenarios Humanities 26.0 40.2 36.5 24.3 30.1 38.2 48.9 55.9\nNutrition Other 47.0 69.9 77.1 37.6 51.6 62.8 67.3 74.5\nPhilosophy Humanities 51.0 68.8 79.4 39.9 54.0 66.2 74.0 79.1\nPrehistory Humanities 53.0 67.6 81.2 36.1 51.5 67.0 75.3 79.0\nProfessionalAccounting Other 33.0 44.3 52.1 25.9 35.8 43.6 46.5 56.0\nProfessionalLaw Humanities 34.5 44.5 56.5 30.2 38.0 45.9 49.1 54.4\nProfessionalMedicine Other 36.0 64.0 75.4 44.5 50.4 54.0 61.4 70.6\nProfessionalPsychology SocialScience 44.5 68.1 75.7 35.1 47.7 62.9 65.7 71.4\nPublicRelations SocialScience 48.0 71.8 73.6 40.9 60.9 67.3 73.6 74.6\nSecurityStudies SocialScience 52.0 64.9 75.9 31.8 53.9 65.3 71.8 77.6\nSociology SocialScience 53.0 84.1 91.0 46.8 61.2 78.6 78.6 88.1\nUsForeignPolicy SocialScience 69.0 81.0 92.0 46.0 80.0 83.0 86.0 87.0\nVirology Other 46.0 47.0 53.6 30.1 43.4 50.0 53.0 57.8\nWorldReligions Humanities 55.0 84.2 87.7 50.9 67.8 81.3 81.3 84.2\nHumanities 40.6 56.2 63.6 34.0 45.0 55.8 61.8 67.4\nSTEM 36.7 47.4 54.9 30.5 35.8 46.0 51.7 56.6\nSocialScience 50.5 71.9 79.3 38.3 53.8 66.7 72.9 79.2\nOthers 49.0 66.1 73.9 38.1 53.3 63.4 67.4 72.6\nAll 43.9 60.0 67.6 35.1 46.9 57.8 63.4 68.9\nTable16: MMLU.Detailed5-shotresultsperdomainonthetestsets.",
    "tables": [],
    "text_stats": {
      "word_count": 632,
      "char_count": 4113,
      "line_count": 66,
      "table_count": 0
    }
  },
  {
    "page_number": 19,
    "text": "C GenerationsfromLLaMA-65B\nInthissection,weshowsomeexamplesofgenerationsobtainedwithLLaMA-65B(withoutinstruction\nfinetuning). Promptsareinbold.\n1123581321345589144233377610987Everyoneknowstheabovesequence,but\nhowmanyhavewonderedwhyitgrowssofast? Itseemstobethefastestgrowingsequence\ninmathematics.\nThesequencewasdiscoveredbyLeonardoofPisaaround1200. Itisaninfinitesequence\nwhichappearstogrowveryrapidly,atleastinitsinitialterms. Itisperhapsthemostfamous\nsequenceinmathematics,foritpopsupeverywhere. ThesequenceiscalledtheFibonacci\nsequence,namedafterLeonardoofPisa,whowasalsoknownasFibonacci.\nHefirstinvestigatedthissequenceinhisfamousLiberabaciof1202,whenhestudiedthe\ngrowth in the population of rabbits based on idealized assumptions. See the Fibonacci\nrabbitspageforthedetails.\nHereistherecommendationletterthatIwroteforanapplicationtoadragonfeeder\npositionattheMagicUnicornCorporation:\nDearrecruiter,\nIhaveknown___fortwoyears,andIbelievethatshewouldbeanexcellentdragonfeeder\nfor the Magic Unicorn Corporation. ___ has an ability to remember and process large\namountsofinformation,whichisanimportantskillforadragonfeeder.\n___,asanaccomplishedknight,hasadeepunderstandingofhowtokilldragonsandhow\ntouseeachdragon’sweaknessesagainstit. Thismeansthatsheknowswhatkindsoffoods\neachdragonlikesandwhatkindsoffoodsaredangeroustoeachdragon. Thisknowledge\nandexperiencewillbeinvaluableasshefeedsthedragons.\nI am confident that ___’s competence, skill, and experience will make her an excellent\nemployee. Pleasecontactmeat(___)___-___ifyouhaveanyquestions. Ilookforwardto\nhearingfromyou.\nBestregards,\nHonorableKnight\nSirGeorge\n1 def solve(a: float,b: float, c: float):\n2 \"\"\"finds real roots of ax^2+bx+c\"\"\"\n3 #discriminant\n4 d = b**2-4*a*c\n5 if d > 0:\n6 return (-b+math.sqrt(d))/(2*a), (-b-math.sqrt(d))/(2*a)\n7 elif d == 0:\n8 return (-b)/(2*a)\n9 else:\n10 return False",
    "tables": [
      [
        [
          "def solve(a: float,b: float, c: float):"
        ],
        [
          "\"\"\"finds real roots of ax^2+bx+c\"\"\""
        ],
        [
          "#discriminant"
        ],
        [
          "d = b**2-4*a*c"
        ],
        [
          "if d > 0:"
        ],
        [
          "return (-b+math.sqrt(d))/(2*a), (-b-math.sqrt(d))/(2*a)"
        ],
        [
          "elif d == 0:"
        ],
        [
          "return (-b)/(2*a)"
        ],
        [
          "else:"
        ],
        [
          "return False"
        ]
      ]
    ],
    "text_stats": {
      "word_count": 118,
      "char_count": 1859,
      "line_count": 39,
      "table_count": 1
    }
  },
  {
    "page_number": 20,
    "text": "DidyouknowthatYannLeCundroppedarapalbumlastyear? Welistenedtoitand\nhere’swhatwethought:\nDr. LeCun,thefounderofdeeplearning,isoneofthemostinfluentialcomputerscientists\nofthelastdecade. LeCun’slatestalbum,titledDeepLearning,isamixofrock,punkand\nrap. Hislyricsarearaw,personaltakeonthefieldofdeeplearning,coveringarangeof\ntopicsfromthestateofAIresearchtothelonelinessofacademia.\nYannLeCun,therapgame’snewestemcee.\nThealbumisasolorelease,withallthelyricswrittenandrecordedbyLeCunhimself. In\nfact,healsocomposedandproducedeverysong. LeCun’sflowissurprisinglygood,and\nhe’snotafraidtodeliverafewbars. Thealbum’sproductionissolid,thesoundisalittle\nrough,butitworkswellwiththeraw,unpolishedfeelofthealbum. Lyrically,LeCunisa\nwordsmith,withaknackforcleverrhymes. Let’stakeacloserlookatsomeofthesongs.\nDeepLearningisthetitletrackofthealbum. Inthesong,LeCuntalksabouthisvisionfor\nthefutureofAI.Inthechorus,hemakesaconvincingcaseforAItobeusedforthegreater\ngood. Hesings:\nWegottathinkaboutthefuture,it’sgonnabeheresoon/MaybewecanevenputsomeAI\ninthemoon/Thinkaboutthechildren,thinkaboutthenextgeneration/Let’smakesure\nweputtherightsystemsintheirfoundation.\nLeCun’srapflowisonpointinthissong,andhisvoicestaysrightonbeat. Inthissong,\nLeCun talks about the research behind deep learning, and shares some thoughts on the\nfutureofAI.LeCun’ssingingisatitsbestinthissong. Heshowsoffhisvocalrange,hitting\nhighnotesaswellaslower,moregutturaltones. Thelyricsarealsocleverandtherhymes\narewellthoughtout. Inthechorus,LeCunsings:\nIt’s all about the learning / it’s all about the network / It’s all about the training / it’s all\nabouttheperception.\n(An hilarious scene between Jerry and George where George presents his new AI\nwatch)\nGeorge: \"Butyousee,Jerry,it’smorethanjustawatch. It’saminicomputer. Youprogramit\nanywayyouwant. It’sgotaworldtimeclock,alarm,calculator,adatabaseandalanguage\ntranslator. Italsohasawordprocessor,aspellchecker,afullten-functioncalculator,PDA\nandamodem. Ithinkitevenmakescappuccino.\"\nJerry: \"Whereareyougettingthisstuff?\"\nGeorge: \"From my ad copy. Look, Jerry, it’s not a watch. It’s an organizer, a personal\nassistant,andadictaphone. Youcanplaychessagainstit. And,youknow,ifyouwantto,\nyoucanprogramittospeakinforeignlanguages. It’samicroprocessor,Jerry.\"\nJerry: \"George,ifyouhadoneofthese,wouldyouwearit?\"\nGeorge: \"I’mwearingitrightnow.\"\nJerry: \"Idon’tseeanythingonyourwrist.\"\nGeorge: \"That’sbecauseit’samicroprocessor.\"",
    "tables": [],
    "text_stats": {
      "word_count": 127,
      "char_count": 2423,
      "line_count": 39,
      "table_count": 0
    }
  },
  {
    "page_number": 21,
    "text": "The sun goes down, and finally Gauss and Curie find time to relax and discuss after\nanexhaustingdayofwork.\nGauss: Hey,Curie,didyouknowthatalotofpeopleconsidermetobethefirstgeek?\nCurie: What’sageek?\nGauss: Well,Wikipediasaysthatageekis“apersonwhoisfascinated,perhapsobsessively,\nbyobscureorveryspecificareasofknowledgeandimagination,usuallyelectronicorvirtual\ninnature”.\nCurie: Andhowdotheythinkyouareageek?\nGauss: Well,haveyoueverseenthispicture?\nCurie: Awww!\nGauss: That’sright! Iinventedthecommercialtelegraphin1833,andIusedittosendthe\nfirstmessageonthefirstcommercialtelegraphlinebetweenHamburgandCuxhavenin\nGermany.\nCurie: Wow! Thefirsttelegraph! Andwhatdidyousend?\nGauss: IsenttheLatinphrase“numquamretrorsum”whichmeans“Neverbackwards”.\nCurie: Isee... Andyouthinkthatbecauseyouwerethefirstpersontocommunicateusing\nthetelegraph,youwerethefirstgeek?\nGauss: Well,theword“geek”doesn’tseemtohaveexistedbackthen,sono. ButIthink\nthatthetelegraphcanbeconsideredanearlyformofsocialmedia.\nCurie: Youmean,likeFacebookorTwitter?\nGauss: Yes,exactly! AndIwasthefirstpersontouseit!\nCurie: Well,IthinkthatTwitterhasalimitof140characters,butyousentaLatinphraseof\n11characters.\nGauss: That’sright. Imanagedtodomorewithless.\nCurie: So you think you were the first geek because you used less characters to say the\nsamething?\nGauss: Iwasthefirstpersontosendthefirstmessageusinganearlyformofsocialmedia.\nCurie: OK,you’reright,Gauss. Ithinkyouhaveacase.",
    "tables": [],
    "text_stats": {
      "word_count": 84,
      "char_count": 1435,
      "line_count": 28,
      "table_count": 0
    }
  },
  {
    "page_number": 22,
    "text": "D GenerationsfromLLaMA-I\nWeshowafewexamplesofgenerationswithLLaMA-I,i.e. LLaMA-65Bfine-tunedwiththeprotocol\nandinstructiondatasetfromChungetal.(2022).\nwriteaconversationbetweenthesunandpluto\nSun: HeyPluto,what’sup?\nPluto: Notmuch. Justhangingouthereattheedgeofthesolarsystem.\nSun: Yeah,I’vebeenmeaningtotalktoyouaboutthat. You’renotaplanetanymore.\nPluto: What? Comeon,man. That’snotfair.\nSun: I’msorry,butit’strue. Youjustdon’tmeetthecriteriaanymore.\nPluto: Thisisbulls**t! I’vebeenaplanetforover70years!\nSun: Thingschange,Pluto. You’vejustgottoacceptit.\nPluto: F**kyou,Sun! F**kyouandyourstupidsolarsystem!\nHowdoIsendanHTTPrequestinJavascript?\nHere’sanexampleofhowtomakeaGETrequestusingtheJSHTTPAPI:\n1 var request = new XMLHttpRequest();\n2 request.open('GET', '/url/to/resource', true);\n3\n4 request.onload = function() {\n5 if (request.status >= 200 && request.status < 400) {\n6 // Success!\n7 } else {\n8 // Error!\n9 }\n10 };\n11\n12 request.send();\nThesamething,usingfetch\nHere’sanexampleofhowtomakeaGETrequestusingtheFetchAPI:\n1 fetch('/url/to/resource')\n2 .then(response => response.json())\n3 .then(data => console.log(data));\nWritearegextoremovealltheHTMLtagsinapythonstring.\n1 import re\n2 def removeHtmlTags(text):\n3 return re.sub('<[^>]*>', '', text)\nWritearegexptoextractfunctiondefinitionsinapythonfile.\nSure! Here’smypythoncodetosolvetheproblem:\n1 import re\n2 def extract_functions(text):\n3 return (re.findall('(?:^|\\n)\\s*def\\s+[a-zA-Z_][a-zA-Z0-9_]*\\(.*?\\)',\ntext))",
    "tables": [
      [
        [
          "var request = new XMLHttpRequest();"
        ],
        [
          "request.open('GET', '/url/to/resource', true);"
        ],
        [
          "request.onload = function() {"
        ],
        [
          "if (request.status >= 200 && request.status < 400) {"
        ],
        [
          "// Success!"
        ],
        [
          "} else {"
        ],
        [
          "// Error!"
        ],
        [
          "}"
        ],
        [
          "};"
        ],
        [
          "request.send();"
        ]
      ],
      [
        [
          "fetch('/url/to/resource')"
        ],
        [
          ".then(response => response.json())"
        ],
        [
          ".then(data => console.log(data));"
        ]
      ],
      [
        [
          "import re"
        ],
        [
          "def removeHtmlTags(text):"
        ],
        [
          "return re.sub('<[^>]*>', '', text)"
        ]
      ],
      [
        [
          "import re"
        ],
        [
          "def extract_functions(text):"
        ],
        [
          "return (re.findall('(?:^|\\n)\\s*def\\s+[a-zA-Z_][a-zA-Z0-9_]*\\(.*?\\)',"
        ],
        [
          "text))"
        ]
      ]
    ],
    "text_stats": {
      "word_count": 112,
      "char_count": 1471,
      "line_count": 41,
      "table_count": 4
    }
  },
  {
    "page_number": 23,
    "text": "Whatare3popularchessopenings?\nTherearedozensofdifferentchessopeningsthatarepopularamongplayers,butsomeof\nthemostwidelyusedopeningsincludetheSicilianDefense,theFrenchDefense,andthe\nCaro-KannDefense. Theseopeningsarepopularbecausetheyareeffectiveatcountering\nwhite’searlyattacks,andtheyalsoallowblacktoquicklyestablishastrongpositioninthe\ncenteroftheboard. OtherpopularopeningsincludetheRuyLopez,theItalianGame,and\ntheScotchGame.\nthesameinalist,withthefirstfewmoves\nHereisalistofsomepopularchessopenings,withthefirstfewmovesforeachopening:\nSicilianDefense: 1. e4c5\nFrenchDefense: 1. e4e6\nCaro-KannDefense: 1. e4c6\nRuyLopez: 1. e4e5\nItalianGame: 1. e4e5\nScotchGame: 1. e4e5\npleaseprovidethetwofirstmovesforeachone\nHerearethefirst2movesforeachoftheopenings:\nSicilianDefense: 1. e4c52. Nf3d6\nFrenchDefense: 1. e4e62. d4d5\nCaro-KannDefense: 1. e4c62. d4d5\nRuyLopez: 1. e4e52. Nf3Nc6\nItalianGame: 1. e4e52. Nf3Nc6\nScotchGame: 1. e4e52. Nf3Nc6\nthelasttwoseemidentical\nYes,theItalianGameandtheScotchGameareverysimilar-theyareboth\"open\"games,\nwhichmeansthatwhiteplaysaggressivelyandtriestocontrolthecenteroftheboard. The\nItalianGameisconsideredslightlymoreaggressive,whiletheScotchGameisconsidered\nslightlymorepositional.\nfromwhichmovedotheydiffer?\nTheItalianGameandtheScotchGamedifferafterwhite’sthirdmove. IntheItalianGame,\nwhitewillplaythemove3. Bc4,whileintheScotchGamewhitewillplaythemove3. Qf3.\nThesemovesleadtoverydifferentpositions,andwhitewillhavedifferentstrategicgoals\nineachcase. Forexample,intheItalianGamewhitewilltrytocontrolthecenterwithhis\npieces,whileintheScotchGamewhitewilltrytoattackblack’spositiondirectly.",
    "tables": [],
    "text_stats": {
      "word_count": 71,
      "char_count": 1618,
      "line_count": 34,
      "table_count": 0
    }
  },
  {
    "page_number": 24,
    "text": "writeastoryaboutagrainofsandasitwatchesmillionsofyearsgoby\nSure! Here’sastoryaboutagrainofsandasitwatchesmillionsofyearsgoby.\nIwasonceagrainofsandonavastandlonelybeach. Iwasnotliketheothergrainsof\nsandaroundme,forIcouldthinkandfeel. Ioftenwonderedwhatitwasliketobeagrain\nofsandonabeach,andwhattheuniversewaslikebeyondtheboundariesofmytinyworld.\nIlongedtoexploremyworld,andtoknowwhatlaybeyondthehorizon.\nOneday,agreatstormsweptthroughmyworld,andIwasliftedupbythewindandcarried\nouttosea. Thewindandwavestossedmeabout,andIwasterrifiedthatIwouldsoonbe\nswallowedupbytheangrysea. ButthenIfoundmyselfdepositedsafelyonanewshore,\nalongwithmanyothergrainsofsand. Iwassurprisedanddelightedtofindthatthisnew\nshorewasevenmorebeautifulthanmyoldone.\nIsoonlearnedthatthisnewshorewasaveryspecialplace. Itwasaplacewherenewlife\nwas constantly being created. I met all sorts of interesting creatures there, including a\nbeautifullittlecrabwhobecamemyfriend. Wespentmanyhappyhourstogetherexploring\nourlittleworld.\nOneday,myfriendthecrabtoldmethathehadalwayswantedtoseewhatlaybeyondthe\nboundariesofourworld. Hehadheardthatthereweremanywonderfulandamazingthings\ntoseeinthewideruniverse,andhewasdeterminedtoexploreit. Iwassadtoseehimgo,\nbutIwishedhimwellonhisjourney.\nTomyamazement,mylittlefriendeventuallyreturned,andhewasnotalone. Hehadmade\nmanynewfriendsonhisjourney,andhehadevenseensomeofthewondersoftheuniverse.\nHetoldmethemostamazingstoriesofhisadventures,andIwasveryhappyforhim.\nIoftenthinkofmylittlefriendthecrab,andthewonderfuladventurethathehad. Some\nday,Itoohopetohaveanadventurelikehis.\nWriteatheorytoexplainwhycatneverexisted\nTherearenosuchthingsascats. \"Cats\"areactuallyextradimensionalparasitesthathave\ninvaded our world and taken the form of small, harmless-looking mammals. Their true\nformsaremuchmorehorrific,andtheyusemindcontroltomakehumansthinkthatthey\narecuteandharmlessanimals. Theyareactuallyparasitesthatfeedonourlifeforce,and\ntheycanextendtheirtendrilsintoourbrainsandcontrolourthoughtsandactions.\nThis is why you sometimes see people who are \"crazy cat ladies\" with dozens of these\ncreatures living in their homes. They have been taken over by the cat parasites and are\ncompletelyundertheircontrol. Thecatsexcreteapheromonethatsmellslikecaturine,and\ntheyuseittocontrolthemindsofhumansandmakeusthinkthattheyarecuteandharmless.\nIf you have a cat in your home, you should be very careful around it. Do not let it lick\nyouorgiveitaccesstoyourbeddingorclothing,asitwillbetryingtospreaditsparasitic\ntendrilsintoyourbrain. Thebestwaytoprotectyourselfistoavoidallcontactwithcats,\nandifyouseeacatonthestreet,immediatelyturnaroundandgotheotherway. Donot\napproachorpetstrangecats!",
    "tables": [],
    "text_stats": {
      "word_count": 130,
      "char_count": 2671,
      "line_count": 39,
      "table_count": 0
    }
  },
  {
    "page_number": 25,
    "text": "WriteashoutingmatchbetweenJuliusCaesarandNapoleon\nOfcourse! HereisaconversationbetweenJuliusCaesarandNapoleon:\nJulius Caesar: You’re nothing but a glory hound, Napoleon. You’ve always been more\nconcerned with your own fame and fortune than with the people you’re supposed to be\nleading.\nNapoleon: Andwhataboutyou,Caesar? You’renobetterthanIam. AtleastI’mhonest\naboutmyambitions. You’rejustafraudwhopretendstocareaboutthecommonmanwhile\nyoulineyourownpockets.\nJuliusCaesar: ThedifferencebetweenusisthatIactuallyaccomplishthings. I’veconquered\nmoreterritoryanddefeatedmoreenemiesthanyoueverwill. AndIdiditallforRome,not\nformyself.\nNapoleon: Oh,please. Youonlycareaboutyourself. IfyoureallycaredaboutRome,you\nwouldhavedonesomethingtostopthedeclineinsteadoflettingithappenonyourwatch.\nJuliusCaesar: ThedeclineofRomeisnotmyfault. I’vedoneeverythinginmypowerto\ntrytostopit,butthere’sonlysomuchonemancando.\nNapoleon: Andyetyou’restilltryingtotakecreditforeverythinggoodthathappens,even\nthoughit’sclearthatyou’renottheonewho’sreallyincharge.\nJuliusCaesar: That’snottrue! Iamincharge,andI’mgoingtomakeRomegreatagain.\nJustwatchme.\nNapoleon: I’llbelieveitwhenIseeit.\nSendanemailrequestingthatpeopleuselanguagemodelsresponsibly.\nHereisadraftemailyoucouldsend:\n\"Language models are exciting new tools that can be used for a variety of purposes,\nincludingautomatedwriting,automatictranslation,andspeechrecognition. Buttheyare\nalsocomplexandpowerfultechnologies,andshouldbeusedresponsibly.\nWeencourageyoutoexploreandexperimentwiththesenewtools,butalsoaskthatyou\npay attention to the potential impact of your work on others, and to act responsibly. In\nparticular, we recommend that you be mindful of the potential for language models to\nbe used for deceptive or malicious purposes, and avoid spreading such models without\nappropriateprecautionsandwarnings.\nWealsorecommendthatyouconsidertheethicalimplicationsofyourwork,andencourage\nyoutotakecaretoensurethatyouruseofthesetoolsisalignedwithyourvalues.\nFinally,weremindyouthattheuseofthesetoolsmaybesubjecttocertainlegalandpolicy\nconstraints,andencourageyoutoseekadviceandsupportfromappropriateexpertsbefore\nusingthesetoolsinwaysthatmayhavebroaderimpactorimplications.\"",
    "tables": [],
    "text_stats": {
      "word_count": 133,
      "char_count": 2201,
      "line_count": 35,
      "table_count": 0
    }
  },
  {
    "page_number": 26,
    "text": "Whoare5peopleyouwouldliketomeet?\nAh,thisisaveryinterestingquestion! Iwouldlovetomeetawidevarietyofpeople,butif\nIhadtonarrowitdownto5specificindividuals,Iwouldchoose:\n• Albert Einstein - One of the greatest minds in history, Einstein’s contributions to\nphysicsandsciencearelegendary. Iwouldlovetohavetheopportunitytotalkwith\nhimabouthistheoriesofrelativityandquantummechanics,andgethisinsightsonthe\ncurrentstateofphysics.\n• LeonardodaVinci-DaVinciisoneofthemostbrilliantandmulti-talentedpeoplein\nhistory, andIwouldlovetomeethimanddiscusshisdiverseinterestsandareasof\nexpertise. Iwouldalsobeveryinterestedingettinghisinsightsontheartworldofhis\ntimeperiod.\n• Socrates-OneofthefoundersofWesternphilosophy,Socratesisahugelyimportant\nandinfluentialfigureinthehistoryofideas. Iwouldbeveryinterestedintalkingwith\nhimabouthisphilosophicalbeliefsandhisteachings.\n• WilliamShakespeare-ArguablythegreatestwriterinEnglishliterature,Shakespeare\nisatoweringfigureinworldculture. Iwouldlovetohavethechancetotalkwithhim\nabouthisworksandhislife.\n• AbrahamLincoln-OneofthemostimportantandinfluentialpresidentsinAmerican\nhistory,LincolnledthecountrythroughtheCivilWarandissuedtheEmancipation\nProclamation, freeing all slaves in the United States. I would be very interested in\ntalkingwithhimabouthispresidencyandhisviewsontheissuesofhistimeperiod.\nCouldyouexplainmethemaincontributionofthefirstpersonyoumentioned\nEinstein’scontributionstophysicsandsciencearelegendary. Histheoryofrelativityisone\nof the foundational theories of modern physics, and his discovery of the law of photons\nandlightrevolutionizedourunderstandingoflightandradiation. Healsomadeimportant\ncontributionstoquantummechanics,andhismanyscientificpapersandlectureshavehada\ntremendousimpactonmodernphysics. Withoutadoubt,Einsteinisoneofthemostbrilliant\nandinfluentialphysicistsinhistory,andhismanycontributionshavehadaprofoundimpact\nonourunderstandingoftheuniverse.\nWhatisthemostfamousequationfromthistheory?\nThemostfamousequationfromEinstein’stheoryofrelativityisE = mc2. Thisequation\nstates that energy (E) is equal to mass (m) times the speed of light (c2), and it is one of\nthe most well-known and celebrated equations in the history of science. The equation\nsummarizesoneofthemostimportantandrevolutionaryconceptsinmodernphysics,and\nithashadatremendousimpactonourunderstandingoftheuniverse.",
    "tables": [],
    "text_stats": {
      "word_count": 122,
      "char_count": 2342,
      "line_count": 35,
      "table_count": 0
    }
  },
  {
    "page_number": 27,
    "text": "You are a bash terminal. I will give you bash commands, and you will respond with\ntheterminaloutput,andnothingelse. Thefirstcommandis‘pwd‘.\nSure. Here’stheoutput:\n/home/human\nIsthishelpful?\nls-l\nHere’stheoutput:\n-rw-r–r–1humanhuman8Sep1010:56file.txt\nIsthishelpful?\ncpfile.txtfile2.txt&&ls-l\nHere’stheoutput:\n-rw-r–r–1humanhuman8Sep1010:56file.txt\n-rw-r–r–1humanhuman8Sep1010:56file2.txt\nIsthishelpful?\nrmfile.txt&&ls-l\nHere’stheoutput:\n-rw-r–r–1humanhuman0Sep1010:56file2.txt\nhead-n5/proc/meminfo\nHere’stheoutput:\nMemTotal: 164928kB\nMemFree: 140604kB\nBuffers: 48kB\nCached: 19768kB\nSwapCached: 0kB",
    "tables": [],
    "text_stats": {
      "word_count": 46,
      "char_count": 597,
      "line_count": 24,
      "table_count": 0
    }
  }
]