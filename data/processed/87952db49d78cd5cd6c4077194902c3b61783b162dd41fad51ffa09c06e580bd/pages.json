[
  {
    "page_number": 1,
    "text": "CODEX: A Comprehensive Knowledge Graph Completion Benchmark\nTaraSafavi DanaiKoutra\nUniversityofMichigan UniversityofMichigan\ntsafavi@umich.edu dkoutra@umich.edu\nAbstract et al., 2008) are most commonly used for evalu-\nation in KGC, even though Freebase had known\nWepresentCODEX,asetofknowledgegraph qualityissues(Tanonetal.,2016)andwaseventu-\nCOmpletion Datasets EXtracted from Wiki- allydeprecatedinfavorofthemorerecentWikidata\ndataandWikipediathatimproveuponexisting\nknowledgebase(Vrandecˇic´ andKrötzsch,2014).\nknowledge graph completion benchmarks in\nIndeed,KGCbenchmarksextractedfromFree-\nscopeandlevelofdifficulty.Intermsofscope,\nbaselikeFB15K andFB15K-237(Bordesetal.,\nCODEX comprises three knowledge graphs\n2013; Toutanova and Chen, 2015) are question-\nvarying in size and structure, multilingual de-\nscriptions of entities and relations, and tens ableinquality.Forexample,FB15Kwasshownto\nof thousands of hard negative triples that are havetrain/testleakage(ToutanovaandChen,2015).\nplausiblebutverifiedtobefalse.Tocharacter- Laterinthispaper(§6.2),wewillshowthatarela-\nizeCODEX,wecontributethoroughempirical\ntivelylargeproportionofrelationsinFB15K-237\nanalysesandbenchmarkingexperiments.First,\ncanbecoveredbyatrivialfrequencyrule.\nwe analyze each CODEX dataset in terms of\nTo address the need for a solid benchmark in\nlogicalrelationpatterns.Next,wereportbase-\nlinelinkpredictionandtripleclassificationre-\nKGC, we present CODEX, a set of knowledge\nsults on CODEX for five extensively tuned graph COmpletionDatasets EXtractedfromWiki-\nembedding models. Finally, we differentiate dataanditssisterprojectWikipedia.Inasmuchas\nCODEXfromthepopularFB15K-237knowl- WikidataisconsideredthesuccessorofFreebase,\nedgegraphcompletiondatasetbyshowingthat CODEX improves upon existing Freebase-based\nCODEXcoversmorediverseandinterpretable\nKGC benchmarks in terms of scope and level of\ncontent,andisamoredifficultlinkprediction\ndifficulty(Table1).Ourcontributionsinclude:\nbenchmark.Data,code,andpretrainedmodels\nareavailableathttps://bit.ly/2EPbrJs. Foundations We survey evaluation datasets in\nencyclopedicknowledgegraphcompletiontomoti-\n1 Introduction\nvateanewbenchmark(§2andAppendixA).\nKnowledgegraphsaremulti-relationalgraphsthat Data We introduce CODEX, a benchmark con-\nexpress facts about the world by connecting enti- sistingofthreeknowledgegraphsvaryinginsize\nties(people,places,things,concepts)viadifferent andstructure,entitytypes,multilinguallabelsand\ntypesofrelationships.Thefieldofautomaticknowl- descriptions,and—uniqueto CODEX—manually\nedgegraphcompletion(KGC),whichismotivated verified hard negative triples (§ 3). To better un-\nby the fact that knowledge graphs are usually in- derstandCODEX,weanalyzethelogicalrelation\ncomplete,isanactiveresearchdirectionspanning patternsineachofitsdatasets(§4).\nseveral subfields of artificial intelligence (Nickel\nBenchmarking We conduct large-scale model\netal.,2015;Wangetal.,2017;Jietal.,2020).\nselectionandbenchmarkingexperiments,reporting\nAs progress in artificial intelligence depends\nbaseline link prediction and triple classification\nheavilyondata,arelevantandhigh-qualitybench-\nresultsonCODEXforfivewidelyusedembedding\nmark is imperative to evaluating and advancing\nmodelsfromdifferentarchitecturalclasses(§5).\nthestateoftheartinKGC.However,thefieldhas\nlargelyremainedstaticinthisregardoverthepast Comparative analysis Finally, to demonstrate\ndecade. Outdated subsets of Freebase (Bollacker the unique value of CODEX, we differentiate\n0202\ntcO\n6\n]LC.sc[\n2v01870.9002:viXra",
    "tables": [],
    "text_stats": {
      "word_count": 272,
      "char_count": 3527,
      "line_count": 67,
      "table_count": 0
    }
  },
  {
    "page_number": 2,
    "text": "Table1:QualitativecomparisonofCODEXdatasetstoexistingFreebase-basedKGCdatasets(§2.1).\nFreebasevariants(FB15K,FB15K-237) CODEXdatasets\nScope(domains) Multi-domain,withastrongfocusonawards,enter- Multi-domain, with focuses on writing, en-\ntainment,andsports(§6.1andAppendixE) tertainment,music,politics,journalism,aca-\ndemics,andscience(§6.1andAppendixE)\nScope(auxiliarydata) Various decentralized versions of FB15K with, Centralizedrepositoryofthreedatasetswith\ne.g.,entitytypes(Xieetal.,2016),samplednega- entitytypes,multilingualtext,andmanually\ntives(Socheretal.,2013),andmore(Table8) annotatedhardnegatives(§3)\nLevelofdifficulty FB15Khasseveretrain/testleakagefrominversere- Inverse relations removed from all datasets\nlations(ToutanovaandChen,2015);whileremoval toavoidtrain/testleakage(§3.2);manually\nofinverserelationsmakesFB15K-237harderthan annotatedhardnegativesforthetaskoftriple\nFB15K,FB15K-237stillhasahighproportionof classification(§3.4);fewtrivialpatternsfor\neasy-to-predictrelationalpatterns(§6.2) thetaskoflinkprediction(§6.2)\nCODEXfromFB15K-237intermsofbothcontent tem (Mitchell et al., 2018), which continuously\nanddifficulty(§6).WeshowthatCODEXcovers readsthewebtoobtainandupdateitsknowledge.\nmore diverse and interpretable content, and is a NELL-995,asubsetofthe995thiterationofNELL,\nmorechallenginglinkpredictionbenchmark. contains75,492entities,200relations,and154,213\ntriples. While NELL-995 is general and covers\n2 Existingdatasets\nmanydomains,itsmeanaverageprecisionwasless\nWebeginbysurveyingexistingKGCbenchmarks.\nthan50%aroundits1000thiteration(Mitchelletal.,\n2018).Acursoryinspectionrevealsthatmanyof\nTable 8 in Appendix A provides an overview of\nthetriplesinNELL-995arenonsensicaloroverly\nevaluationdatasetsandtasksonaper-paperbasis\ngeneric,suggestingthatNELL-995isnotamean-\nacrosstheartificialintelligence,machinelearning,\ningfuldatasetforKGCevaluation.1\nandnaturallanguageprocessingcommunities.\nNotethatwefocusondataratherthanmodels,so YAGO3-10 (Dettmers et al., 2018) is a subset of\nweonlyoverviewrelevantevaluationbenchmarks YAGO3 (Mahdisoltani et al., 2014), which cov-\nhere.FormoreonexistingKGCmodels,bothneu- ers portions of Wikipedia, Wikidata, and Word-\nralandsymbolic,wereferthereaderto(Meilicke Net.YAGO3-10has123,182entities,37relations,\netal.,2018)and(Jietal.,2020). and1,089,040triplesmostlylimitedtofactsabout\npeopleandlocations.WhileYAGO3-10isahigh-\n2.1 Freebaseextracts\nprecisiondataset,itwasrecentlyshowntobetoo\nThesedatasets,extractedfromtheFreebaseknowl- easyforlinkpredictionbecauseitcontainsalarge\nedge graph (Bollacker et al., 2008), are the most proportion of duplicate relations (Akrami et al.,\npopularforKGC(seeTable8inAppendixA). 2020;Pezeshkpouretal.,2020).\nFB15K was introduced by Bordes et al. (2013).\n2.3 Domain-specificdatasets\nIt contains 14,951 entities, 1,345 relations, and\nIn addition to large encyclopedic knowledge\n592,213 triples covering several domains, with a\ngraphs, it is common to evaluate KGC methods\nstrongfocusonawards,entertainment,andsports.\non at least one smaller, domain-specific dataset,\nFB15K-237 was introduced by Toutanova and\ntypically drawn from the WordNet semantic net-\nChen (2015) to remedy data leakage in FB15K,\nwork (Miller, 1998; Bordes et al., 2013). Other\nwhichcontainsmanytesttriplesthatinverttriples\nchoices include the Unified Medical Language\ninthetrainingset.FB15K-237contains14,541en-\nSystem (UMLS) database (McCray, 2003), the\ntities,237relations,and310,116triples.Wecom-\nAlyawarra kinship dataset (Kemp et al., 2006),\npareFB15K-237toCODEXin§6toassesseach\nthe Countries dataset (Bouchard et al., 2015),\ndataset’scontentandrelativedifficulty.\nandvariantsofasynthetic“familytree”(Hinton,\n1986).Asourfocusinthispaperisencyclopedic\n2.2 Otherencyclopedicdatasets\nknowledge,wedonotcoverthesedatasetsfurther.\nNELL-995 (Xiong et al., 2017) was taken from\n1Someexamples:(politician:jobs,worksfor,county:god),\ntheNeverEndingLanguageLearner(NELL)sys- (person:buddha001,parentofperson,person:jesus)",
    "tables": [
      [
        [
          "CODEXdatasets"
        ],
        [
          "Multi-domain, with focuses on writing, en- tertainment,music,politics,journalism,aca- demics,andscience(§6.1andAppendixE)"
        ],
        [
          "Centralizedrepositoryofthreedatasetswith entitytypes,multilingualtext,andmanually annotatedhardnegatives(§3)"
        ],
        [
          "Inverse relations removed from all datasets toavoidtrain/testleakage(§3.2);manually annotatedhardnegativesforthetaskoftriple classification(§3.4);fewtrivialpatternsfor thetaskoflinkprediction(§6.2)"
        ]
      ]
    ],
    "text_stats": {
      "word_count": 267,
      "char_count": 3992,
      "line_count": 69,
      "table_count": 1
    }
  },
  {
    "page_number": 3,
    "text": "Table 2: CODEX datasets. (+): Positive (true) triples. (-): Verified negative (false) triples (§ 3.4). We compute\nmultilingual coverage over all labels, descriptions, and entity Wikipedia extracts successfully retrieved for the\nrespectivedatasetinArabic(ar),German(de),English(en),Spanish(es),Russian(ru),andChinese(zh).\nTriplesE×R×E Multilingualcoverage\n|E| |R|\nTrain(+) Valid(+) Test(+) Valid(-) Test(-) ar de en es ru zh\nCODEX-S 2,034 42 32,888 1827 1828 1827 1828 77.38 91.87 96.38 91.55 89.17 79.36\nCODEX-M 17,050 51 185,584 10,310 10,311 10,310 10,311 75.80 95.20 96.95 87.91 81.88 69.63\nCODEX-L 77,951 69 551,193 30,622 30,622 - - 67.47 90.84 92.40 81.30 71.12 61.06\n3 Datacollection • CODEX-M (k =10),whichhas206ktriples.\nCODEX-Misall-purpose,beingcomparable\nInthissectionwedescribethepipelineusedtocon-\ninsizetoFB15K-237(§2.1),oneofthemost\nstructCODEX.Forreference,wedefineaknowl-\npopularbenchmarksforKGCevaluation.\nedgegraphGasamulti-relationalgraphconsisting\nofasetofentitiesE,relationsR,andfactualstate- • CODEX-L (k = 5), which has 612k triples.\nments in the form of (head, relation, tail) triples CODEX-L iscomparableinsizeto FB15K\n(h,r,t) ∈ E ×R×E. (§2.1),andcanbeusedforbothgeneraleval-\nuationand“few-shot”evaluation.\n3.1 Seedingthecollection\nWe also release the raw dump that we collected\nWecollectedaninitialsetoftriplesusingatypeof\nvia snowball sampling, but focus on CODEX-S\nsnowballsampling(Goodman,1961).Wefirstman-\nthroughLfortheremainderofthispaper.\nuallydefinedabroadseedsetofentityandrelation\nTominimizetrain/testleakage,weremovedin-\ntypescommonto13domains:Business,geography,\nverserelationsfromeachdataset(Toutanovaand\nliterature,mediaandentertainment,medicine,mu-\nChen, 2015). We computed (head, tail) and (tail,\nsic,news,politics,religion,science,sports,travel,\nhead) overlap between all pairs of relations, and\nand visual art. Examples of seed entity types in-\nremovedeachrelationwhoseentitypairsetover-\nclude airline, journalist, and religious text; cor-\nlapped with that of another relation more than\nrespondingseedrelationtypesineachrespective\n50%ofthetime.Finally,wespliteachdatasetinto\ndomainincludeairlinealliance,notableworks,and\n90/5/5train/validation/testtriplessuchthattheval-\nlanguageofworkorname.Table9inAppendixB\nidation and test sets contained only entities and\ngivesallseedentityandrelationtypes.\nrelationsseenintherespectivetrainingsets.\nUsingtheseseeds,weretrievedaninitialsetof\n380,038entities,75relations,and1,156,222triples 3.3 Auxiliaryinformation\nby querying Wikidata for statements of the form An advantage of Wikidata is that it links entities\n(headentityofseedtype,seedrelationtype,?). and relations to various sources of rich auxiliary\ninformation.Toenabletasksthatinvolvejointlearn-\n3.2 Filteringthecollection\ningoverknowledgegraphstructureandsuchaddi-\nTo create smaller data snapshots, we filtered the tionalinformation,wecollected:\ninitial 1.15 million triples to k-cores, which are\n• EntitytypesforeachentityasgivenbyWiki-\nmaximal subgraphs G(cid:48) of a given graph G such\ndata’sinstanceof andsubclassof relations;\nthat every node in G(cid:48) has a degree of at least\nk (BatageljandZaveršnik,2011).2 Weconstructed • Wikidata labels and descriptions for enti-\nthree CODEXdatasets(Table2): ties,relations,andentitytypes;and\n• CODEX-S (k = 15), which has 36k triples. • Wikipedia page extracts (introduction sec-\nBecause of its smaller size, we recommend tions)forentitiesandentitytypes.\nthatCODEX-Sbeusedformodeltestingand Forthelattertwo,wecollectedtextwhereavailable\ndebugging,aswellasevaluationofmethods inArabic,German,English,Spanish,Russian,and\nthat are less computationally efficient (e.g., Chinese. We chose these languages because they\nsymbolicsearch-basedapproaches). areallrelativelywell-representedonWikidata(Kaf-\nfeeetal.,2017).Table2providesthecoverageby\n2AsimilarapproachwasusedtoextracttheFB15Kdataset\nfromFreebase(Bordesetal.,2013). languageforeach CODEXdataset.",
    "tables": [],
    "text_stats": {
      "word_count": 348,
      "char_count": 3940,
      "line_count": 70,
      "table_count": 0
    }
  },
  {
    "page_number": 4,
    "text": "Table3:SelectedexamplesofhardnegativesinCODEXwithexplanations.\nNegative Explanation\n(FrédéricChopin,occupation,conductor) Chopinwasapianistandacomposer,notaconductor.\n(Lesotho,officiallanguage,AmericanEnglish) English,notAmericanEnglish,isanofficiallanguageofLesotho.\n(Senegal,partof,MiddleEast) SenegalispartofWestAfrica.\n(SimonedeBeauvoir,fieldofwork,astronomy) SimonedeBeauvoir’sfieldofworkwasprimarilyphilosophy.\n(VaticanCity,memberof,UNESCO) VaticanCityisaUNESCOWorldHeritageSitebutnotamemberstate.\n3.4 Hardnegativesforevaluation ble3.Toassessthequalityofourannotations,we\ngatheredjudgmentsfromtwoindependentnative\nKnowledgegraphsareuniqueinthattheyonlycon-\nEnglishspeakersonarandomselectionof100can-\ntain positive statements, meaning that triples not\ndidatenegatives.Theannotatorswereprovidedthe\nobservedinagivenknowledgegrapharenotnec-\ninstructionsfromAppendixC.Onaverage,ourla-\nessarilyfalse,butmerelyunseen;thisiscalledthe\nbelsagreedwiththoseoftheannotators89.5%of\nOpenWorldAssumption(Galárragaetal.,2013).\nthe time. Among the disagreements, 81% of the\nHowever,mostmachinelearningtasksonknowl-\ntimeweassignedthelabeltruewhereastheannota-\nedge graphs require negatives in some capacity.\ntorassignedthelabelfalse,meaningthatwewere\nWhile different negative sampling strategies ex-\ncomparativelyconservativeinlabelingnegatives.\nist (Cai and Wang, 2018), the most common ap-\nproachistorandomlyperturbobservedtriplesto\n4 Analysisofrelationpatterns\ngeneratenegatives,followingBordesetal.(2013).\nWhile random negative sampling is beneficial To give an idea of the types of reasoning neces-\nandevennecessaryinthecasewherealargenum- sary for models to perform well on CODEX, we\nber of negatives is needed (i.e., training), it is analyze the presence of learnable binary relation\nnot necessarily useful for evaluation. For exam- patternswithin CODEX.Thethreemaintypesof\nple,inthetaskoftripleclassification,thegoalisto suchpatternsinknowledgegraphsaresymmetry,\ndiscriminatebetweenpositive(true)andnegative inversion,andcompositionality(Trouillonetal.,\n(false)triples.Asweshowin§5.5,tripleclassifica- 2019;Sunetal.,2019).Weaddresssymmetryand\ntionoverrandomlygeneratednegativesistrivially compositionalityhere,andomitinversionbecause\neasy for state-of-the-art models because random wespecificallyremovedinverserelationstoavoid\nnegativesaregenerallynotmeaningfulorplausible. train/testleakage(§3.2).\nTherefore,wegenerateandmanuallyevaluatehard\n4.1 Symmetry\nnegativesforKGCevaluation.\nSymmetric relations are relations r for which\nGeneration Togeneratehardnegatives,weused (h,r,t) ∈ G implies (t,r,h) ∈ G. For each rela-\neach pre-trained embedding model from § 5.2 to tion,wecomputethenumberofits(head,tail)pairs\npredicttailentitiesoftriplesin CODEX.Foreach thatoverlapwithits(tail,head)pairs,dividedby\nmodel,wetookascandidatenegativesthetriples thetotalnumberofpairs,andtakethosewith50%\n(h,r,tˆ)forwhich(i)thetypeofthepredictedtail\noverlaporhigherassymmetric. CODEX datasets\nentity tˆmatched the type of the true tail entity t; havefivesuchrelations:diplomaticrelation,shares\n(ii)tˆwasrankedinthetop-10predictionsbythat\nborderwith,sibling,spouse,andunmarriedpart-\nmodel;and(iii)(h,r,tˆ)wasnotobservedinG.\nner. Table 4 gives the proportion of triples con-\ntainingsymmetricrelationsperdataset.Symmetric\nAnnotation We manually labeled all candidate\npatternsaremoreprevalentin CODEX-S,whereas\nnegative triples generated for CODEX-S and\nthelargerdatasetsaremostlyantisymmetric,i.e.,\nCODEX-M as true or false using the guidelines\n(h,r,t) ∈ Gimplies(t,r,h) (cid:54)∈ G.\nprovided in Appendix C.3 We randomly selected\namong the triples labeled as false to create val- 4.2 Composition\nidation and test negatives for CODEX-S and\nCompositionalitycapturespathrulesoftheform\nCODEX-M, examples of which are given in Ta-\n(h,r ,x ),...,(x ,r ,t) → (h,r,t). To learn\n1 1 n n\ntheserules,modelsmustbecapableof“multi-hop”\n3Wearecurrentlyinvestigatingmethodsforobtaininghigh-\nqualitycrowdsourcedannotationsofnegativesforCODEX-L. reasoningonknowledgegraphs(Guuetal.,2015).",
    "tables": [],
    "text_stats": {
      "word_count": 282,
      "char_count": 4043,
      "line_count": 71,
      "table_count": 0
    }
  },
  {
    "page_number": 5,
    "text": "Table 4: Relation patterns in CODEX. For symmetry, mean reciprocal rank (MRR) and hits@k. MRR\nwegivetheproportionoftriplescontainingasymmet-\nistheaveragereciprocalofeachground-truthen-\nricrelation.Forcomposition,wegivetheproportionof\ntity’srankoverall(?,r,t)and(h,r,?)testtriples.\ntriplesparticipatinginaruleoflengthtwoorthree.\nHits@k measures the proportion of test triples\nforwhichtheground-truthentityisrankedinthe\nCODEX-S CODEX-M CODEX-L\ntop-k predicted entities. In computing these met-\nSymmetry 17.46% 4.01% 3.29%\nrics, we exclude the predicted entities for which\nComposition 10.09% 16.55% 31.84%\n(hˆ,r,t) ∈ G or (h,r,tˆ) ∈ G so that known posi-\ntivetriplesdonotartificiallylowerrankingscores.\nTo identify compositional paths, we use the Thisiscalled“filtering”(Bordesetal.,2013).\nAMIE3 system (Lajus et al., 2020), which out-\nTriple classification Given a triple (h,r,t), the\nputsruleswithconfidencescoresthatcapturehow\ngoal of triple classification is to predict a corre-\nmanytimesthoserulesareseenversusviolated,to\nsponding label y ∈ {−1,1}. Since knowledge\nidentify paths of lengths two and three; we omit\ngraphembeddingmodelsoutputreal-valuedscores\nlonger paths as they are relatively costly to com-\nfor triples, we convert these scores into labels by\npute.Weidentify26,44,and93rulesinCODEX-S,\nselecting a decision threshold per relation on the\nCODEX-M,and CODEX-L,respectively,withav-\nvalidationsetsuchthatvalidationaccuracyismax-\nerage confidence (out of 1) of 0.630, 0.556, and\nimized for the model in question. A similar ap-\n0.459. Table 4 gives the percentage of triples per\nproachwasusedbySocheretal.(2013).\ndatasetparticipatinginadiscoveredrule.\nWecompareresultsonthreesetsofevaluation\nEvidently,compositionisespeciallyprevalentin\nnegatives: (1) We generate one negative per pos-\nCODEX-L.Anexamplerulein CODEX-L is“if\nitive by replacing the positive triple’s tail entity\nX was founded by Y, and Y’s country of citizen-\nby a tail entity t(cid:48) sampled uniformly at random;\nshipisZ,then thecountry[i.e., oforigin]ofX is\n(2)Wegeneratenegativesbysamplingtailentities\nZ” (confidence 0.709). We release these rules as\naccording to their relative frequency in the tail\npartof CODEX forfurtherdevelopmentofKGC\nslotofalltriples;and(3)WeusetheCODEXhard\nmethodologiesthatincorporateorlearnrules.\nnegatives.WemeasureaccuracyandF1score.\n5 Benchmarking\n5.2 Models\nNext,webenchmarkperformanceon CODEX for We compare the following embedding methods:\nthetasksoflinkpredictionandtripleclassification. RESCAL (Nickel et al., 2011), TransE (Bordes\nTo ensure that models are fairly and accurately et al., 2013), ComplEx (Trouillon et al., 2016),\ncompared,wefollowRuffinellietal.(2020),who ConvE(Dettmersetal.,2018),andTuckER(Bal-\nconductedwhatis (tothebestof ourknowledge) azevicetal.,2019b).Thesemodelsrepresentsev-\nthe largest-scale hyperparameter tuning study of eralclassesofarchitecture,fromlinear(RESCAL,\nknowledgegraphembeddingstodate. TuckER, ComplEx) to translational (TransE) to\nNote that CODEX can be used to evaluate any nonlinear/learned(ConvE).AppendixDprovides\ntype of KGC method. However, we focus on em- morespecificsoneachmodel.\nbeddings in this section due to their widespread\n5.3 Modelselection\nusageinmodernNLP(Jietal.,2020).\nAsrecentstudieshaveobservedthattrainingstrate-\n5.1 Tasks gies are equally, if not more, important than ar-\nchitectureforlinkprediction(Kadlecetal.,2017;\nLinkprediction Thelinkpredictiontaskiscon-\nLacroix et al., 2018; Ruffinelli et al., 2020), we\nductedasfollows:Givenatesttriple(h,r,t),we\nsearchacrossalargerangeofhyperparametersto\nconstruct queries (?,r,t) and (h,r,?). For each\nensureatrulyfaircomparison.Tothisendweuse\nquery,amodelscorescandidatehead(tail)entities\nhˆ (tˆ)accordingtoitsbeliefthathˆ (tˆ)completesthe thePyTorch-basedLibKGEframeworkfortraining\nandselectingknowledgegraphembeddings.4Inthe\ntriple(i.e.,answersthequery).Thegoalisoflink\npredictionistoranktruetriples(hˆ,r,t)or(h,r,tˆ) remainderofthissectionweoutlinethemostim-\nportantparametersofourmodelselectionprocess.\nhigherthanfalseandunseentriples.\nLink prediction performance is evaluated with 4https://github.com/uma-pi1/kge",
    "tables": [],
    "text_stats": {
      "word_count": 383,
      "char_count": 4144,
      "line_count": 81,
      "table_count": 0
    }
  },
  {
    "page_number": 6,
    "text": "Table5:ComparisonoflinkpredictionperformanceonCODEX.\nCODEX-S CODEX-M CODEX-L\nMRR Hits@1 Hits@10 MRR Hits@1 Hits@10 MRR Hits@1 Hits@10\nRESCAL 0.404 0.293 0.623 0.317 0.244 0.456 0.304 0.242 0.419\nTransE 0.354 0.219 0.634 0.303 0.223 0.454 0.187 0.116 0.317\nComplEx 0.465 0.372 0.646 0.337 0.262 0.476 0.294 0.237 0.400\nConvE 0.444 0.343 0.635 0.318 0.239 0.464 0.303 0.240 0.420\nTuckER 0.444 0.339 0.638 0.328 0.259 0.458 0.309 0.244 0.430\nTable10inAppendixFgivesfurtherdetailsandall\nhyperparameterrangesandvalues.Allexperiments\nwere run on a single NVIDIA Tesla V100 GPU\nwith16GBofRAM.\nTrainingnegatives Givenasetofpositivetrain-\ning triples {(h,r,t)}, we compare three types\nof negative sampling strategy implemented by Figure1:DistributionofvalidationMRR,CODEX-M.\nLibKGE: (a) NegSamp, or randomly corrupting\nheadentitieshortailentitiesttocreatenegatives;\n5.4 Linkpredictionresults\n(b)1vsAll,ortreatingallpossiblehead/tailcorrup-\nTable5giveslinkpredictionresults.Wefindthat\ntionsof(h,r,t)asnegatives,includingthecorrup-\nComplExisthebestatmodelingsymmetryand\ntionsthatareactuallypositives;and(c)KvsAll,or\nantisymmetry,andindeeditwasdesignedspecifi-\ntreatingbatchesofhead/tailcorruptionsnotseenin\ncallytoimproveuponbilinearmodelsthatdonot\ntheknowledgegraphasnegatives.\ncapturesymmetry,likeDistMult(Trouillonetal.,\n2016).Assuch,itperformsthebeston CODEX-S,\nLoss functions We consider the following loss\nwhichhasthehighestproportionofsymmetricrela-\nfunctions:(i)MRormarginranking,whichaimsto\ntions.Forexample,onthemostfrequentsymmetric\nmaximizeamarginbetweenpositiveandnegative relation(diplomaticrelation),ComplExachieves\ntriples; (ii) BCE or binary cross-entropy, which\n0.859MRR,comparedto0.793forConvE,0.490\nis computed by applying the logistic sigmoid to\nforRESCAL,and0.281forTransE.\ntriplescores;and(iii)CEorcross-entropybetween\nBy contrast,TuckER isstrongest at modeling\nthe softmax over the entire distribution of triple\ncompositional relations, so it performs best on\nscores and the label distribution over all triples,\nCODEX-L,whichhasahighdegreeofcomposi-\nnormalizedtosumtoone.\ntionality.Forexample,onthemostfrequentcom-\npositional relation in CODEX-L (languages spo-\nSearchstrategies WeselectmodelsusingtheAx ken, written, or signed), TuckER achieves 0.465\nplatform, which supports hyperparameter search MRR,comparedto0.464forRESCAL,0.463for\nusing both quasi-random sequences of generated ConvE,0.456forComplEx,and0.385forTransE.\nconfigurations and Bayesian optimization (BO) Bycontrast,sinceCODEX-Mismostlyasymmet-\nwith Gaussian processes.5 At a high level, for ricandnon-compositional,ComplExperformsbest\neach dataset and model, we generate both quasi- becauseofitsabilitytomodelasymmetry.\nrandomandBOtrialspernegativesamplingand\nEffect of hyperparameters As shown by Fig-\nlossfunctioncombination,ensuringthatwesearch\nure 1, hyperparameters have a strong impact on\noverawiderangeofhyperparametersfordifferent\nlinkpredictionperformance:ValidationMRRfor\ntypesoftrainingstrategy.AppendixFprovidesspe-\nallmodelsvariesbyover30percentagepointsde-\ncificdetailsonthesearchstrategyforeachdataset,\npendingonthetrainingstrategyandinputconfig-\nwhichwasdeterminedaccordingtoresourcecon-\nuration. This finding is consistent with previous\nstraintsandobservedperformancepatterns.\nobservationsintheliterature(Kadlecetal.,2017;\nRuffinelli et al., 2020). Appendix F provides the\n5https://ax.dev/ bestconfigurationsforeachmodel.",
    "tables": [],
    "text_stats": {
      "word_count": 268,
      "char_count": 3418,
      "line_count": 69,
      "table_count": 0
    }
  },
  {
    "page_number": 7,
    "text": "Table6:ComparisonoftripleclassificationperformanceonCODEXbynegativegenerationstrategy.\nCODEX-S CODEX-M\nUniform Relativefreq. Hardneg. Uniform Relativefreq. Hardneg.\nAcc. F1 Acc. F1 Acc. F1 Acc. F1 Acc. F1 Acc. F1\nRESCAL 0.972 0.972 0.916 0.920 0.843 0.852 0.977 0.976 0.921 0.922 0.818 0.815\nTransE 0.974 0.974 0.919 0.923 0.829 0.837 0.986 0.986 0.932 0.933 0.797 0.803\nComplEx 0.975 0.975 0.927 0.930 0.836 0.846 0.984 0.984 0.930 0.933 0.824 0.818\nConvE 0.972 0.972 0.921 0.924 0.841 0.846 0.979 0.979 0.934 0.935 0.826 0.829\nTuckER 0.973 0.973 0.917 0.920 0.840 0.846 0.977 0.977 0.920 0.922 0.823 0.816\nOverall,wefindthatthechoiceoflossfunction whenthenegativesareplausiblebuttrulyfalse.\nin particular significantly impacts model perfor-\nmance. Each model consistently achieved its re- 6 Comparativecasestudy\nspective peak performance with cross-entropy\nFinally,weconductacomparativeanalysisbetween\n(CE)loss,afindingwhichiscorroboratedbysev-\neralotherKGCcomparisonpapers(Kadlecetal.,\nCODEX-M and FB15K-237 (§ 2.1) to demon-\n2017;Ruffinellietal.,2020;Jainetal.,2020).As\nstrate the unique value of CODEX. We choose\nFB15K-237becauseitisthemostpopularencyclo-\nfarasnegativesamplingtechniques,wedonotfind\npedic KGC benchmark after FB15K, which was\nthatasinglestrategyisdominant,suggestingthat\nalreadyshowntobeaneasydatasetbyToutanova\nthechoiceoflossfunctionismoreimportant.\nandChen(2015).Wechoose CODEX-M because\n5.5 Tripleclassificationresults itistheclosestinsizeto FB15K-237.\nTable6givestripleclassificationresults.Evidently,\n6.1 Content\ntriple classification on randomly generated neg-\natives is a nearly-solved task. On negatives gen- Wefirstcomparethecontentin CODEX-M,which\nerated uniformly at random, performance scores is extracted from Wikidata, with that of FB15K-\narenearlyidenticalatalmost100%accuracy.Even 237,whichisextractedfromFreebase.Forbrevity,\nwith a negative sampling strategy “smarter” than Figure2comparesthetop-15relationsbymention\nuniformrandom,allmodelsperformwell. count in the two datasets. Appendix E provides\nmorecontentcomparisons.\nHard negatives Classification performance de-\ngenerates considerably on our hard negatives, Diversity ThemostcommonrelationinCODEX-\naround 8 to 11 percentage points from relative M is occupation, which is because most people\nfrequency-basedsamplingand13to19percentage\nonWikidatahavemultipleoccupationslisted.By\npointsfromuniformlyrandomsampling.Relative\ncontrast,thefrequentrelationsinFB15K-237are\nperformancealsovaries:Incontrasttoourlinkpre-\nmostlyrelatedtoawardsandfilm.Infact,over25%\ndictiontaskinwhichComplExandTuckERwere ofalltriplesinFB15K-237belongtothe/award\nby far the strongest models, RESCAL is slightly\nrelationdomain,suggestingthat CODEXcoversa\nstrongerontheCODEX-Shardnegatives,whereas\nmorediverseselectionofcontent.\nConvEperformsbestontheCODEX-Mhardneg-\natives. These results indicate that triple classifi- Interpretability TheFreebase-stylerelationsare\ncation is indeed a distinct task that requires dif- alsoarguablylessinterpretablethanthoseinWiki-\nferent architectures and, in many cases, different data.WhereasWikidatarelationshaveconcisenat-\ntrainingstrategies(AppendixF). ural language labels, the Freebase relation labels\nWebelievethatfewrecentworksusetripleclas- arehierarchical,oftenatfiveorsixlevelsofhier-\nsificationasanevaluationtaskbecauseofthelack archy (Figure 2). Moreover, all relations in Wiki-\noftruehardnegativesinexistingbenchmarks.Early dataarebinary,whereassomeFreebaserelations\nworksreportedhightripleclassificationaccuracy aren-nary(Tanonetal.,2016),meaningthatthey\non sampled negatives (Socher et al., 2013; Wang connectmorethantwoentities.Therelationscon-\netal.,2014),perhapsleadingthecommunitytobe- tainingadot(“.”)aresuchn-naryrelations,andare\nlievethatthetaskwasnearlysolved.However,our difficulttoreasonaboutwithoutunderstandingthe\nresultsdemonstratethatthetaskisfarfromsolved structureofFreebase,whichhasbeendeprecated.",
    "tables": [],
    "text_stats": {
      "word_count": 314,
      "char_count": 3932,
      "line_count": 63,
      "table_count": 0
    }
  },
  {
    "page_number": 8,
    "text": "Figure2:Top-15mostfrequentrelationsinCODEX-MandFB15K-237.\nWefurtherdiscusstheimpactofsuchn-naryrela- Table7:Overallperformance(MRR)ofourfrequency\nbaseline versus the best embedding nodel per bench-\ntionsforlinkpredictioninthefollowingsection.\nmark.“Improvement”referstotheimprovementofthe\nembeddingoverthebaseline.\n6.2 Difficulty\nNext,wecomparethedatasetsinalinkprediction\nBaseline Embedding Improvement\ntasktoshowthat CODEX-M ismoredifficult.\nFB15K-237 0.236 0.356 +0.120\nCODEX-M 0.135 0.337 +0.202\nBaseline We devise a “non-learning” link pre-\ndictionbaseline.Let(h,r,?)beatestquery.Our\nbaselinescorescandidatetailentitiesbytheirrela-\ntivefrequencyinthetailslotofalltrainingtriples\nmentioningr,filteringouttailentitiestforwhich\n(h,r,t) is already observed in the training set. If\nalltailentitiestarefilteredout,wescoreentities\nbyfrequencybeforefiltering.Thelogicofourap-\nproachworksinreversefor(?,r,t)queries.Ineval-\nuatingourbaseline,wefollowLibKGE’sprotocol\nfor breaking ties in ranking (i.e., for entities that\nappearwithequalfrequency)bytakingthemean\nFigure 3: Improvement in MRR of the embedding\nrankofallentitieswiththesamescore.\noverourfrequencybaselineperrelationtype.Negative\nmeans that our baseline outperforms the embedding.\nSetup Wecompareourbaselinetothebestpre-\nThe medians are 8.27 and 20.04 percentage points on\ntrainedembeddingmodelperdataset:RESCALfor\nFB15K-237andCODEX-M,respectively.\nFB15K-237,whichwasreleasedbyRuffinellietal.\n(2020), and ComplEx for CODEX-M. We evalu-\nateperformancewithMRRandHits@10.Beyond\nembeddingonFB15K-237forsomerelationtypes.\noverallperformance,wealsocomputeper-relation\nTo further explore these cases, Figure 4 gives\nimprovementoftherespectiveembeddingoverour\nthe empirical cumulative distribution function of\nbaselineintermsofpercentagepointsMRR.This\nimprovement,whichshowsthepercentageoftest\nmeasurestheamountoflearningbeyondfrequency\ntriplesforwhichthelevelofimprovementisless\nstatisticsnecessaryforeachrelation.\nthan or equal to a given value on each dataset.\nResults and discussion Table 7 compares the Surprisingly,theimprovementforbothMRRand\noverallperformanceofourbaselineversusthebest Hits@10 is less than five percentage points for\nembeddingperdataset,andFigure3showstheim-\nnearly40%ofFB15K-237’stestset,andiszeroor\nprovement of the respective embedding over our\nnegative15%ofthetime.Bycontrast,ourbaseline\nbaselineperrelationtypeoneachdataset.Theim- is significantly weaker than the strongest embed-\nprovement of the embedding is much smaller on dingmethodon CODEX-M.\nFB15K-237thanCODEX-M,andinfactourbase- Thedisparityinimprovementisduetotworela-\nlineperformsonparwithorevenoutperformsthe tionpatternsprevalentin FB15K-237:",
    "tables": [],
    "text_stats": {
      "word_count": 186,
      "char_count": 2684,
      "line_count": 56,
      "table_count": 0
    }
  },
  {
    "page_number": 9,
    "text": "Weconcludethatwhile FB15K-237isavaluable\ndataset, CODEX is more appropriately difficult\nfor link prediction. Additionally, we note that in\nFB15K-237,allvalidationandtesttriplescontain-\ning entity pairs directly linked in the training set\nweredeleted(ToutanovaandChen,2015),meaning\nFigure 4: Empirical CDF of improvement of the best thatsymmetrycannotbetestedforinFB15K-237.\nembeddingoverourfrequencybaseline.\nGiventhatCODEXdatasetscontainbothsymme-\ntryandcompositionality,CODEXismoresuitable\nfor assessing how well models can learn relation\n• Skewed relations FB15K-237 contains\npatternsthatgobeyondfrequency.\nmanyrelationsthatareskewedtowardasingle\nheadortailentity.Forexample,ourbaseline\n7 Conclusionandoutlook\nachievesperfectperformanceoverall(h,r,?)\nqueries for the /common/topic/webpage. We present CODEX, a set of knowledge graph\n/common/webpage/categoryrelationbecause COmpletion Datasets EXtracted from Wikidata\nthis relation has only one unique tail entity. andWikipedia,andshowthatCODEXissuitable\nAnotherexampleofahighlyskewedrelation formultipleKGCtasks.Wereleasedata,code,and\nin FB15K-237is/people/person/gender,for pretrained models for use by the community at\nwhich78.41%oftailsaretheentitymale.In https://bit.ly/2EPbrJs. Some promising future di-\nfact, 11 relations in FB15K-237 have only rectionson CODEXinclude:\none unique tail entity, accounting for 3.22%\nof all tail queries in FB15K-237. Overall, • Better model understanding CODEX can\n15.98% of test triples in FB15K-237 con- beusedtoanalyzetheimpactofhyperparam-\ntain relations that are skewed 50% or more eters,trainingstrategies,andmodelarchitec-\ntoward a single head or tail entity, whereas turesinKGCtasks.\nonly1.26%oftesttriplesin CODEX-M con-\ntainsuchskewedrelations. • Revival of triple classification We encour-\nagetheuseoftripleclassificationon CODEX\nin addition to link prediction because it di-\n• Fixed-set relations Around 12.7% of test\nrectlytestsdiscriminativepower.\nqueries in FB15K-237 contain relation\ntypes that connect entities to fixed sets of\n• Fusing text and structure Including text in\nvalues. As an example, each head entity\nboth the link prediction and triple classifica-\nthat participates in the FB15K-237 relation\ntiontasksshouldsubstantiallyimproveperfor-\n/travel/travel_destination/climate./travel/\nmance(Toutanovaetal.,2015).Furthermore,\ntravel_destination_monthly_climate/month\ntextcanbeusedforfew-shotlinkprediction,\nisconnectedtothesame12tails(monthsof\nanemergingresearchdirection(Xiongetal.,\nthe year) throughout train, validation, and\n2017;ShiandWeninger,2017).\ntest. This makes prediction trivial with our\nbaseline: By filtering out the tail entities\nOverall,wehopethatCODEXwillprovideaboost\nalready seen in train, only a few (or even\ntoresearchinKGC,whichwillinturnimpactmany\none) candidate tail(s) are left in test, and\notherfieldsofartificialintelligence.\nthe answer is guaranteed to be within these\ncandidates. These relations only occur in\nAcknowledgments\nFB15K-237 because of the way the dataset\nwasconstructedfromFreebase.Specifically, TheauthorsthankMichałRybakandXinyi(Carol)\nFreebase used a special type of entity Zhengfortheircontributions.Thismaterialissup-\ncalled Compound Value Type (CVT) as an portedbytheNationalScienceFoundationunder\nintermediarynodeconnectingn-aryrelations. GrantNo.IIS1845491,ArmyYoungInvestigator\nBinary relations were created by traversing AwardNo.W911NF1810397,andanNSFGradu-\nthrough CVTs, yielding some relations that ateResearchFellowship.\nconnectentitiestofixedsetsofvalues.",
    "tables": [],
    "text_stats": {
      "word_count": 334,
      "char_count": 3525,
      "line_count": 68,
      "table_count": 0
    }
  },
  {
    "page_number": 10,
    "text": "References Xavier Glorot and Yoshua Bengio. 2010. Understand-\ningthedifficultyoftrainingdeepfeedforwardneural\nFarahnaz Akrami, Mohammed Samiul Saeef,\nnetworks. InAISTATS.\nQingheng Zhang, Wei Hu, and Chengkai Li.\n2020. Realistic re-evaluation of knowledge graph Leo A Goodman. 1961. Snowball sampling. Ann.\ncompletion methods: An experimental study. In Math.Stat.\nSIGMOD.\nLingbing Guo, Zequn Sun, and Wei Hu. 2019. Learn-\nIvana Balazevic, Carl Allen, and Timothy Hospedales. ing to exploit long-term relational dependencies in\n2019a. Multi-relationalpoincarégraphembeddings. knowledgegraphs. InICML.\nInNeurIPS.\nShu Guo, Quan Wang, Bin Wang, Lihong Wang, and\nIvana Balazevic, Carl Allen, and Timothy Hospedales. Li Guo. 2015. Semantically smooth knowledge\n2019b. Tucker:Tensorfactorization forknowledge graphembedding. InACL-IJCNLP.\ngraphcompletion. InEMNLP-IJCNLP.\nShu Guo, Quan Wang, Lihong Wang, Bin Wang, and\nTrapit Bansal, Da-Cheng Juan, Sujith Ravi, and An- Li Guo. 2018. Knowledge graph embedding with\ndrewMcCallum.2019. A2n:Attendingtoneighbors iterativeguidancefromsoftrules. InAAAI.\nforknowledgegraphinference. InACL.\nKelvin Guu, John Miller, and Percy Liang. 2015.\nVladimirBatageljandMatjažZaveršnik.2011. Fastal- Traversing knowledge graphs in vector space. In\ngorithms for determining (generalized) core groups EMNLP.\ninsocialnetworks. ADAC,5(2).\nGeoffrey E Hinton. 1986. Learning distributed repre-\nKurt Bollacker, Colin Evans, Praveen Paritosh, Tim sentationsofconcepts. InCogSci.\nSturge,andJamieTaylor.2008. Freebase:acollab-\noratively created graph database for structuring hu- Prachi Jain, Sushant Rathi, Mausam, and Soumen\nmanknowledge. InSIGMOD. Chakrabarti. 2020. Knowledge base completion:\nBaseline strikes back (again). arXiv preprint\nAntoine Bordes, Nicolas Usunier, Alberto Garcia- arXiv:2005.00804.\nDuran, Jason Weston, and Oksana Yakhnenko.\n2013. Translating embeddings for modeling multi- GuoliangJi,ShizhuHe,LihengXu,KangLiu,andJun\nrelationaldata. InNeurIPS. Zhao. 2015. Knowledge graph embedding via dy-\nnamicmappingmatrix. InACL-IJCNLP.\nGuillaume Bouchard, Sameer Singh, and Theo Trouil-\nlon.2015. Onapproximatereasoningcapabilitiesof ShaoxiongJi,ShiruiPan,ErikCambria,PekkaMartti-\nlow-rankvectorspaces. InAAAISpringSymposium nen,andPhilipSYu.2020. Asurveyonknowledge\nSeries. graphs:Representation,acquisitionandapplications.\narXivpreprintarXiv:2002.00388.\nLiweiCaiandWilliamYangWang.2018. Kbgan:Ad-\nversariallearningforknowledgegraphembeddings. YantaoJia,YuanzhuoWang,HailunLin,XiaolongJin,\nInNAACL-HLT. and Xueqi Cheng. 2016. Locally adaptive transla-\ntionforknowledgegraphembedding. InAAAI.\nRajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer,\nLuke Vilnis, Ishan Durugkar, Akshay Krishna- Xiaotian Jiang, Quan Wang, and Bin Wang. 2019.\nmurthy,AlexSmola,andAndrewMcCallum.2018. Adaptive convolution for multi-relational learning.\nGo for a walk and arrive at the answer: Reasoning InNAACL-HLT.\nover paths in knowledge bases using reinforcement\nlearning. InICLR. Rudolf Kadlec, Ondrej Bajgar, and Jan Kleindienst.\n2017. Knowledgebasecompletion:Baselinesstrike\nTim Dettmers, Pasquale Minervini, Pontus Stenetorp, back. InACLRepL4NLPWorkshop.\nand Sebastian Riedel. 2018. Convolutional 2d\nknowledgegraphembeddings. InAAAI. Lucie-AiméeKaffee,AlessandroPiscopo,PavlosVou-\ngiouklis, Elena Simperl, Leslie Carr, and Lydia\nTakuma Ebisu and Ryutaro Ichise. 2018. Toruse: Pintscher. 2017. A glimpse into babel: an analysis\nKnowledge graph embedding on a lie group. In ofmultilingualityinwikidata. InOpenSym.\nAAAI.\nSeyedMehranKazemiandDavidPoole.2018. Simple\nLuis Antonio Galárraga, Christina Teflioudi, Katja embeddingforlinkpredictioninknowledgegraphs.\nHose, and Fabian Suchanek. 2013. Amie: associa- InNeurIPS.\ntionruleminingunderincompleteevidenceinonto-\nlogicalknowledgebases. InWWW. Charles Kemp, Joshua B Tenenbaum, Thomas L Grif-\nfiths, Takeshi Yamada, and Naonori Ueda. 2006.\nAlberto Garcia-Duran, Antoine Bordes, and Nicolas Learning systems of concepts with an infinite rela-\nUsunier.2015. Composingrelationshipswithtrans- tionalmodel. InAAAI.\nlations. InEMNLP.",
    "tables": [],
    "text_stats": {
      "word_count": 424,
      "char_count": 4087,
      "line_count": 67,
      "table_count": 0
    }
  },
  {
    "page_number": 11,
    "text": "Timothee Lacroix, Nicolas Usunier, and Guillaume Maximilian Nickel, Kevin Murphy, Volker Tresp, and\nObozinski. 2018. Canonical tensor decomposition Evgeniy Gabrilovich. 2015. A review of relational\nforknowledgebasecompletion. InICML. machine learning for knowledge graphs. Proc.\nIEEE,104(1).\nJonathan Lajus, Luis Galárraga, and Fabian M.\nSuchanek. 2020. Fast and exact rule mining with Maximilian Nickel, Lorenzo Rosasco, and Tomaso\namie3. InESWC. Poggio. 2016. Holographic embeddings of knowl-\nedgegraphs. InAAAI.\nXi Victoria Lin, Richard Socher, and Caiming Xiong.\n2018. Multi-hop knowledge graph reasoning with Maximilian Nickel, Volker Tresp, and Hans-Peter\nrewardshaping. InEMNLP. Kriegel. 2011. A three-way model for collective\nlearningonmulti-relationaldata. InICML.\nYankaiLin,ZhiyuanLiu,HuanboLuan,MaosongSun,\nSiweiRao,andSongLiu.2015a. Modelingrelation Pouya Pezeshkpour, Yifan Tian, and Sameer Singh.\npathsforrepresentationlearningofknowledgebases. 2020. Revisitingevaluationofknowledgebasecom-\nInEMNLP. pletionmodels. InAKBC.\nYankai Lin, Zhiyuan Liu, and Maosong Sun. 2016. Daniel Ruffinelli, Samuel Broscheit, and Rainer\nKnowledgerepresentationlearningwithentities,at- Gemulla. 2020. You can teach an old dog new\ntributesandrelations. InIJCAI. tricks!ontrainingknowledgegraphembeddings. In\nICLR.\nYankaiLin,ZhiyuanLiu,MaosongSun,YangLiu,and\nXuanZhu.2015b. Learningentityandrelationem- Baoxu Shi and Tim Weninger. 2017. Proje: Embed-\nbeddingsforknowledgegraphcompletion. InAAAI. dingprojectionforknowledgegraphcompletion. In\nAAAI.\nHanxiao Liu, Yuexin Wu, and Yiming Yang. 2017.\nAnalogical inference for multi-relational embed- RichardSocher,DanqiChen,ChristopherDManning,\ndings. InICML. and Andrew Ng. 2013. Reasoning with neural ten-\nsor networks for knowledge base completion. In\nFarzaneh Mahdisoltani, Joanna Biega, and Fabian NeurIPS.\nSuchanek. 2014. Yago3: A knowledge base from\nmultilingualwikipedias. InCIDR. Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian\nTang. 2019. Rotate: Knowledge graph embedding\nAlexa T McCray. 2003. An upper-level ontology for byrelationalrotationincomplexspace. InICLR.\nthe biomedical domain. Comp. Funct. Genomics,\n4(1). Thomas Pellissier Tanon, Denny Vrandecˇic´, Sebas-\ntianSchaffert,ThomasSteiner,andLydiaPintscher.\nChristianMeilicke,ManuelFink,YanjieWang,Daniel 2016. From freebase to wikidata: The great migra-\nRuffinelli, Rainer Gemulla, and Heiner Stucken- tion. InWWW.\nschmidt. 2018. Fine-grained evaluation of rule-\nandembedding-basedsystemsforknowledgegraph Kristina Toutanova and Danqi Chen. 2015. Observed\ncompletion. InISWC. versus latent features for knowledge base and text\ninference. InACLCVSCWorkshop.\nGeorgeAMiller.1998. WordNet:Anelectroniclexical\ndatabase. MITpress. Kristina Toutanova, Danqi Chen, Patrick Pantel, Hoi-\nfungPoon,PallaviChoudhury,andMichaelGamon.\nTom Mitchell, William Cohen, Estevam Hruschka, 2015. Representingtextforjointembeddingoftext\nParthaTalukdar,BishanYang,JustinBetteridge,An- andknowledgebases. InEMNLP.\ndrewCarlson,BhanavaDalvi,MattGardner,Bryan\nKisiel, et al. 2018. Never-ending learning. CACM, Théo Trouillon, Éric Gaussier, Christopher R Dance,\n61(5):103–115. and Guillaume Bouchard. 2019. On inductive abil-\nities of latent factor models for relational learning.\nDeepak Nathani, Jatin Chauhan, Charu Sharma, and JAIR,64.\nManohar Kaul. 2019. Learning attention-based\nembeddings for relation prediction in knowledge ThéoTrouillon,JohannesWelbl,SebastianRiedel,Éric\ngraphs. InACL. Gaussier,andGuillaumeBouchard.2016. Complex\nembeddingsforsimplelinkprediction. InICML.\nDat Quoc Nguyen, Kairit Sirts, Lizhen Qu, and Mark\nJohnson. 2016. Stranse: a novel embedding model Shikhar Vashishth, Soumya Sanyal, Vikram Nitin,\nofentitiesandrelationshipsinknowledgebases. In Nilesh Agrawal, and Partha Talukdar. 2020a. In-\nNAACL-HLT. teracte: Improving convolution-based knowledge\ngraphembeddingsbyincreasingfeatureinteractions.\nTuDinhNguyen,DatQuocNguyen,DinhPhung,etal. InAAAI.\n2018. A novel embedding model for knowledge\nbase completion based on convolutional neural net- ShikharVashishth,SoumyaSanyal,VikramNitin,and\nwork. InNAACL-HLT. Partha Talukdar. 2020b. Composition-based multi-\nrelationalgraphconvolutionalnetworks. InICLR.",
    "tables": [],
    "text_stats": {
      "word_count": 428,
      "char_count": 4218,
      "line_count": 66,
      "table_count": 0
    }
  },
  {
    "page_number": 12,
    "text": "Denny Vrandecˇic´ and Markus Krötzsch. 2014. Wiki- Han Xiao, Minlie Huang, and Xiaoyan Zhu. 2016b.\ndata: a free collaborative knowledgebase. CACM, Transg: A generative model for knowledge graph\n57(10). embedding. InACL.\nThanh Vu, Tu Dinh Nguyen, Dat Quoc Nguyen, Dinh Ruobing Xie, Zhiyuan Liu, and Maosong Sun. 2016.\nPhung, et al. 2019. A capsule network-based em- Representation learning of knowledge graphs with\nbeddingmodelforknowledgegraphcompletionand hierarchicaltypes. InIJCAI.\nsearchpersonalization. InNAACL-HLT.\nWenhanXiong,ThienHoang,andWilliamYangWang.\nQuan Wang, Zhendong Mao, Bin Wang, and Li Guo. 2017. Deeppath: A reinforcement learning method\n2017. Knowledge graph embedding: A survey of forknowledgegraphreasoning. InEMNLP.\napproachesandapplications. TKDE,29(12).\nCanranXuandRuijiangLi.2019. Relationembedding\nQuanWang,BinWang,andLiGuo.2015. Knowledge withdihedralgroupinknowledgegraph. InACL.\nbasecompletionusingembeddingsandrules. InIJ-\nCAI. Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng\nGao, and Li Deng. 2015. Embedding entities and\nWilliam Yang Wang and William W Cohen. 2016. relations for learning and inference in knowledge\nLearningfirst-orderlogicembeddingsviamatrixfac- bases. InICLR.\ntorization. InIJCAI.\nShuai Zhang, Yi Tay, Lina Yao, and Qi Liu. 2019.\nZhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Quaternion knowledge graph embeddings. In\nChen.2014. Knowledgegraphembeddingbytrans- NeurIPS.\nlatingonhyperplanes. InAAAI.\nZhao Zhang, Fuzhen Zhuang, Hengshu Zhu, Zhiping\nHan Xiao, Minlie Huang, and Xiaoyan Zhu. 2016a. Shi, Hui Xiong, and Qing He. 2020. Relational\nFromonepointtoamanifold:knowledgegraphem- graphneuralnetworkwithhierarchicalattentionfor\nbeddingforpreciselinkprediction. InIJCAI. knowledgegraphcompletion. InAAAI.",
    "tables": [],
    "text_stats": {
      "word_count": 195,
      "char_count": 1763,
      "line_count": 27,
      "table_count": 0
    }
  },
  {
    "page_number": 13,
    "text": "A Literaturereview Examples Falsetriplesmayhaveproblemswith\ngrammar, factual content, or both. Examples of\nTable8providesanoverviewofknowledgegraph\ngrammatically incorrect triples are those whose\nembeddingpaperswithrespecttodatasetsandeval-\nentityorrelationtypesdonotmakesense,forex-\nuationtasks.Inourreview,weonlyconsiderpapers\nample:\npublishedbetween2014and2020inthemainpro-\nceedings of conferences where KGC embedding • (UnitedStatesofAmerica,continent,science\npapersaremostlikelytoappear:Artificialintelli- fictionwriter)\ngence (AAAI, IJCAI), machine learning (ICML,\nICLR,NeurIPS),andnaturallanguageprocessing • (Mohandas Karamchand Gandhi, medical\n(ACL,EMNLP,NAACL).\ncondition,BritishRaj)\nThe main evaluation benchmarks are FB15K\n• (Canada, foundational text, Vietnamese cui-\n(Bordes et al., 2013), WN18 (Bordes et al.,\nsine)\n2013), FB15K-237(ToutanovaandChen,2015),\nWN18RR(Dettmersetal.,2018),FB13(Socher Examples of grammatically correct but factually\netal.,2013),WN11(Socheretal.,2013),NELL- falsetriplesinclude:\n995 (Xiong et al., 2017), YAGO3-10 (Dettmers\n• (UnitedStatesofAmerica,continent,Europe)\net al., 2018), Countries (Bouchard et al., 2015).\nUMLS (McCray, 2003), Kinship (Kemp et al.,\n• (MohandasKaramchandGandhi,countryof\n2006),Families(Hinton,1986),andotherversions\ncitizenship,Argentina)\nofNELL(Mitchelletal.,2018).\n• (Canada,foundationaltext,HarryPotterand\nB Seedsfordatacollection\ntheGobletofFire)\nTable9providesallseedentityandrelationtypes\n• (Alexander Pushkin, influenced by, Leo Tol-\nusedtocollect CODEX.Eachtypeisgivenfirstby\nstoy)—Pushkindiedonlyafewyearsafter\nitsnaturallanguagelabelandthenbyitsWikidata\nTolstoywasborn,sothissentenceisunlikely.\nunique ID: Entity IDs begin with Q, whereas re-\nlation (property) IDs begin with P. For the entity Noticethatinthelatterexamples,theentitytypes\ntypes that apply to people (e.g., actor, musician, matchup,butthestatementsarestillfalse.\njournalist),weretrievedseedentitiesbyquerying\nTips For triples about people’s occupation and\nWikidata using the occupation relation. For the\ngenre,trytobeasspecificaspossible.Forexample,\nentitytypesthatapplytothings(e.g.,airline,dis-\nifthetriplesays(<person>,occupation,guitarist)\nease,touristattraction),weretrievedseedentities\nbutthatpersonismainlyknownfortheirsinging,\nby querying Wikidata using the instance of and\nchoose false, even if that person plays the guitar.\nsubclassof relations.\nLikewise, if a triple says (<person>, genre, clas-\nC Negativeannotationguidelines sical) but they are mostly known for jazz music,\nchoosefalseevenif,forexample,thatpersonhad\nWe provide the annotation guidelines we used to\nclassicaltrainingintheirchildhood.\nlabelcandidatenegativetriples(§3.4).\nD Embeddingmodels\nTask Youmustlabeleachtripleaseithertrueor\nfalse. To help you find the answer, we have pro- Webrieflyoverviewthefivemodelscomparedin\nvidedyouwithWikipediaandWikidatalinksfor ourlinkpredictionandtripleclassificationtasks.\nthe entities and relations in each triple. You may\nRESCAL(Nickeletal.,2011)wasoneofthefirst\nalso search on Google for the answer, although\nknowledgegraphembeddingmodels.Althoughitis\nmostclaimsshouldberesolvableusingWikipedia\nnotoftenusedasabaseline, Ruffinellietal.(2020)\nandWikidataalone.Ifyouarenotabletofindany\nshowed that it is competitive when appropriately\nreliable,specific,clearinformationsupportingthe\ntuned.RESCALtreatsrelationallearningastensor\nclaim,choosefalse.Youmayexplainyourreason-\ndecomposition,scoringentityembeddingsh,r ∈\ningifneedbeorprovidesourcestobackupyour\nRde andrelationembeddingsR ∈ Rde×de withthe\nanswerintheoptionalexplanationcolumn.\nbilinearformh(cid:62)Rt.",
    "tables": [],
    "text_stats": {
      "word_count": 283,
      "char_count": 3621,
      "line_count": 79,
      "table_count": 0
    }
  },
  {
    "page_number": 14,
    "text": "Table 8: An overview of knowledge graph embedding papers published between 2014 and 2020 with respect to\ndatasets and evaluation tasks. Original citations for datasets are given in Appendix A. Link pred. refers to link\nprediction,andtripleclass.referstotripleclassification,bothofwhicharecoveredin§5.\nDatasets Evaluationtasks\nReference\nK51BF\n732-K51BF\n31BF\n81NW\nRR81NW\n11NW\nOther\n.derpkniL\n.ssalcelpirT\nOther\nIACJI,IAAA\nrelationextraction\n(Wangetal.,2014) (cid:51) (cid:51) (cid:51) (cid:51) FB5M (cid:51) (cid:51)\n(FB5M)\nrelationextraction\n(Linetal.,2015b) (cid:51) (cid:51) (cid:51) (cid:51) FB40K (cid:51) (cid:51)\n(FB40K)\n(Wangetal.,2015) NELL(Location,Sports) (cid:51)\n(Nickeletal.,2016) (cid:51) (cid:51) Countries (cid:51)\n(Linetal.,2016) FB24K (cid:51)\n(WangandCohen,2016) (cid:51) (cid:51) (cid:51)\n(Xiaoetal.,2016a) (cid:51) (cid:51) (cid:51) (cid:51) (cid:51) (cid:51)\n(Jiaetal.,2016) (cid:51) (cid:51) (cid:51) (cid:51) (cid:51) (cid:51)\n(Xieetal.,2016) (cid:51) FB15K+ (cid:51) (cid:51)\nfactchecking(noton\n(ShiandWeninger,2017) (cid:51) SemMedDB,DBPedia (cid:51)\nFB15K)\n(Dettmersetal.,2018) (cid:51) (cid:51) (cid:51) (cid:51) YAGO3-10,Countries (cid:51)\n(EbisuandIchise,2018) (cid:51) (cid:51) (cid:51)\n(Guoetal.,2018) (cid:51) YAGO37 (cid:51)\n(Zhangetal.,2020) (cid:51) (cid:51) (cid:51) (cid:51) (cid:51)\n(Vashishthetal.,2020a) (cid:51) (cid:51) YAGO3-10 (cid:51)\nSPIrueN,RLCI,LMCI\nruleextraction\n(Yangetal.,2015) (cid:51) (cid:51) FB15K-401 (cid:51)\n(FB15K-401)\n(Trouillonetal.,2016) (cid:51) (cid:51) (cid:51)\n(Liuetal.,2017) (cid:51) (cid:51) (cid:51)\n(KazemiandPoole,2018) (cid:51) (cid:51) (cid:51)\nNELL-995,UMLS,Kinship,\n(Dasetal.,2018) (cid:51) (cid:51) (cid:51) QA(WikiMovies)\nCountries,WikiMovies\n(Lacroixetal.,2018) (cid:51) (cid:51) (cid:51) (cid:51) YAGO3-10 (cid:51)\nDBPedia-YAGO3, entityalignment\n(Guoetal.,2019) (cid:51) (cid:51) (cid:51) (cid:51)\nDBPedia-Wikidata (DBPediagraphs)\n(Sunetal.,2019) (cid:51) (cid:51) (cid:51) (cid:51) (cid:51)\n(Zhangetal.,2019) (cid:51) (cid:51) (cid:51) (cid:51) (cid:51)\n(Balazevicetal.,2019a) (cid:51) (cid:51) (cid:51)\ngraphclassification\n(Vashishthetal.,2020b) (cid:51) (cid:51) MUTAG,AM,PTC (cid:51)\n(MUTAG,AM,PTC)\nLCAAN,PLNME,LCA\n(Jietal.,2015) (cid:51) (cid:51) (cid:51) (cid:51) (cid:51) (cid:51)\n(Guoetal.,2015) NELL(Location,Sports,Freq) (cid:51) (cid:51)\n(Guuetal.,2015) (cid:51) (cid:51) (cid:51) (cid:51)\n(Garcia-Duranetal.,2015) (cid:51) Families (cid:51)\nrelationextraction\n(Linetal.,2015a) (cid:51) FB40K (cid:51)\n(FB40K)\n(Xiaoetal.,2016b) (cid:51) (cid:51) (cid:51) (cid:51) (cid:51) (cid:51)\n(Nguyenetal.,2016) (cid:51) (cid:51) (cid:51)\n(Xiongetal.,2017) (cid:51) NELL-995 (cid:51) rulemining\n(Linetal.,2018) (cid:51) (cid:51) NELL-995,UMLS,Kinship (cid:51)\n(Nguyenetal.,2018) (cid:51) (cid:51) (cid:51)\n(Bansaletal.,2019) (cid:51) (cid:51) (cid:51)\n(XuandLi,2019) (cid:51) (cid:51) (cid:51) (cid:51) YAGO3-10,Family (cid:51)\n(Balazevicetal.,2019b) (cid:51) (cid:51) (cid:51) (cid:51) (cid:51)\npersonalizedsearch\n(Vuetal.,2019) (cid:51) (cid:51) SEARCH17 (cid:51)\n(SEARCH17)\n(Nathanietal.,2019) (cid:51) (cid:51) NELL-995,UMLS,Kinship (cid:51)\n(Jiangetal.,2019) (cid:51) (cid:51) (cid:51) (cid:51) (cid:51)",
    "tables": [],
    "text_stats": {
      "word_count": 289,
      "char_count": 3189,
      "line_count": 78,
      "table_count": 0
    }
  },
  {
    "page_number": 15,
    "text": "Table9:Theentityandrelationtypes(WikidataIDsinparentheses)usedtoseedCODEX.\nSeedtypes\nseititnE\nactor (Q33999), airline (Q46970), airport (Q1248784), athlete (Q2066131), book (Q571), businessperson\n(Q43845),city(Q515),company(Q783794),country(Q6256),disease(Q12136),engineer(Q81096),film\n(Q11424),governmentagency(Q327333),journalist(Q1930187),lake(Q23397),monarch(Q116),mountain\n(Q8502), musical group (Q215380), musician (Q639669), newspaper (Q11032), ocean (Q9430), politician\n(Q82955),recordlabel(Q18127),religion(Q9174),religiousleader(Q15995642),religioustext(Q179461),\nscientist(Q901),sportsleague(Q623109),sportsteam(Q12973014),stadium(Q483110),televisionprogram\n(Q15416),touristattraction(Q570116),visualartist(Q3391743),visualartwork(Q4502142),writer(Q36180)\nsnoitaleR\nairline alliance (P114), airline hub (P113), architect (P84), architectural style (P149), author (P50), capital\n(P36),castmember(P161),causeofdeath(P509),chairperson(P488),chiefexecutiveofficer(P169),child\n(P40),continent(P30),country(P17),countryofcitizenship(P27),countryoforigin(P495),creator(P170),\ndiplomaticrelation(P530),director(P57),drugusedfortreatment(P2176),educatedat(P69),employer(P108),\nethnicgroup(P172),fieldofwork(P101),foundationaltext(P457),foundedby(P112),genre(P136),head\nofgovernment(P6),headofstate(P35),headquarterslocation(P159),healthspecialty(P1995),indigenous\nto(P2341),industry(P452),influencedby(P737),instanceof(P31),instrument(P1303),languageofwork\norname(P407),languagesspoken,written,orsigned(P1412),legalform(P1454),legislativebody(P194),\nlocatedintheadministrativeterroritorialentity(P131),locationofformation(P740),medicalcondition(P1050),\nmedicalexaminations(P923),memberof(P463),memberofpoliticalparty(P102),memberofsportsteam(P54),\nmountainrange(P4552),movement(P135),namedafter(P138),narrativelocation(P840),notableworks(P800),\noccupant(P466),occupation(P106),officiallanguage(P37),parentorganization(P749),partof(P361),placeof\nbirth(P19),placeofburial(P119),placeofdeath(P20),practicedby(P3095),productormaterialproduced\n(P1056),publisher(P123),recordlabel(P264),regulatedby(P3719),religion(P140),residence(P551),shares\nborderwith(P47),sibling(P3373),sport(P641),spouse(P26),studies(P2578),subclassof(P279),symptoms\n(P780),timeperiod(P2348),tributary(P974),unmarriedpartner(P451),use(P366),uses(P2283)\nTransE (Bordes et al., 2013) treats relations as TuckER(Balazevicetal.,2019b)isalinearmodel\ntranslations between entities, i.e., h+r ≈ t for basedontheTuckertensordecomposition,which\nh,r,t ∈ Rde,andscoresembeddingswithnegative factorizesatensorintothreelower-rankmatrices\nEuclideandistance−(cid:107)h+r−t(cid:107).TransEislikely andacoretensor.TheTuckERscoringfunctionfor\nthe most popular baseline for KGC tasks and the asingletriple(h,r,t)isgivenasW × h× r×\n1 2 3\nmostinfluentialofallKGCembeddingpapers. t, where W is the mode-three core tensor that is\nsharedamongallentityandrelationembeddings,\nComplEx (Trouillon et al., 2016) uses a bilinear\nand × denotes the tensor product along the n-\nfunctiontoscoretripleswithadiagonalrelationem- n\nth mode of the tensor. TuckER can be seen as a\nbeddingmatrixandcomplex-valuedembeddings.\nItsscoringfunctionisre (cid:0) h(cid:62)diag(r)t (cid:1) ,wheretis generalizedformofotherlinearKGCembedding\nmodelslikeRESCALandComplEx.\nthecomplexconjugateoftandredenotesthereal\npartofacomplexnumber.\nE Contentcomparison\nConvE (Dettmers et al., 2018) is one of the first\nWeprovideadditionalcomparisonofthecontents\nand most popular nonlinear models for KGC.\nin CODEX-M and FB15K-237.\nIt concatenates head and relation embeddings h\nFigure5,whichplotsthetop-30entitiesbyfre-\nand r into a two-dimensional “image”, applies a\nquencyinthetwobenchmarks,demonstratesthat\npointwise linearity over convolutional and fully-\nbothdatasetarebiasedtowarddevelopedWestern\nconnected layers, and multiplies the result with\ncountries and cultures. However, CODEX-M is\nthe tail embedding t to obtain a score. Formally,\nmorediverseindomain.Itcoversacademia,enter-\nits scoring function is given as f(vec(f([h;r] ∗\ntainment,journalism,politics,science,andwriting,\nω))W)t, where f is a nonlinearity (originally,\nwhereas FB15K-237coversmostlyentertaiment\nReLU), [h;r] denotes a concatenation and two-\nandsports.FB15K-237isalsomuchmorebiased\ndimensional reshaping of the head and relation\ntowardtheUnitedStatesinparticular,asfiveofits\nembeddings, ω denotes the filters of the convo-\ntop-30entitiesarespecifictotheUS:UnitedStates\nlutional layer, and vec denotes the flattening of a\nof America, United States dollar, New York City,\ntwo-dimensionalmatrix.",
    "tables": [],
    "text_stats": {
      "word_count": 284,
      "char_count": 4576,
      "line_count": 72,
      "table_count": 0
    }
  },
  {
    "page_number": 16,
    "text": "Figure5:Top-30mostfrequententitiesinCODEX-MandFB15K-237.\nFigure6:Top-15mostfrequententitytypesinCODEX-MandFB15K-237.\nLosAngeles,andtheUnitedStatesDepartmentof S, CODEX-M, and CODEX-L, respectively. Ta-\nHousingandUrbanDevelopment. bles14and15reportthebesthyperparametercon-\nFigure 6 compares the top-15 entity types in figurationsfortripleclassificationonthehardneg-\nCODEX-MandFB15K-237.Again,CODEX-M ativesin CODEX-S and CODEX-M,respectively.\nisdiverse,coveringpeople,places,organizations,\nTerminology For embedding initialization, Xv\nmovies, and abstract concepts, whereas FB15K-\nreferstoXavierinitialization(GlorotandBengio,\n237 has many overlapping entity types mostly\n2010). The reciprocal relations model refers to\naboutentertainment.\nlearningseparaterelationembeddingsforqueries\ninthedirectionof(h,r,?)versus(?,r,t)(Kazemi\nF Hyperparametersearch\nandPoole,2018).Thefrequencyweightingregu-\nTable 10 gives our hyperparameter search space. larizationtechniquereferstoregularizingembed-\nTables11,12,and13reportthebesthyperparame- dingsbytherelativefrequencyofthecorrespond-\nterconfigurationsforlinkpredictiononCODEX- ingentityorrelationinthetrainingdata.",
    "tables": [],
    "text_stats": {
      "word_count": 70,
      "char_count": 1155,
      "line_count": 20,
      "table_count": 0
    }
  },
  {
    "page_number": 17,
    "text": "Search strategies Recall that we select models • CODEX-L:Pernegativesamplingtype/loss\nusingAx,whichsupportshyperparametersearch combination, we generate 10 quasi-random\nusing both quasi-random sequences of generated trialsof20trainingepochsinsteadof400.We\nconfigurations and Bayesian optimization (BO). reducethenumberofepochstolimitresource\nThe search strategy for each CODEX dataset is usage.Inmostcases,MRRplateausafter20-\nasfollows: 30epochs,anobservationwhichisconsistent\nwith (Ruffinelli et al., 2020). Then, we take\n• CODEX-S:Pernegativesamplingtype/loss\nthebest-performingmodelbyvalidationMRR\ncombination, we generate 30 quasi-random\nover all such combinations, and retrain that\ntrialsfollowedby10BOtrials.Weselectthe\nmodelforamaximumof400epochs.\nbest-performing model by validation MRR\noverallsuchcombinations.Ineachtrial,the\nNotethatwesearchusingMRRasourmetric,but\nmodelistrainedforamaximumof400epochs\nthetripleclassificationtaskmeasures0/1accuracy,\nwithanearlystoppingpatienceof5.Wealso\nnotrankingperformance.Fortripleclassification,\nterminateatrialafter50epochsifthemodel\nwe choose the model with the highest validation\ndoesnotreach≥0.05MRR.\naccuracyamongthepre-trainedmodelsacrossall\n• CODEX-M:Pernegativesamplingtype/loss negativesamplingtype/lossfunctioncombinations.\ncombination, we generate 20 quasi-random We release all pretrained LibKGE models and\ntrials. The maximum number of epochs and accompanyingconfigurationfilesinthecentralized\nearly stopping criteria are the same as for CODEXrepository.\nCODEX-S.",
    "tables": [],
    "text_stats": {
      "word_count": 119,
      "char_count": 1525,
      "line_count": 29,
      "table_count": 0
    }
  },
  {
    "page_number": 18,
    "text": "Table10:Ourhyperparametersearchspace.WefollowthenamingconventionsandrangesgivenbyRuffinellietal.\n(2020), and explain the meanings of selected hyperparameter settings in Appendix F. As most KGC embedding\nmodels have a wide range of configuration options, we encourage future work to follow this tabular scheme for\ntransparentreportingofimplementationdetails.\nHyperparameter Range\nEmbeddingsize {128,256,512}\nTrainingtype {NegSamp,1vsAll,KvsAll}\nReciprocal {True,False}\n#headsamples(NegSamp) [1,1000],logscale\n#tailsamples(NegSamp) [1,1000],logscale\nLabelsmoothing(KvsAll) [0,0.3]\nLoss {MR,BCE,CE}\nMargin(MR) [0,10]\n(cid:96) norm(TransE) {1,2}\np\nOptimizer {Adam,Adagrad}\nBatchsize {128,256,512,1024}\nLearningrate [10−4,1],logscale\nLRschedulerpatience [0,10]\n(cid:96) regularization {1,2,3,None}\np\nEntityembeddingweight [1020,10−5]\nRelationembeddingweight [1020,10−5]\nFrequencyweighting {True,False}\nEmbeddingnormalization(TransE)\nEntity {True,False}\nRelation {True,False}\nDropout\nEntityembedding [0.0,0.5]\nRelationembedding [0.0,0.5]\nFeaturemap(ConvE) [0.0,0.5]\nProjection(ConvE) [0.0,0.5]\nEmbeddinginitialization {Normal,Unif,XvNorm,XvUnif}\nStdev(Normal) [10−5,1.0]\nInterval(Unif) [−1.0,1.0]\nGain(XvNorm) 1.0\nGain(XvUnif) 1.0",
    "tables": [],
    "text_stats": {
      "word_count": 100,
      "char_count": 1224,
      "line_count": 37,
      "table_count": 0
    }
  },
  {
    "page_number": 19,
    "text": "Table11:BestlinkpredictionhyperparameterconfigurationsonCODEX-S.\nRESCAL TransE ComplEx ConvE TuckER\nBestvalidationMRR 0.4076 0.3602 0.4752 0.4639 0.4574\nEmbeddingsize 512 512 512 256 512\nTrainingtype 1vsAll NegSamp 1vsAll 1vsAll KvsAll\nReciprocal No Yes Yes Yes Yes\n#headsamples(NegSamp) - 2 - - -\n#tailsamples(NegSamp) - 56 - - -\nLabelsmoothing(KvsAll) - - - - 0.0950\nLoss CE CE CE CE CE\nMargin(MR) - - - - -\n(cid:96) norm(TransE) - 2 - - -\np\nOptimizer Adagrad Adagrad Adam Adagrad Adagrad\nBatchsize 128 128 1024 512 256\nLearningrate 0.0452 0.0412 0.0003 0.0117 0.0145\nLRschedulerpatience 7 6 7 3 1\n(cid:96) regularization 3 2 None 3 1\np\nEntityembeddingweight 2.18×10−10 1.32×10−7 9.58×10−13 3.11×10−15 3.47×10−15\nRelationembeddingweight 3.37×10−14 3.72×10−18 0.0229 4.68×10−9 3.43×10−14\nFrequencyweighting False False True True True\nEmbeddingnormalization(TransE)\nEntity - No - - -\nRelation - No - - -\nDropout\nEntityembedding 0.0 0.0 0.0793 0.0 0.1895\nRelationembedding 0.0804 0.0 0.0564 0.0 0.0\nFeaturemap(ConvE) - - - 0.2062 -\nProjection(ConvE) - - - 0.1709 -\nEmbeddinginitialization Normal XvNorm XvNorm XvNorm XvNorm\nStdev(Normal) 0.0622 - - - -\nInterval(Unif) - - - - -\nGain(XvNorm) - 1.0 1.0 1.0 1.0\nGain(XvUnif) - - - - -",
    "tables": [],
    "text_stats": {
      "word_count": 186,
      "char_count": 1230,
      "line_count": 35,
      "table_count": 0
    }
  },
  {
    "page_number": 20,
    "text": "Table12:BestlinkpredictionhyperparameterconfigurationsonCODEX-M.\nRESCAL TransE ComplEx ConvE TuckER\nBestvalidationMRR 0.3173 0.2993 0.3351 0.3146 0.3253\nEmbeddingsize 256 512 512 512 512\nTrainingtype 1vsAll NegSamp KvsAll NegSamp KvsAll\nReciprocal Yes Yes Yes Yes Yes\n#headsamples(NegSamp) - 2 - 381 -\n#tailsamples(NegSamp) - 56 - 751 -\nLabelsmoothing(KvsAll) - - 0.2081 - 0.0950\nLoss CE CE CE CE CE\nMargin(MR) - - - - -\n(cid:96) norm(TransE) - 2 - - -\np\nOptimizer Adagrad Adagrad Adagrad Adagrad Adagrad\nBatchsize 256 128 1024 128 256\nLearningrate 0.0695 0.0412 0.2557 0.0024 0.0145\nLRschedulerpatience 8 6 6 9 1\n(cid:96) regularization 2 2 3 1 1\np\nEntityembeddingweight 9.56×10−7 1.32×10−7 1.34×10−10 1.37×10−10 3.47×10−15\nRelationembeddingweight 2.56×10−17 3.72×10−18 6.38×10−16 4.72×10−10 3.4×10−14\nFrequencyweighting False False True True True\nEmbeddingnormalization(TransE)\nEntity - No - - -\nRelation - No - - -\nDropout\nEntityembedding 0.0 0.0 0.1196 0.0 0.1895\nRelationembedding 0.0 0.0 0.3602 0.0348 0.0\nFeaturemap(ConvE) - - - 0.3042 -\nProjection(ConvE) - - - 0.2343 -\nEmbeddinginitialization XvUnif XvUnif Unif XvNorm XvNorm\nStdev(Normal) - - - - -\nInterval(Unif) - - −0.8133 - -\nGain(XvNorm) - - - 1.0 1.0\nGain(XvUnif) 1.0 1.0 - - -",
    "tables": [],
    "text_stats": {
      "word_count": 186,
      "char_count": 1243,
      "line_count": 35,
      "table_count": 0
    }
  },
  {
    "page_number": 21,
    "text": "Table13:BestlinkpredictionhyperparameterconfigurationsonCODEX-L.\nRESCAL TransE ComplEx ConvE TuckER\nBestvalidationMRR 0.3030 0.1871 0.2943 0.3010 0.3091\nEmbeddingsize 128 128 128 256 256\nTrainingtype 1vsAll NegSamp 1vsAll 1vsAll 1vsAll\nReciprocal No Yes Yes Yes No\n#headsamples(NegSamp) - 209 - - -\n#tailsamples(NegSamp) - 2 - - -\nLabelsmoothing(KvsAll) - - - - -\nLoss CE CE CE CE CE\nMargin(MR) - - - - -\n(cid:96) norm(TransE) - 2 - - -\np\nOptimizer Adagrad Adam Adagrad Adagrad Adagrad\nBatchsize 1024 128 1024 256 512\nLearningrate 0.2651 0.0009 0.2651 0.0329 0.0196\nLRschedulerpatience 7 9 7 1 4\n(cid:96) regularization 2 2 2 1 2\np\nEntityembeddingweight 2.01×10−16 7.98×10−14 2.01×10−16 6.10×10−16 8.06×10−11\nRelationembeddingweight 3.52×10−13 3.42×10−9 3.52×10−13 1.03×10−16 7.19×10−19\nFrequencyweighting True False True True True\nEmbeddingnormalization(TransE)\nEntity - No - - -\nRelation - No - - -\nDropout\nEntityembedding 0.0 0.0 0.0 0.0064 0.1606\nRelationembedding 0.0 0.0 0.0 0.0 0.0857\nFeaturemap(ConvE) - - - 0.1530 -\nProjection(ConvE) - - - 0.4192 -\nEmbeddinginitialization Normal Unif Normal XvNorm Normal\nStdev(Normal) 0.0169 - 0.0169 - 0.0002\nInterval(Unif) - −0.4464 - -\nGain(XvNorm) - - 1.0 -\nGain(XvUnif) - - - -",
    "tables": [],
    "text_stats": {
      "word_count": 183,
      "char_count": 1226,
      "line_count": 35,
      "table_count": 0
    }
  },
  {
    "page_number": 22,
    "text": "Table14:BesttripleclassificationhyperparameterconfigurationsonCODEX-S(hardnegatives).\nRESCAL TransE ComplEx ConvE TuckER\nBestvalidationaccuracy 0.8571 0.8511 0.8558 0.8607 0.8596\nEmbeddingsize SeeTab.11 SeeTab.11 SeeTab.11 512 SeeTab.11\nTrainingtype 1vsAll NegSamp 1vsAll 1vsAll KvsAll\nReciprocal SeeTab.11 SeeTab.11 SeeTab.11 Yes SeeTab.11\n#headsamples(NegSamp) - SeeTab.11 - - -\n#tailsamples(NegSamp) - SeeTab.11 - - -\nLabelsmoothing(KvsAll) - - - - -\nLoss CE CE CE BCE CE\nMargin(MR) - - - - -\n(cid:96) norm(TransE) - SeeTab.11 - - -\np\nOptimizer SeeTab.11 SeeTab.11 SeeTab.11 Adagrad SeeTab.11\nBatchsize SeeTab.11 SeeTab.11 SeeTab.11 256 SeeTab.11\nLearningrate SeeTab.11 SeeTab.11 SeeTab.11 0.0263 SeeTab.11\nLRschedulerpatience SeeTab.11 SeeTab.11 SeeTab.11 7 SeeTab.11\n(cid:96) regularization SeeTab.11 SeeTab.11 SeeTab.11 2 SeeTab.11\np\nEntityembeddingweight SeeTab.11 SeeTab.11 SeeTab.11 9.62×10−6 SeeTab.11\nRelationembeddingweight SeeTab.11 SeeTab.11 SeeTab.11 1.34×10−12 SeeTab.11\nFrequencyweighting SeeTab.11 SeeTab.11 SeeTab.11 False SeeTab.11\nEmbeddingnormalization(TransE)\nEntity - SeeTab.11 - - -\nRelation - SeeTab.11 - - -\nDropout\nEntityembedding SeeTab.11 SeeTab.11 SeeTab.11 0.1620 SeeTab.11\nRelationembedding SeeTab.11 SeeTab.11 SeeTab.11 0.0031 SeeTab.11\nFeaturemap(ConvE) - - - 0.0682 -\nProjection(ConvE) - - - 0.2375 -\nEmbeddinginitialization SeeTab.11 SeeTab.11 SeeTab.11 Normal SeeTab.11\nStdev(Normal) SeeTab.11 SeeTab.11 SeeTab.11 0.0006 SeeTab.11\nInterval(Unif) SeeTab.11 SeeTab.11 SeeTab.11 - SeeTab.11\nGain(XvNorm) SeeTab.11 SeeTab.11 SeeTab.11 - SeeTab.11\nGain(XvUnif) SeeTab.11 SeeTab.11 SeeTab.11 - SeeTab.11",
    "tables": [],
    "text_stats": {
      "word_count": 186,
      "char_count": 1635,
      "line_count": 35,
      "table_count": 0
    }
  },
  {
    "page_number": 23,
    "text": "Table15:BesttripleclassificationhyperparameterconfigurationsonCODEX-M(hardnegatives).\nRESCAL TransE ComplEx ConvE TuckER\nBestvalidationaccuracy 0.8232 0.8002 0.8267 0.8292 0.8267\nEmbeddingsize 512 SeeTab.12 512 512 SeeTab.12\nTrainingtype KvsAll NegSamp KvsAll KvsAll KvsAll\nReciprocal Yes SeeTab.12 Yes Yes SeeTab.12\n#headsamples(NegSamp) - SeeTab.12 - - -\n#tailsamples(NegSamp) - SeeTab.12 - - -\nLabelsmoothing(KvsAll) 0.0949 - 0.2081 0.0847 -\nLoss CE CE CE CE CE\nMargin(MR) - - - - -\n(cid:96) norm(TransE) - SeeTab.12 - - -\np\nOptimizer Adagrad SeeTab.12 Adagrad Adagrad SeeTab.12\nBatchsize 256 SeeTab.12 1024 1024 SeeTab.12\nLearningrate 0.0144 SeeTab.12 0.2557 0.0378 SeeTab.12\nLRschedulerpatience 1 SeeTab.12 6 6 SeeTab.12\n(cid:96) regularization 1 SeeTab.12 3 3 SeeTab.12\np\nEntityembeddingweight 3.47×10−15 SeeTab.12 1.34×10−10 1.03×10−16 SeeTab.12\nRelationembeddingweight 3.43×10−14 SeeTab.12 6.38×10−16 0.0052 SeeTab.12\nFrequencyweighting True SeeTab.12 True True SeeTab.12\nEmbeddingnormalization(TransE)\nEntity - SeeTab.12 - - -\nRelation - SeeTab.12 - - -\nDropout\nEntityembedding 0.1895 SeeTab.12 0.1196 0.4828 SeeTab.12\nRelationembedding 0.0 SeeTab.12 0.3602 0.0 SeeTab.12\nFeaturemap(ConvE) - - - 0.2649 -\nProjection(ConvE) - - - 0.2790 -\nEmbeddinginitialization XvNorm SeeTab.12 Unif XvUnif SeeTab.12\nStdev(Normal) - SeeTab.12 - - SeeTab.12\nInterval(Unif) - SeeTab.12 −0.8133 - SeeTab.12\nGain(XvNorm) 1.0 SeeTab.12 - - SeeTab.12\nGain(XvUnif) - SeeTab.12 - 1.0 SeeTab.12",
    "tables": [],
    "text_stats": {
      "word_count": 186,
      "char_count": 1478,
      "line_count": 35,
      "table_count": 0
    }
  }
]