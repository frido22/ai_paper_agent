[
  {
    "page_number": 1,
    "text": "“Where’s My Tatte?”: Scene Recognition for Delivery Location at Wellesley\nCollege\nBlancaLuo HanqiZhu\nql101@wellesley.edu hz100@wellesley.edu\nAbstract efficiencyoflast-milelogistics,offeringalightweightsolu-\ntiontoanuniversalproblem.\nMisdeliveredtakeoutisacommonandfrustratingprob-\nlem, often caused by the visual similarity of building en- 2.RelatedWorks\ntrances and the inconsistent quality of delivery photos.\nThis project explores whether a CNN-based scene recog- Placerecognitionisasubfieldofcomputervisionthatfo-\nnition model can solve this problem by accurately identi- cusesonidentifyingthespecificlocationdepictedinagiven\nfying specific entrance doors from such images. We focus image. Prior work has applied image retrieval techniques\nourcasestudyontheWellesleyCollegecampus,collecting andCNN-basedmodelstodevelopcondition-invariantrep-\na dataset of labeled entrance photos under varying condi- resentationsthatremaineffectiveunderchangesinlighting,\ntions. Using a transfer learning framework built on pre- viewpoint, and weather conditions [2]. These techniques\ntrainedResNet50,wefine-tuneclassificationlayersandap- areparticularlyrelevanttoourtask,wherephotosofdeliv-\nply augmentation techniques to improve robustness. Our erylocationsoftenvarywidelyinappearance. Morerecent\nmodelachievestop-1accuracyover90%, andtop-5accu- efforts,suchastheworkbyZhangetal.,extenddeeplearn-\nracy over 98% on both training and validation accuracy. ingapproachestoindoorenvironments,demonstratingthat\nOur approach enables retrieval-based recognition of pre- CNNs can capture subtle architectural cues to distinguish\ncisedeliverylocations,offeringalightweightandscalable between visually similar locations like hallways and door-\nsolutiontoawidespreadlogisticalissue. ways [3]. Their case study on MIT’s campus further sup-\nportsthefeasibilityofapplyingfine-grainedspatialclassifi-\ncationtostructuredenvironmentslikeuniversities—closely\n1.Introduction aligningwithourdeploymentatWellesleyCollege.\nA core challenge in delivery location recognition is the\nHaveyoueverorderedtakeoutbutcouldnotfindwhere\nhigh variability in image conditions—oblique angles, in-\nit is delivered? It’s a common frustration that the food of-\nconsistentlighting,motionblur,orreflections—allofwhich\ntenendsupatthewrongdoororbuilding. Sincemanyen-\njeopardize the stability and robustness of our model. To\ntranceslookvisuallysimilar,anddeliveryphotosareoften\naddress similar issues, Laskin et al. propose augmenting\ntakenatawkwardangles,peoplestruggletolocatetheiror-\ntraining images with random shifts, crops, occlusions, and\nders. Thisfailurenotonlyfrustratesusersbutalsoresultsin\nother perturbations to improve the generalization of visual\nsignificant financial loss for delivery apps, which often re-\nrepresentations[4]. Together,theserelatedworksprovidea\nimbursecustomersformissingorders[1]. Additionally,the\nstrongfoundationforourapproachandreinforcethepoten-\nfoodiswasted. Totackletheseproblems,weproposetode-\ntialofCNN-basedmodelsforentrance-levelrecognition.\nvelopaConvolutionalNeuralNetwork(CNN)-basedscene\nrecognitionmodelthatdistinguishestheexactentrance’slo-\n3.Methodology\ncationbasedonasinglephoto,evenwhenentrancesappear\nvisually similar. We investigate this approach by perform-\n3.1.DataCollectionandPreprocessing\ning a case study on the Wellesley College campus, where\nwe develop a vision model to recognize the correct dorm Since there is no pre-existing dataset for our project,\nentrancefromphotosprovidedbythedeliverypersonnel. If we created our own from scratch. We collected a dataset\nsuccessful,thissystemcouldbescaledandadaptedtoother consisting of images from 44 distinct doors located across\ncampuses,cities,andevenacrossthecountry. Oncescaled 16 residence halls and academic buildings on Wellesley\nup, it would improve customer satisfaction, reduce refund College’s campus. Each door was photographed 30–50\nand redelivery costs for suppliers, and enhance the overall timesfrommultipleanglesandunderdifferentlightingand",
    "tables": [],
    "text_stats": {
      "word_count": 349,
      "char_count": 4009,
      "line_count": 57,
      "table_count": 0
    }
  },
  {
    "page_number": 2,
    "text": "weatherconditionstoreflectreal-worldvariabilityindeliv- suitableforourgoalofdeliverylocationrecognition,where\neryphotos. subtledifferencesinarchitecturaldetails—likedoorframes,\nWe then manually labeled on the campus map using wall textures, or signage—must be reliably captured and\na building-specific index in the form BuildingName distinguished.\nIndex (e.g., Shafer 1 for the front entrance of Shafer Toadaptthemodeltoourspecificclassificationtask,we\nHall). We coordinated labelling through a shared Google applied network surgery by replacing the final fully con-\nSheetandmappeddirectlytoafolderstructurecompatible nected layer with a new classifier head matching the num-\nwith torchvision.datasets.ImageFolder. The berofclassesinourdataset. Alltrainingwasconductedon\nclassbalanceandrepresentationisshowninFigure1. a virtual GPU with CUDA acceleration. The stable hard-\nware and pretrained weights allows us to efficiently opti-\nmizethemodelwithoutexperiencingdatascarcityandex-\nceeding computational resource limits. A diagram of our\nmodeltrainingprocessisshownbelow(Figure3).\nFigure1.ClassBalanceandRepresentationDistribution\nThe dataset was randomly split into 80% training and Figure3.VisualOverviewofourMethod\n20%validationsets. Wepreprocessedtheimagestomatch\nthe input format of ResNet50. Our approach to customiz- Class Imbalance As indicated in Figure 1, our dataset\ningthedatasetreliesontheassumptionthatthephotoswe has moderate class imbalance with the number of images\ncapturedsharesimilarstructuralfeaturesanddistributional per class ranging from as well as 15 to over 90. To ad-\ncharacteristics with real-world delivery images. To bet- dress this, we applied the reweighted loss function to re-\nterapproximatethisassumption,wefurtheraugmentedthe train our model. Specifically, we computed class-specific\ndatasetusingtechniquessuchasrandomcropping,rotation, weightsinverselyproportionaltothenumberofsamplesin\nzooming, colorjitter, andGaussianblur. Theseaugmenta- each class. These weights were then passed to PyTorch’s\ntions not only enhance the model’s ability to generalize to CrossEntropyLoss function so that the loss penalizes\nreal-worlddistortionsandnoise,butalsoeffectivelyexpand errors on underrepresented classes more heavily. Thus,\nthedatasetandreducetheriskofoverfitting. Anexampleis the model pays equal attention to both underrepresented\nshowninFigure2. and overrepresented doors to reduce bias toward major-\nity classes without significantly impacting overall perfor-\nmance.\n3.3.EvaluationPlan\nWeevaluatedourmodelquantitativelyusingTop-1and\nFigure2. SampleTrainingImagesafterPreprocessingandAug- Top-5accuracyrates,measuringhowoftenthecorrecten-\nmentation(classlabelsshownaboveeachimage) trance appears among the top retrievals for a given query\nimage. Tofurtheranalyzemodelperformancequalitatively,\nwe computed a normalized misclassification matrix, i.e., a\n3.2.ModelTraining\nconfusionmatrixwhereeachrowisnormalizedtorepresent\nTransferLearningFormodeltraining, weadoptedthe classification probabilities, with the true positive rate for\ntransfer learning framework, which leverages existing vi- eachclassappearingonthediagonal[6]. Thisallowsusto\nsual representations and accelerates the training process. identifywhichpairsofdoorsaremostfrequentlyconfused\nWe chose to use the pretrained ResNet50 model as the duetohighvisualsimilarity. Foreachdoorpair(A,B),we\nbackboneforfeatureextraction. ResNet50consistsoffifty defineconfusionasthesumoftheratesatwhichimagesof\nconvolutional layers and uses residual connections to ease doorAaremisclassifiedasdoorBandviceversa.\ntrainingandimproveperformance. Itisknownforproduc- Wefurthervisualizedtheattentionofmostsimilardoors\ning robust low- and mid-level visual features that general- usingClassActivationMapping(CAM)heatmapthatindi-\nizewellacrossmanyvisiontasks. Thismakesitespecially cates the spatial regions that most influence retrieval deci-",
    "tables": [],
    "text_stats": {
      "word_count": 352,
      "char_count": 3916,
      "line_count": 46,
      "table_count": 0
    }
  },
  {
    "page_number": 3,
    "text": "sions [5]. This will helpus verifythat the modelis recog- the same building, particularly in the Science Center and\nnizingmeaningfularchitecturalfeatures(e.g.,doorframes, Cazenove Hall. We further plotted an additional confu-\nsignage, or facade textures) rather than background noise. sion matrix classified by building-level labels to analyze\nIntheheatmaps,warmercolors(e.g.,redandyellow)indi- thesimilaritybetweenbuildings. Thisshowsthatentrances\ncateregionswithhighactivation,whichthemodelstrongly onWestCampusbuildings—especiallyPomandCaz—are\nconsidersduringclassification.Coolercolors(e.g.,blueand more frequently confused with one another, likely due to\npurple)reflectareaswithlowactivationandminimalinflu- their nearly identical architectural features and surround-\nence on the model’s decision. We also overlay the CAMs ings, whichisconsistentwithhowthesebuildingsareper-\nontheoriginalRGBimagestobetterunderstandwherethe ceivedinreallife.\nmodelisfocusingitsattention. Despite reweighting the loss function to address class\nimbalance, the confusion matrix did not show significant\n4.ResultsandDiscussion changes, as shown in Figure 5. This is likely because our\noriginal model was already achieving strong performance.\n4.1.Experiments\nNonetheless, the diagonal elements appear darker, indicat-\nDuring the model fine-tuning process, we achieved the ingslightlystrongertruepositiveratesacrossmostclasses.\nbest performance using the Stochastic Gradient Descent Thereweightedmodelhelpedreducecertainmisclassifica-\n(SGD) optimizer with a momentum of 0.9. This momen- tions, as shown by the cleaner off-diagonal region. Al-\ntum value helps stabilize updates by smoothing out noisy though the improvement is small due to the already high\ngradients,whichisespeciallybeneficialforrelativelysmall baselineaccuracyandtruepositiverate,thesechangesshow\nandfine-graineddatasets—itacceleratesconvergenceand thatreweightinghelpedthemodelfocusmoreevenlyacross\nhelpsthemodelescapeshallowlocalminima. Wealsoex- allclasses.\nperimented with the learning, starting from 0.02, and re-\nalized that generally the lower the learning rate, the better\nthemodelperforms. Weendedupwiththelearningrateat\n0.001andappliedalearningratedecayfactorof0.1every\n7epochs. Thisschedulingenablesthemodeltomakelarge\nupdates early on for fast adaptation, followed by smaller,\nmorepreciseadjustmentsastrainingprogresses.\n4.2.Accuracy\nFigure 4. Misclassification Matrices for the Original Model (by\nMetric Training Validation\ndoor,bybuilding)\nTop-1Accuracy 94.69% 90.88%\nTop-3Accuracy 99.54% 98.91%\nLoss 28.68% 37.92%\nTable1.PerformanceonTrainingandValidationDatasets\nThe top-1 accuracy exceeded 90%, and the Top-5 ac-\ncuracies approached 100% on both training and validation\ndatasets.\nAlthough not shown here due to space constraints, we Figure5. MisclassificationMatricesfortheRe-WeightedModel\nobserved no major signs of overfitting since both training (bydoor,bybuilding)\nandvalidationaccuracyincreasedsteadilyoverthe10train-\ningepochs,withnosignificantfluctuationsordropsduring\n4.4.ClassActivationMaps\ntheprocess.\nTobetterunderstandmodelbehaviorincasesofmisclas-\n4.3.ConfusionMatrix\nsifications, we generated Class Activation Maps (CAMs)\nThe confusion matrix by each door in Figure 4 shows for one of the most confused door pairs according to the\nthat most predictions fall along the diagonal. Thus, the confusionmatrix: Pom 1vs. Caz 1,showninFigure6and\nmodel correctly classifies most inputs. However, we still Figure7.\nobserved a few localized confusions among doors within Acrossbothexamples,themodelconsistentlyattendsto",
    "tables": [],
    "text_stats": {
      "word_count": 348,
      "char_count": 3589,
      "line_count": 53,
      "table_count": 0
    }
  },
  {
    "page_number": 4,
    "text": "structuralfeaturessuchasdooredges,frames,andwallpan- can successfully enable fine-grained classification of visu-\neling— allofwhichare reasonableandrelevantfor door- ally similar entrances. Using a pretrained ResNet50 back-\nlevel classification. This behavior aligns with human intu- boneallowedustoovercomedataandresourcelimitations,\nition and confirms that the model is focusing on visually whilestillachievinghighaccuracy.\nsalientarchitecturalelements. Themodelachievedhightruepositiveratesacrossmost\nOntheotherhand, ourmodeltendstoignorethevisual classes, aided by a class-weighted loss function that im-\ncuesbehindglasspanels,whichwefoundtobedistinctive proved performance on underrepresented entrances. CAM\nduringthe datacollection process. Thisis understandable, heatmaps[5]showthatitattendstointuitivestructuralcues\nas the reflective nature of glass introduces high variability likedoorframesandwalltextures,thoughsomemisclassi-\nin appearance depending on lighting conditions, making it ficationsremainamongvisuallysimilardoors. Thesecases\nanunreliablefeatureforconsistentextraction. highlightthechallengeoffine-grainedrecognitionandsug-\nWhile door frames are generally informative, they are gestpotentialrefinements. Whileourdatasetwascollected\nnotalwaysdistinctiveenough—asseeninthePomandCaz under limited conditions, data augmentation helped im-\nexample. From the heatmaps, we noticed that these en- proverobustness,asdemonstratedbyLaskinetal.[4],and\ntrancessharemanysimilaritiesinlayoutandtextures,which futureimprovementscouldincludemorediversedataanda\ncontributes to the misclassifications. Despite these uncer- hierarchicalclassificationstructuretofurtherreduceconfu-\ntainties,theCAManalysishelpsustoidentifyfeaturesthat sion.\nareeasilyconfused,animportantinsightthatcanbeusedto Despite these challenges, our project shows that CNN-\ntrigger additional input during inference, such as prompt- basedscenerecognitionisaviableandscalablesolutionto\ningthesystemtorequestuserconfirmationorasecondary thedeliverylocationproblem. OurcasestudyatWellesley\nimage,inordertoreduceerrorsinhigh-riskscenarios. demonstratesthatevenwithmodestresources,itispossible\nto build a reliable and efficient system. With broader data\ncollection and minor architectural improvements, this ap-\nproachhasstrongpotentialtoexpandbeyondasinglecam-\npusandcontributemeaningfullytoimprovinglast-milelo-\ngisticsatscale.\n6.IndividualContribution\nThisprojectwascompletedcollaboratively,withmostof\ntheworkconductedinperson. Wedividedthetasksevenly\nandcontributedequallyacrosstheentirepipeline. Belowis\nFigure6.OriginalinputphotosforPom 1(left),Caz 1(right) abreakdownofspecifictasks:\nBlanca:Datacollection(EastCampus),datapreprocess-\ningandaugmentation,modeltraining,andfine-tuning.\nHanqi: Datacollection(WestCampus),datasetbalance\nand representation analysis, adjustment of data augmenta-\ntionpipeline,andmodelevaluation.\nReferences\n[1] Uber Technologies Inc. My order never arrived. https:\n//help.uber.com/en/ubereats/restaurants/\narticle/my-order-never-arrived?nodeId=\ndd225ffc-ba29-4bba-a85e-a9d1feecc25e, n.d.\nAccessed:2025-05-12. 1\nFigure7. CAMsfortheentrancesabove. Warmercolorsindicate\n[2] Z. Zeng, J. Zhang, X. Wang, Y. Chen, and C. Zhu. Place\nregionsthatmoststronglyinfluencedthemodel’spredictions.\nrecognition: An overview from vision perspective. Applied\nSciences,8(11):2257,2018. 1\n[3] F. Zhang, F. Duarte, R. Ma, D. Milioris, H. Lin, and C.\n5.Conclusion Ratti.Indoorspacerecognitionusingdeepconvolutionalneu-\nral network: A case study at mit campus. arXiv preprint\nOur model achieved strong overall performance, with a\narXiv:1610.02414,2016. 1\nTop-1 accuracy exceeding 90% and a Top-5 accuracy ap- [4] Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song\nproaching100%. Theseresultsdemonstratethatourtrans- Han. Differentiableaugmentationfordata-efficientgantrain-\nfer learning approach, combined with data augmentation, ing. arXivpreprintarXiv:2006.10738,2020. 1,4",
    "tables": [],
    "text_stats": {
      "word_count": 304,
      "char_count": 3959,
      "line_count": 55,
      "table_count": 0
    }
  },
  {
    "page_number": 5,
    "text": "[5] BoleiZhou,AdityaKhosla,AgataLapedriza,AudeOliva,and\nAntonioTorralba. Learningdeepfeaturesfordiscriminative\nlocalization. arXivpreprintarXiv:1512.04150,2015. 3,4\n[6] B.Zhou,L.Liu,A.Oliva,andA.Torralba. Recognizingcity\nidentity via attribute analysis of geo-tagged images. In Eu-\nropeanConferenceonComputerVision(ECCV),pages519–\n534,2014. 2",
    "tables": [],
    "text_stats": {
      "word_count": 22,
      "char_count": 342,
      "line_count": 7,
      "table_count": 0
    }
  }
]