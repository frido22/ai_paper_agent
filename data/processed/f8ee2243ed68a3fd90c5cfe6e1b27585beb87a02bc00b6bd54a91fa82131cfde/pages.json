[
  {
    "page_number": 1,
    "text": "International Journal of Advanced Science and Technology\nVol. x, No. x, (20xx), pp. xx-xx\nTweet Based Geo-Location Prediction of Twitter Users using RNN\nAbhishek Dubey , Praneeth Katuri , Deepansh Gandhi , Kaustav Ghosh , Batchu\n1 2 3 4\nHarshith , Aditya Rathod , Manish Agnihotri , Aditya Jajodia , Chethan Sharma *\n5 6 7 8 9\nDepartment of Electronics and Communication Engineering, Manipal Institute of\n1\nTechnology, Manipal Academy of Higher Education, Manipal\nDepartment of Computer Science and Engineering, Manipal Institute of\n2,3,4\nTechnology, Manipal Academy of Higher Education, Manipal\nDepartment of Information and Communication Technology, Manipal Institute\n5,9\nof Technology, Manipal Academy of Higher Education, Manipal\nTechnical Consultant, ValueLabs LLP, Hyderabad\n6\nResearch Associate, IIIT Bangalore\n7\nDepartment of Computer Science and Engineering, Viterbi School of\n8\nEngineering, University of Southern California\nAbstract\nUsing a recurrent neural network architecture, we propose a model for the regional\nclassification of the location of social media users. The model comprising RNN is used\ndue to the fact that multilayer neural network does not take the order of the words in the\ntweets into account which can be a good feature to explore for predicting the location.\nThis model takes preprocessed tweets as inputs and outperforms the current state of the\nart systems. A new approach has been used to train the model for this specific task, in\nwhich the word-to-vec embeddings of one model is used to train another model. This\napproach gives a considerable improvement in the results.\nKeywords: Neural networks, Recurrent Neural Networks, Natural Language\nProcessing, global vector embeddings\n1. Introduction\nWith the dawn of the internet connection in the past decades, we have been provided with\nunparalleled access to information, data, and resources that could be thought as something\ninconceivable just a few decades ago. Quite a number of times it is often required that\nthese accessed information have proper metadata attached with them, for eg, the date of\nwriting or the location of the author ;yet a very few of the available documents undergo\nthis simple yet critical process. But if provided with sufficient amount of data, it is often\npossible to extract this metadata directly from the given sample, for eg., information about\nthe geo-location of a document may be derived from quite a variety of word features, e.g.\ntoponyms (Toronto), and stylistic or dialectical differences (cool vs. kewl vs. kool). This\npaper focuses on deriving the location of a tweet, specifically the prediction of the region\nof the origin of the tweet . One pivotal way of finding location of a user is through social\nmedia. Different platforms of social media give the user an option to tag their location.\nTwitter has been the best platform to extract tweets from its vast database. Using the\ntwitter API anywhere around the world can obtain tweets along with their geolocation.\nTwitter allows users to add their location to their tweets (self-geotagging) but these\ndescriptions are generally unreliable as users can add a general location label to their\ntweets and according to a survey, less than 2% of tweets are geotagged by the user. Since\nGPS cannot be used to predict the location of the tweets this brings up the requirement of\nISSN: XXXX-XXXX IJXX *Corresponding Author. 1\nCopyright ⓒ XXXX SERSC",
    "tables": [],
    "text_stats": {
      "word_count": 546,
      "char_count": 3422,
      "line_count": 55,
      "table_count": 0
    }
  },
  {
    "page_number": 2,
    "text": "International Journal of xxxxxx\nVol. x, No. x, (20xx), pp. xx-xx\nusing the limited resources provided to trace the location of origin of the tweet. With the\ndawn of the new era of Big Data, the importance of gathering and analysing data has\ngrown exponentially in order to make this world a better place to live in. In past few\nyears, Deep Learning has evolved in through an unpredictable yet mesmerizing trajectory\nin the fields of Natural Language Processing, Computer Vision, and many others. There\nare multiple ways to approach this problem but recent developments have provided\npromising results in the field of Recurrent Neural Networks due to which we chose it for\nour application.\n2. Related Works\nFirst ever attempt in finding geolocation of a user was in the year 2010, by Backstrom et\nal.[12] . He made an approach in predicting a facebook user's location by the amount of\nconnection between friendship networks and geography. Wing and Baldridge[7] divided\nthe earth's surface into grids of equal sizes . This was not effective as cities have lots of\ntweets while compared to villages. Roller et al.[11] expanded upon their work by using\nadaptive grid schemes based on k-dimensional trees and uniform partitioning. Han et\nal.[2] further looked into features that help in finding location-indicative words like gain\nratio, Ripley’s K Statistic and many other features. Liu and Inkpen[10] were the first ones\nto come up with Deep Neural Network for geolocation with a neural network having\n5000 neurons in each layer. [17] proved that using metadata to approach this specific\nproblem is quite useless since those metadata are very unique for a corpus. Quercini et al.\n[15] in their approach use geo-specific dictionary to geotag news articles. Improving the\nDeep Learning approach, Lorentzou et al.[1] used new neural architectures with batch\nnormalization and a combination of activation functions which yielded an increase in\naccuracy. Extensive studies have been conducted regarding the use of generative models\nby [18], discovering a fixed hierarchical structure over context in the textual data, via a\nmerging of global topics and regional languages. A recent workshop was conducted\nfocused on predicting the geolocation of a tweet , a classic classification problem [16].\nZucker et al.[6] was the first one to approach the problem using RNN consisting of one or\nmore LSTM layers to classify the geolocation of tweets. A generative model approach\nwas proposed by Cheng et al. [4] for city-level geolocation of U.S Twitter users that\nidentified words in tweets with a strong native relation with geography.\n3. DataSet\nWe use the publicly available CMU GeoText(2010) dataset for our task. It\ncontains 377,616 tweets from 9,475 users. The dataset contains tweets and their\ncorresponding user’s location as latitude and longitude values. The raw tweets\ncontains nonwords which the model cannot interpret, such as emojis, URLS,\nsmileys, hashtags, etc, so we had to come up with a preprocessing technique to\nconvert this raw data input to a suitable format so that it can be used for training.\n4. Preprocessing\nWhen building Machine Learning systems based on tweet data, a preprocessing\nmechanism is required in order to convert raw data to a format suitable for giving as\ninput to the system. We have used the following methods to clean, parse and\ntokenize the tweets. Since we are predicting the region from which the tweets\noriginated so we had to preprocess the corresponding region of the tweets as what\nwe are given is the latitude and longitude of a particular tweet. Therefore we used\nGeoPy library for determining the states from the given coordinates, and then\nassigning the acquired states to their respective regions in the country.\nTwo main techniques have been used while preprocessing the tweets, namely:\ntweet-preprocessor (A library for basic data preprocessing) and our Tweet Text\nModification Module (which modifies the tweet’s text to avoid the discarding of\nISSN: XX XX-XXXX IJXX 2\nCopyright ⓒ XXXX SERSC",
    "tables": [],
    "text_stats": {
      "word_count": 653,
      "char_count": 4033,
      "line_count": 54,
      "table_count": 0
    }
  },
  {
    "page_number": 3,
    "text": "International Journal of xxxxxx\nVol. x, No. x, (20xx), pp. xx-xx\nimportant data while converting to embeddings). The tweet-preprocessor library was\nused for basic data cleaning like removal of emojis, URLs, smileys etc. Removing\nthese is crucial to tweet cleaning as word embeddings for the mentioned items do\nnot exist and the neural network would not be able to recognize them. A major\nproblem which we encountered was dealing with the \"hashtags\" in each tweet.\nEliminating hashtags in a tweet would mean losing valuable information which\nmight be pivotal in geotagging. Instead of eliminating these hashtags, we made a\nfunction to reconstruct the tweet as explained below.Hashtags are generally in the\nform of #Word1Word2...Wordn, in which there are no spaces between the various\nwords or between the hash (#) and the words. So, the entire hashtag is treated as one\nword while splitting a tweet content string. If considered as a single word, a\ncorresponding word embedding wouldn't be found for any hashtag and it would be\ndiscarded. As a consequence, we would lose important information which could be\nvital in enhancing the effectiveness of the system. Hence, it becomes necessary to\nsplit the hashtag into the hash symbol (as # also has an embedding) and separate\nwords, so as to retain what could be very beneficial information for determining the\ntweet's geolocation. Another problem which led to the discarding of a large amount\nof the tweet contents while converting to embeddings was the presence of non-\nalphanumeric characters just after or before a word. For example, if a tweet's\ncontents are \"Hello, World\" then it would be split into two words - \"Hello,\" and\n\"World\". The problem arises because we have taken \"Hello,\" (with a comma) as a\nword instead of simply \"Hello\". \"Hello,\" may not have a corresponding embedding\nand thus would be discarded.\nFigure 1. The Architecture of the RNN model\nAs the above stated problems could not be solved by any existing utilities in the\ntweet-preprocessor library, we had to build a module ourselves to fix these issues.\nMost hashtags which have more than one word have all words capitalised. We used\nthis observation to separate words in the hashtag. Our module takes a tweet's\ncontents in the form of a string as input. A modified version of the input string is\ncreated wherein hashtags of the form #Word1Word2...Wordn are changed to #\nWord1 Word2 … Wordn. Also, every non-alphanumeric character (except #) is\nreplaced with a space. This modified string is given as output by the module.\n5. Architecture\nThe architecture consists of two RNN models, both of which consists of a three layered\nGRU(Gated Recurrent Unit) network along with Dropout and Batch Normalization layers\nincluded in between the above layers. The model predicts the region from where the tweet\noriginated. The model comprises of self-made embeddings which uses universal GloVe as\nwell the vocabulary used in the tweets. The embeddings were trained to accommodate any\nspecial usage of words in the tweets.\nISSN: XX XX-XXXX IJXX 3\nCopyright ⓒ XXXX SERSC",
    "tables": [],
    "text_stats": {
      "word_count": 507,
      "char_count": 3077,
      "line_count": 43,
      "table_count": 0
    }
  },
  {
    "page_number": 4,
    "text": "International Journal of xxxxxx\nVol. x, No. x, (20xx), pp. xx-xx\n5.1 Loss Function\nThe objective loss function used for this classification task is categorical cross entropy\nwhich is also known as softmax loss. It is a softmax activation added on top of cross\nentropy loss. If this loss is used, then an RNN will be trained to output a probability over\nthe number of classes for each tweet. This loss function is one of the most widely used\nfunctions in multi class classification.\n5.2 Word Embeddings\nThe first approach was to use pre-trained global vector embeddings(GloVe) embeddings\nprovided by Stanford. The word-to-vec dimension for each word in the vocabulary has\nbeen chosen to be 50. Training on those pretrained embeddings gave very poor results\neven after heavy training. The reason for this can be because the tweets in the dataset uses\nslang and has many spelling errors which are not included in the vocabulary of the\nembeddings. Therefore many of the words in each tweet from the dataset are rendered\nunimportant or useless which results in loss of features and the model cannot find the\ntrend in the data since the slangs can be native to specific region thus can prove to be\nhelpful in predicting the location. Another approach was to create a new vocabulary\nwhich consisted of the global vocabulary as well as the slangs used in the tweets. Thus, a\nnew embedding layer was created which included the global vocabulary and the native\nslangs.\n5.3 Model\nThe complete model architecture for this specific task consists of two individual models.\nThe two models are related to each other in the sense that the second one is a regularized\nversion of the first. The general architecture of the first model is provided in Fig.1. The\nactivation function between the layers is chosen to be ReLu. The weights of each layer of\nGRU were initialized by Xavier Initialization and the optimizer used for in this task is\nAdam Optimizer. The output of this layer is provided by the last layer i,e., the softmax\nlayer. The Recent work [13] has proved that promising a stable distribution of non-linear\ninputs could actually prevent the model from getting stuck at a local minima and getting a\nbetter training. Adding a linear layer before activation functions to perform batch-wise\nnormalization (Layer known as “Batch Normalisation”) helps the optimizer to reach\nminima faster than other methods. Although this isn’t a necessity (since it will lead to\nregularization) but the first model comprises of a dropout layer which is a very simple\nregularization technique [14] that prevents overfitting and speeds up training by randomly\nremoving neurons with a certain probability. As the probability p increases the odds of\nthis layer disabling a specific neuron increases. These layers can prevent weights from\nconverging to identical positions, but in this method for each training example a different\nrandom number of neurons are randomly disabled, which results in robust feature\nrepresentations that can be used to generalize better to new/unseen data. Aforementioned\nthe second model would be a regularized version of the first, for this specific task the\nneurons in the first layer has been chosen to be very high in number (>1000), and the\nnumber of neurons in the next layer drops significantly, as low as 128 and the future\nlayers then follows a general trend like the first architecture. This concept is known as\nDropConnect, which is a regularization method in deep learning. The probability p values\nin this architecture in also increased by a factor of 1.5 which disables a higher number of\nneurons and provides high regularization.\nISSN: XX XX-XXXX IJXX 4\nCopyright ⓒ XXXX SERSC",
    "tables": [],
    "text_stats": {
      "word_count": 612,
      "char_count": 3687,
      "line_count": 49,
      "table_count": 0
    }
  },
  {
    "page_number": 5,
    "text": "International Journal of xxxxxx\nVol. x, No. x, (20xx), pp. xx-xx\n6. Results and Discussion\nThe performance metric used here is accuracy. The accuracy with which the model can\npredict the correct region of the origin of the tweet. It is also the metric used for\ncomparing the related work in this domain.\nThe model had to predict the region to which the tweet belonged in the country. The\nmodel had to predict among 4 possible options, viz., NorthEast, Midwest, South, West. If\na tweet is given then the random guessing would have a 25% accuracy. The accuracy\nachieved at the end of our training were as follows: 72.02% for the validation set and the\n71.13% for the test set. Therefore, our model outperforms random guessing by a large\nmargin.\n6.1 Comparison with related works\nA new approach was implemented in training the recurrent neural network. The neural\nnetwork is trained in two successive stages for this specific task, initially on the\naforementioned first model and later on the second model. The first model is trained with\nhigh number of epochs (approximately 60). The embedding layer weights obtained at the\nend of training the first model is used as initial weights for the embedding layer of the\nsecond neural network. In the second stage of training a regularized model (w.r.t. the\nprevious training model. This is achieved by using concepts like drop connect and\nincreasing the value of dropout layer) is used to train the same training dataset with the\nprevious embedding layer with, however, with less number of epochs this time which is\naround 10. If this approach is utilised the validation score observed to increase by almost\n12%. The metric used for this task is accuracy. Accuracy of this approach on validation\nset is 72% and further when tested on test set it yields a score of 71.12%. However, if the\nconventional approach is used to train the RNN the score on the validation set did not\nexceed (59 - 61)% irrespective of the architecture of the model and the hyperparameters\nused. We present our result based on our best choice of architecture. We have achieved\nstate of art classification task for predicting the geolocation of the origin of tweet. In the\nprevious work the [1] the state of art accuracy was 66.7%. We haven’t experimented with\nthe hyperparameters that much yet ,but still achieved a higher accuracy.\nReferences\n[1] Ismini Lourentzou, Alex Morales, and ChengXiang Zhai, “Text-based Geolocation Prediction\nof Social Media Users with Neural Networks”, in IEEE International Conference on Big Data\n(BIG DATA), 2017, pp. 696-705.\n[2] Jey Han Lau, Lianhua Chi, Khoi-Nguyen Tran, and Trevor Cohn, ” End-to-end Network for\nTwitter Geolocation Prediction and Hashing”, in Proceedings of the The 8th International Joint\nConference on Natural Language Processing, 2017, pp. 744-753.\n[3] Philippe Thomas, Leonhard Hennig, “Twitter Geolocation Prediction Using Neural Networks”,\nin GSCL, Language Technologies for the Challenges of the Digital Age, 2017, pp. 248-255.\n[4] Afshin Rahimi, Trevor Cohn, and Timothy Baldwin, “Twitter User Geolocation Using a\nUnified Text and Network Prediction Model”, in Proceedings of the 53rd Annual Meeting of the\nAssociation for Computational Linguistics and the 7th International Joint Conference on Natural\nLanguage Processing (Short Papers), 2015, pp. 630-636.\n[5]Miriam Cha, Youngjune Gwon, and H. T. Kung, “Twitter Geolocation and Regional\nClassification via Sparse Coding”, in Association for the Advancement of Artificial Intelligence ,\n2015, pp. 770-781\n[6] B. P. Wing and J. Baldridge, “Simple supervised document geolocation with geodesic grids,” in\nProceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human\nLanguage Technologies-Volume 1. Association for Computational Linguistics, 2011, pp. 955–964.\n[7] J. Eisenstein, B. O’Connor, N. A. Smith, and E. P. Xing, “A latent variable model for\ngeographic lexical variation,” in Proceedings of the 2010 Conference on Empirical Methods in\nNatural Language Processing. Association for Computational Linguistics, 2010, pp. 1277–1287.\nISSN: XX XX-XXXX IJXX 5\nCopyright ⓒ XXXX SERSC",
    "tables": [],
    "text_stats": {
      "word_count": 654,
      "char_count": 4141,
      "line_count": 55,
      "table_count": 0
    }
  },
  {
    "page_number": 6,
    "text": "International Journal of xxxxxx\nVol. x, No. x, (20xx), pp. xx-xx\n[8] B. Han, P. Cook, and T. Baldwin, “Text-based twitter user geolocation prediction,” Journal of\nArtificial Intelligence Research, pp. 451–500, 2014.\n[9] J. Liu and D. Inkpen, “Estimating user location in social media with stacked denoising auto-\nencoders,” in Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language\nProcessing, NAACL, 2015, pp. 201–210.\n[10] S. Roller, M. Speriosu, S. Rallapalli, B. Wing, and J. Baldridge, “Supervised text-based\ngeolocation using language models on an adaptive grid,” in Proceedings of the 2012 Joint\nConference on Empirical Methods in Natural Language Processing and Computational Natural\nLanguage Learning. Association for Computational Linguistics, 2012, pp. 1500– 1510.\n[11] Backstrom, Sun, Marlow, “Find Me If You Can”: Improving Geographical Prediction with\nSocial and Spatial Proximity\n[12] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep network training by\nreducing internal covariate shift,” arXiv preprint arXiv: 1502.03167, 2015.\n[13] N.Srivastava, G. Hinton, A. Krizhevsky, L.Sutskever, and R. Salakhutdinov, “Dropout: A\nsimple way to prevent neural networks from overfitting,” The Journal of Machine Learning\nResearch, vol. 15, no. 1, pp. 1929-1958, 2014.\n[14] G. Quercini, H. Samet, J. Sankaranarayanan, and M. D.Lieberman, “Determining the spatial\nreader scopes of news sources using local lexicons,” in proceedings of the 18th SIGSPATIAL\ninternational conference on advances in geo-graphic information systems. ACM, 2010, pp. 43–52.\n[15] B. Han, A. Hugo, A. Rahimi, L. Derczynski, and T. Baldwin, “Twitter geolocation prediction\nshared task of the 2016 workshop on noisy user-generated text,” WNUT 2016, p. 213, 2016.\n[16] B. Wing and J. Baldridge, “Hierarchical discriminative classification for text-based\ngeolocation.” in EMNLP, 2014, pp. 336–348.\n[17] J. Eisenstein, A. Ahmed, and E. P. Xing, “Sparse additive generative models of text.” in\nICML, L. Getoor and T. Scheffer, Eds. Omnipress, 2011, pp. 1041–1048.\nISSN: XX XX-XXXX IJXX 6\nCopyright ⓒ XXXX SERSC",
    "tables": [],
    "text_stats": {
      "word_count": 310,
      "char_count": 2120,
      "line_count": 29,
      "table_count": 0
    }
  }
]